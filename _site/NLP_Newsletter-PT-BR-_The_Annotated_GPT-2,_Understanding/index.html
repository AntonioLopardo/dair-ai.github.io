<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->

<head>
<meta charset="utf-8">
<title>NLP Newsletter: GPT-2 Anotado, Entendendo self-distillation, Haiku, GANILLA, Sparkwiki, Ética em NLP, Torchmeta,… &#8211; dair.ai</title>
<meta name="description" content="Esta edição abrange tópicos como a compreensão de self-distillation, tradução de imagem para ilustração, considerações éticas para modelos de NLP, etc.">
<meta name="keywords" content="nlp_newsletter">


<!-- Twitter Cards -->
<meta name="twitter:title" content="NLP Newsletter: GPT-2 Anotado, Entendendo self-distillation, Haiku, GANILLA, Sparkwiki, Ética em NLP, Torchmeta,…">
<meta name="twitter:description" content="Esta edição abrange tópicos como a compreensão de self-distillation, tradução de imagem para ilustração, considerações éticas para modelos de NLP, etc.">
<meta name="twitter:site" content="@dair_ai">
<meta name="twitter:creator" content="@flavioclesio">

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:4000/images/nlp_newsletter_5.png">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP Newsletter: GPT-2 Anotado, Entendendo self-distillation, Haiku, GANILLA, Sparkwiki, Ética em NLP, Torchmeta,…">
<meta property="og:description" content="Esta edição abrange tópicos como a compreensão de self-distillation, tradução de imagem para ilustração, considerações éticas para modelos de NLP, etc.">
<meta property="og:url" content="http://localhost:4000/NLP_Newsletter-PT-BR-_The_Annotated_GPT-2,_Understanding/">
<meta property="og:site_name" content="dair.ai">

<meta property="og:image" content="http://localhost:4000/images/nlp_newsletter_5.png">







<link rel="canonical" href="http://localhost:4000/NLP_Newsletter-PT-BR-_The_Annotated_GPT-2,_Understanding/">
<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="dair.ai Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
	<script src="http://localhost:4000/assets/js/vendor/html5shiv.min.js"></script>
	<script src="http://localhost:4000/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="http://localhost:4000/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<link href='//fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700%7CPT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://localhost:4000/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://localhost:4000/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://localhost:4000/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/images/apple-touch-icon-144x144-precomposed.png">

</head>

<body class="post">

<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.8&appId=1537934899816329";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

<!-- Go to www.addthis.com/dashboard to customize your tools -->
<script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-4e43ef4f23bf37b0"></script>

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="http://localhost:4000/">dair.ai</a>
	</div><!-- /.site-name -->
	<div class="top-navigation">
		<nav role="navigation" id="site-nav" class="nav">
		    <ul>
		        
				    
				    <li><a href="http://localhost:4000/posts/" >Blog ✍️</a></li>
				
				    
				    <li><a href="http://localhost:4000/about/" >About ℹ️</a></li>
				
				    
				    <li><a href="http://localhost:4000/newsletter/" >NLP Newsletter 🗞️</a></li>
				
				    
				    <li><a href="http://localhost:4000/projects/" >Projects 💡</a></li>
				
				    
				    <li><a href="https://github.com/dair-ai" target="_blank">GitHub 📁</a></li>
				
				    
				    <li><a href="https://github.com/dair-ai/dair-ai.github.io/contribute" target="_blank">Contribute ✨</a></li>
				
				    
				    <li><a href="https://medium.com/dair-ai" target="_blank">Medium 📰</a></li>
				
				    
				    <li><a href="https://nlpoverview.com/" target="_blank">NLP Overview 📘</a></li>
				
				    
				    <li><a href="https://github.com/dair-ai/nlp_highlights" target="_blank">2019 NLP Highlights (PDF) 🔥</a></li>
				
		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->



<div id="main" role="main">
  <div class="article-author-side">
    

<div itemscope itemtype="http://schema.org/Person">


	<img src="http://localhost:4000/images/flavio.png" class="bio-photo" alt="Flavio Clesio bio photo">


  <h3 itemprop="name">Flavio Clesio</h3>
  <p>Machine Learning Engineer (NLP, CV, Marketplace RecSys)</p>

  <a href="http://twitter.com/flavioclesio" class="author-social" target="_blank"><i class="fa fa-fw fa-twitter-square"></i> Twitter</a>
  
  
  
  
  <a href="http://instagram.com/flavioclesio" class="author-social" target="_blank"><i class="fa fa-fw fa-instagram"></i> Instagram</a>
  
  <a href="http://github.com/fclesio" class="author-social" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a>
  
  
  
  
  
  
  
  
  
  
</div>

  </div>
  <article class="post">
    <div class="headline-wrap">
      
        
          <h1><a href="http://localhost:4000/NLP_Newsletter-PT-BR-_The_Annotated_GPT-2,_Understanding/" rel="bookmark" title="NLP Newsletter: GPT-2 Anotado, Entendendo self-distillation, Haiku, GANILLA, Sparkwiki, Ética em NLP, Torchmeta,…">NLP Newsletter: GPT-2 Anotado, Entendendo self-distillation, Haiku, GANILLA, Sparkwiki, Ética em NLP, Torchmeta,…</a></h1>
        
      
    </div><!--/ .headline-wrap -->

    
    <div class="article-wrap">
      <p><img src="https://cdn-images-1.medium.com/max/1200/1*YIhZsPaiBFkRMMWo5FAhGw.png" alt="" /></p>

<p><br />
Antes de tudo, gostaria de agradecer de ❤️ a todos vocês pelo incrível apoio e incentivo para continuar com a NLP Newsletter. Esse esforço requer pesquisa, edição, e tradução tediosas, mas que considero gratificantes e úteis para fornecer o melhor conteúdo. Espero que você esteja gostando deste conteúdo. 😉</p>

<p><br />
<a href="https://dair.ai/newsletter/"><em>Assine a NLP Newsletter</em></a> <em>🔖 para receber edições futuras via e-mail.</em></p>

<h1 id="publicações-">Publicações 📙</h1>

<p><strong><em>Um entendimento teórico do self-distillation</em></strong></p>

<p><br />
No contexto de Deep Learning, <a href="https://arxiv.org/pdf/1503.02531.pdf"><em>self-distillation</em></a> (<em>NT: auto-destilação</em>) é o processo de transferência de conhecimento de uma arquitetura para outra. As previsões do modelo original são alimentadas como valores de destino para o outro modelo durante o treinamento. Além de ter propriedades desejáveis como a redução do tamanho dos modelos, os resultados empíricos mostram que essa abordagem funciona bem em conjuntos de dados não vistos anteriormente pelo modelo (NT: amostras <em>held out</em>). Um grupo de pesquisadores publicou recentemente um artigo que fornece uma análise teórica com o foco em um melhor entendimento sobre o que está acontecendo neste processo de <em>destilação do conhecimento</em> e o porque ele é eficaz. Os resultados mostram que alguns poucos ciclos de destilação amplificam a regularização (devido ao fato que a técnica <a href="https://twitter.com/TheGradient/status/1228132843630387201?s=20"><em>progressivamente ajuda a limitar o número de funções base que representam a solução</em></a>) as quais tendem a reduzir o over-fitting. (Leia o paper <a href="https://arxiv.org/abs/2002.05715"><strong>aqui</strong></a>)</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*nWYO71awfooL4MrGK-tFww.png" alt="" /></p>

<p><a href="https://arxiv.org/abs/2002.05715"><em>Fonte</em></a></p>

<p><br />
<strong><em>Os anos 2010s: Nossa década de Deep Learning / Perspectivas para os 2020s</em></strong></p>

<p><br />
<a href="http://people.idsia.ch/~juergen/">Jürgen Schmidhuber,</a> um dos pioneiros em Inteligência Artificial,  <a href="http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html"><strong>postou recentemente em seu blog</strong></a> uma visão histórica sobre Deep Learning desde o ano de 2010. Alguns tópicos incluem <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTMs</a>, <a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">feedforward neural networks</a>, <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">GANs</a>, <a href="https://en.wikipedia.org/wiki/Deep_reinforcement_learning">deep reinforcement learning</a>, <a href="https://en.wikipedia.org/wiki/Meta_learning_(computer_science)">meta-learning</a>, world models, <a href="https://arxiv.org/abs/1503.02531">distilling NNs</a>, <a href="https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f">attention learning</a>, etc. O artigo traz algumas perspectivas futuras para os anos 2020 chamando atenção para questões como privacidade e mercado de dados.</p>

<p><br />
<strong><em>Usando Redes Neurais para a resolução de equações matemáticas</em></strong></p>

<p><br />
Pesquisadores do Facebook AI publicaram um <a href="https://arxiv.org/abs/1912.01412"><strong>paper</strong></a> em que apresentam um modelo treinado em problemas de matemática para prever possíveis soluções para inúmeras tarefas como, por exemplo, problemas de integração. A abordagem é baseada em uma nova estrutura semelhante à usada na <a href="https://en.wikipedia.org/wiki/Neural_machine_translation">neural machine translation</a> (<em>NT: tradução automática neural</em>), em que expressões matemáticas são representadas como um tipo de linguagem e as soluções tratadas como um problema de tradução. Assim, ao invés do modelo produzir uma tradução, a saída desta tradução é a própria solução do problema. Com isso, os pesquisadores afirmam que as redes Deep Learning não são apenas boas em raciocínio simbólico, mas podem ser usadas também para tarefas mais diversas.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*P_JtxoC8pYkXuXCp3e3QeQ.png" alt="" /></p>

<p><em>Equações sendo usadas como entrada, juntamente com a solução correspondente gerada pelo modelo—</em> <a href="https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/"><em>fonte</em></a></p>

<h1 id="criatividade-e-sociedade-">Criatividade e Sociedade 🎨</h1>

<p><strong><em>Inteligência Artificial para descobertas científicas</em></strong></p>

<p><br />
Mattew Hutson <a href="https://www.sciencemag.org/news/2020/02/models-galaxies-atoms-simple-ai-shortcuts-speed-simulations-billions-times"><strong>informa</strong></a> como a inteligência artificial (IA) pode ser utilizada para produzir emuladores que têm um uso importante na modelagem de fenômenos naturais complexos e que, por sua vez, podem levar a diferentes tipos de <em>descobertas científicas</em>. A mudança na construção desses emuladores acontece devido ao fato de que estes modelos geralmente exigem dados em larga escala e uma vasta exploração de parâmetros. Um <a href="https://arxiv.org/abs/2001.08055"><strong>paper recente</strong></a> propõe um método chamado DENSE que é uma abordagem baseada em <a href="https://en.wikipedia.org/wiki/Neural_architecture_search"><em>neural architecture search (NAS)</em></a> (NT: Exploração e busca de arquitetura de Redes Neurais) para criar emuladores precisos, contando apenas com uma quantidade limitada de dados de treinamento. Eles o testaram executando simulações para casos que incluem astrofísica, ciência climática e energia de fusão, entre outros.</p>

<p><br />
<strong><em>Melhorando a tradução de imagem para imagem</em></strong></p>

<p><br />
<a href="https://arxiv.org/abs/2002.05638">GANILLA</a> é uma abordagem que propõe o uso de <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">GANs</a> para melhorar a transferência de estilo e conteúdo em pares para tarefas de tradução <a href="https://paperswithcode.com/task/image-to-image-translation"><em>image-to-image</em></a> (NT: imagem para imagem). A abordagem propōe um modelo de imagem para imagem (com uma rede de geradores aprimorada) e este modelo é avaliado com base em uma nova estrutura de avaliação quantitativa que considera tanto o conteúdo quanto o estilo. A novidade do trabalho está na rede de geradores proposta, que considera um equilíbrio entre estilo e conteúdo que os modelos anteriores não conseguem. O código e os modelos pré-treinados estão <a href="https://github.com/giddyyupp/ganilla">disponíveis</a>. Leia o artigo completo <a href="https://arxiv.org/abs/2002.05638"><strong>aqui</strong></a>.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*l_B4vfaHVkXDwzM7SldiqQ.png" alt="" /></p>

<p><br />
<strong><em>Andrew Ng fala sobre o interesse em aprendizagem auto-supervisionada</em></strong></p>

<p><br />
Andrew Ng, o fundador do <a href="deeplearning.ai">deeplearning.ai</a>, falou no <a href="https://www.youtube.com/watch?v=0jspaMLxBig"><strong>podcast de Inteligência Artificial do Lex Friedman</strong></a> sobre os seguintes tópicos: seus primeiros anos em ML, o futuro da IA, educação em IA, recomendações para o uso adequado da ML, seus objetivos pessoais e quais técnicas de ML que devemos prestar atenção nesta década de 2020.</p>

<p><br />
Andrew explicou o motivo da sua animação em relação ao <em>self-supervised representation learning.</em> <a href="https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html"><strong>Self-supervised learning</strong></a> (NT: aprendizado de representação auto-supervisionado) envolve a estruturação de um problema de aprendizagem que visa obter supervisão dos próprios dados para fazer uso de grandes quantidades de dados não rotulados, o que é mais comum que os dados rotulados limpos. As representações são importantes e podem ser usadas para lidar com tarefas posteriores, semelhantes às usadas em modelos de linguagem como o <a href="https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#bert">BERT</a>.</p>

<p><br />
Também há muito interesse em usar o aprendizado auto-supervisionado para treinamento de representações visuais generalizadas que tornam os modelos mais precisos em ambientes com poucos recursos. Por exemplo, um método recente chamado <a href="https://arxiv.org/abs/2002.05709"><strong>SimCLR</strong></a> (liderado por <a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton">Geoffrey Hinton</a>) propõe uma estrutura para <em>aprendizagem auto-supervisionada contrastante</em> (<em>NT: contrastive self-supervised learning</em>) de representações visuais para melhorar a classificação de imagens em diferentes configurações, como transferência de aprendizado (NT: <a href="https://en.wikipedia.org/wiki/Transfer_learning">transfer learning</a>) e aprendizado semi-supervisionado.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*8zLzHFCyM3goc9y7KApHfg.png" alt="" /></p>

<p><a href="https://arxiv.org/abs/2002.05709"><em>fonte</em></a></p>

<h1 id="ferramentas-e-datasets-️">Ferramentas e Datasets ⚙️</h1>

<p><strong><em>Bibliotecas JAX</em></strong></p>

<p><br />
<a href="https://github.com/google/jax">JAX</a> é uma nova biblioteca que combina o NumPy e <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">diferenciação automática</a> para realizar pesquisas de ML de alto desempenho. Para simplificar os pipelines para a construção de redes neurais usando JAX, a <a href="https://deepmind.com/">DeepMind</a> lançou o <a href="https://github.com/deepmind/dm-haiku"><strong>Haiku</strong></a> e <a href="https://github.com/deepmind/rlax"><strong>RLax</strong></a>. O RLax simplifica a implementação de agentes de aprendizado por reforço e o Haiku simplifica a construção de redes neurais usando <em>modelos familiares com o paradigma de programação orientada a objetos.</em></p>

<p><br />
<strong><em>Uma ferramenta para processar dados da Wikipédia</em></strong></p>

<p><br />
<a href="https://github.com/epfl-lts2/sparkwiki"><strong>Sparkwiki</strong></a> é uma ferramenta para processar dados da Wikipédia. Esta versão faz parte de muitos esforços para permitir pesquisas interessantes de análise comportamental, como <a href="https://arxiv.org/abs/2002.06885">a captura de tendências e preconceitos em diferentes idiomas na Wikipédia</a>. Os autores descobriram que, independentemente do idioma, o comportamento de navegação dos usuários da Wikipédia mostra que eles tendem a compartilhar interesses comuns por categorias como filmes, música e esportes, mas as diferenças se tornam mais aparentes com eventos locais e particularidades culturais.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*K7N9KbQlbuqowUeePjLtdw.jpeg" alt="" /></p>

<p><br />
<strong><em>Tokenizers em Rust, DistilBERT e outros</em></strong></p>

<p><br />
Um novo release dos <a href="https://github.com/huggingface/transformers/releases/tag/v2.5.0"><strong>Transformers</strong></a> da Hugging Face inclui a integração de sua biblioteca de tokenização rápida, que visa acelerar modelos como o <a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">BERT</a>, <a href="https://arxiv.org/abs/1907.11692">RoBERTa</a>, <a href="https://openai.com/blog/better-language-models/">GPT-2</a> e outros modelos criados pela comunidade.</p>

<h1 id="Ética-em-inteligência-artificial-">Ética em Inteligência Artificial 🚨</h1>

<p><strong><em>Considerações éticas para modelos de NLP (Processamento de Linguagem Natural) e Machine Learning</em></strong></p>

<p><br />
Em um novo <a href="https://soundcloud.com/nlp-highlights/106-ethical-considerations-in-nlp-research-emily-bender"><strong>episódio</strong></a> do postcast <a href="https://soundcloud.com/nlp-highlights">NLP Highlights,</a> <a href="https://twitter.com/emilymbender">Emily Bender</a> e os hosts conversaram sobre algumas considerações éticas no desenvolvimento de modelos e tecnologias de NLP no contexto da academia e do seu uso no mundo real. Alguns dos tópicos da discussão incluem considerações éticas nas tarefas de NLP, abordagens sobre coleta de dados e eventualmente considerações na publicação de resultados.</p>

<p><br />
Além de todas as considerações acima, uma preocupação discutida é que a comunidade de IA está se concentrando demais na otimização de métricas específicas, o que contraria os objetivos que a IA pretende alcançar. Rachel Thomas e David Uminsky discutem os problemas dessa abordagem através de uma <a href="https://arxiv.org/abs/2002.08512"><strong>análise completa</strong></a> de diferentes casos de uso. Eles também propõem uma estrutura simples para mitigar este problema, que envolve o uso e a combinação de várias métricas, seguidas pelo envolvimento das pessoas afetadas diretamente pela tecnologia.</p>

<h1 id="artigos-e-blog-posts-️">Artigos e Blog posts ✍️</h1>

<p><strong>“The Annotated GPT-2”</strong></p>

<p><br />
Aman Arora publicou recentemente uma postagem no blog excepcionalmente intitulada <a href="https://amaarora.github.io/2020/02/18/annotatedGPT2.html">“The Annotated GPT-2“</a> explicando o funcionamento interno do modelo baseado em <a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">Transformer</a> chamado <a href="https://openai.com/blog/better-language-models/">GPT-2</a>. Sua abordagem foi inspirada em <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a> que adotou uma abordagem de anotação para explicar as partes importantes do modelo. Aman fez um grande esforço para reimplementar o <a href="https://openai.com/blog/better-language-models/">GPT-2</a> da <a href="https://openai.com/">OpenAI</a> usando o <a href="https://pytorch.org/">PyTorch</a> e a biblioteca <a href="https://huggingface.co/transformers/">Transformers da Hugging Face</a>. É um trabalho brilhante!</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*oRFMJTEojyQ-uocVES5GYA.png" alt="" /></p>

<p><a href="https://amaarora.github.io/2020/02/18/annotatedGPT2.html"><em>fonte</em></a></p>

<p><br />
<strong><em>Além do BERT?</em></strong></p>

<p><br />
<a href="https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1"><strong>Um ponto interessante foi levantado</strong></a> por Sergi Castella sobre o que está além do <a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">BERT</a>. Os principais tópicos incluem o aprimoramento das métricas, uma reflexão de como a biblioteca <a href="https://huggingface.co/transformers/">Transformers da Hugging Face</a> ajuda na pesquisa, alguns conjuntos de dados interessantes para análise, etc.</p>

<p><br />
<strong><em>Operador de Compressão de Matrizes</em></strong></p>

<p><br />
O Blog do TensorFlow publicou um <a href="https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016"><strong>post</strong></a> explicando as técnicas e a importância por trás da compressão matrizes em um modelo de Deep Learning. <em>A compactação matricial</em> (<em>NT: Matrix compression</em>) pode ajudar a criar modelos menores e mais eficientes que podem ser incorporados a dispositivos menores, como telefones e assistentes domésticos. Concentrar-se na compressão dos modelos por meio de métodos como <a href="https://en.wikipedia.org/wiki/Low-rank_approximation">low-rank-approximation</a> e <a href="https://en.wikipedia.org/wiki/Quantization_(signal_processing)">quantização</a> significa que não precisamos comprometer a qualidade do modelo.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*fpAdJvBIf4SKxF3gTIpe_g.png" alt="" /></p>

<p><a href="https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016"><em>fonte</em></a></p>

<h1 id="educação-">Educação 🎓</h1>

<p><strong><em>Fundamentos de NLP</em></strong></p>

<p><br />
Estou animado por lançado um rascunho do Capítulo 1 da minha nova série chamado <a href="https://medium.com/dair-ai/fundamentals-of-nlp-chapter-1-tokenization-lemmatization-stemming-and-sentença-segmentação-b362c5d07684"><strong>Fundamentos de NLP</strong></a>. Esta série ensina conceitos de NLP a partir do básico, compartilhando boas práticas, referências importantes, erros comuns a serem evitados e o que está por vir no que se refere a NLP. Um <a href="https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ">notebook no Colab</a> foi incluído e o projeto será mantido <a href="https://github.com/dair-ai/nlp_fundamentals">aqui</a>.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*mS5NcoJ_c8hYTjiJsuu_8g.gif" alt="" /></p>

<p><br />
<strong><em>Revisão/Discussão Online: Parte I sessão de leitura para fundamentos da matemática</em></strong></p>

<p><br />
O time do <a href="https://www.meetup.com/Machine-Learning-Tokyo/">Meetup “Machine Learning Tokyo”</a> está hospedando uma discussão on-line remota, revisando capítulos que foram abordados em suas recentes sessões de estudo on-line. O grupo já havia estudado capítulos com base no livro <a href="https://mml-book.github.io/">Mathematics For Machine Learning</a> escrito por Marc Peter Deisenroth, A Aldo Faisal e Cheng Soon Ong. O <a href="https://www.meetup.com/Machine-Learning-Tokyo/events/268817313/"><strong>evento</strong></a> está programado para 8 de março de 2020.</p>

<p><br />
<strong><em>Recomendações de livros</em></strong></p>

<p><br />
Em um segmento anterior, discutimos a importância da compressão de matriz para a construção de modelos pequenos (em termos de espaço) de ML. Se você estiver interessado em aprender mais sobre como construir redes neurais profundas menores para sistemas embarcados, confira este ótimo livro chamado <a href="https://tinymlbook.com/?linkId=82595412"><strong>TinyML</strong></a> de Pete Warden e Daniel Situnayake.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/0*omOa3aw2bfMzX2Qm.jpg" alt="" /></p>

<p><a href="https://www.amazon.com/TinyML-Learning-TensorFlow-Ultra-Low-Micro-Controllers/dp/1492052043"><em>fonte</em></a></p>

<p><br />
Outro livro interessante para ficar de olho é o próximo título  <strong>“</strong><a href="https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527"><strong>Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD</strong></a><strong>”</strong> de Jeremy Howard e Sylvain Gugger. O livro tem como objetivo fornecer a base matemática necessária para criar e treinar modelos para abordar tarefas nas áreas de visão computacional e NLP.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/0*i2OmtWOncatOYsZv.jpg" alt="" /></p>

<p><a href="https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527"><em>source</em></a></p>

<h1 id="menções-honrosas-️">Menções honrosas ⭐️</h1>

<p>Você pode acessar a NLP Newsletter anterior em PT-BR <a href="https://dair.ai/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG/">aqui</a>.</p>

<p><br />
<a href="https://arxiv.org/abs/1909.06576"><strong>Torchmeta</strong></a> é uma biblioteca para pesquisa em meta-aprendizado. Esta biblioteca é de autoria de Tristan Deleu.</p>

<p><br />
Manuel Tonneau escreveu um <a href="https://creatext.ai/blog-posts/machine-text-writing-gpt2-beam-search?utm_medium=newsletter"><strong>post</strong></a> oferecendo uma visão mais detalhada em relação ao hardware envolvido em modelagem de linguagem. Alguns tópicos incluem <em>greedy</em> e <a href="https://en.wikipedia.org/wiki/Beam_search">beam search</a> e <a href="https://openreview.net/forum?id=rygGQyrFvH">nucleus sampling</a>.</p>

<p><br />
O MIT <a href="http://introtodeeplearning.com/"><strong>lançou</strong></a> o plano de estudos completo e a programação do curso intitulado “Introdução ao Deep Learning”, incluindo vídeos das palestras já ministradas. Eles pretendem lançar palestras em vídeo e slides uma vez por semana.</p>

<p><br />
Aprenda a treinar um modelo para <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">reconhecimento de entidade (NER)</a> usando uma abordagem baseada no Transformer em <a href="https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py"><strong>300 linhas de código</strong></a>. Você pode encontrar o Google Colab em anexo <a href="https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn">aqui</a>.</p>

<hr />

<p>Se você tiver datasets, projetos, postagens de blog, tutoriais ou documentos que deseja compartilhar na próxima edição da NLP Newsletter, entre em contato conosco pelo e-mail ellfae@gmail.com ou via <a href="https://twitter.com/omarsar0"><strong>DM no Twitter</strong></a>.</p>

<p><br />
<a href="https://dair.ai/newsletter/"><em>Assine a NLP Newsletter</em></a> <em>🔖 para receber edições futuras via e-mail.</em></p>

      <hr />
      <footer role="contentinfo">
        <div class="social-share">
  <!-- Go to www.addthis.com/dashboard to customize your tools --> 
  <div class="addthis_inline_share_toolbox"></div>
  <!--
  <h4>Share on</h4>
  <ul>
    <li>
      <a href="https://twitter.com/intent/tweet?text=http://localhost:4000/NLP_Newsletter-PT-BR-_The_Annotated_GPT-2,_Understanding/" class="twitter" title="Share on Twitter"><i class="fa fa-twitter"></i><span> Twitter</span></a>
    </li>
    <li>
      <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/NLP_Newsletter-PT-BR-_The_Annotated_GPT-2,_Understanding/" class="facebook" title="Share on Facebook"><i class="fa fa-facebook"></i><span> Facebook</span></a>
    </li>
    <li>
      <a href="https://plus.google.com/share?url=http://localhost:4000/NLP_Newsletter-PT-BR-_The_Annotated_GPT-2,_Understanding/" class="google-plus" title="Share on Google Plus"><i class="fa fa-google-plus"></i><span> Google+</span></a>
    </li>
  </ul>-->
</div><!-- /.social-share -->
        <p class="byline"><strong>NLP Newsletter: GPT-2 Anotado, Entendendo self-distillation, Haiku, GANILLA, Sparkwiki, Ética em NLP, Torchmeta,…</strong> was published on <time datetime="2020-02-29T00:00:00+01:00">February 29, 2020</time>.</p>
        
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dair-ai'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<!--
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
-->

      </footer>
    </div><!-- /.article-wrap -->
  
    <section id="disqus_thread"></section><!-- /#disqus_thread -->
  
  </article>
</div><!-- /#main -->

<div class="footer-wrap">
  
  <div class="related-articles">
  <h4>You might also enjoy <small class="pull-right">(<a href="http://localhost:4000/posts/">View all posts</a>)</small></h4>
    <ul>
    
      <li><a href="http://localhost:4000/NLP_Newsletter_NLP_7-ZH-.md/" title="NLP 简报（Issue#7）: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,…">NLP 简报（Issue#7）: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,…</a></li>
    
      <li><a href="http://localhost:4000/NLP_Newsletter_NLP_7/" title="NLP Newsletter: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,…">NLP Newsletter: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,…</a></li>
    
      <li><a href="http://localhost:4000/NLP_Newsletter_-7_-FR/" title="NLP Newsletter [FR] #7: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,…">NLP Newsletter [FR] #7: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,…</a></li>
    
    </ul>
    <hr />
  </div><!-- /.related-articles -->
  
  <footer>
    

<span>&copy; 2020 dair.ai. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="http://mademistakes.com/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a> theme.</span>

  </footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://localhost:4000/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://localhost:4000/assets/js/scripts.min.js"></script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  var _gaq = _gaq || [];
  var pluginUrl =
 '//www.google-analytics.com/plugins/ga/inpage_linkid.js';
  _gaq.push(['_require', 'inpage_linkid', pluginUrl]);
  _gaq.push(['_setAccount', 'UA-158959084-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
<script async defer src="https://buttons.github.io/buttons.js"></script>


  
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dair-ai'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<!--
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
-->




</body>
</html>
