<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->

<head>
<meta charset="utf-8">
<title>Bolet√≠n informativo NLP #5: GPT-2 Explicado, Entendiendo ‚ÄòSelf-Distillation‚Äô, Haiku, GANILLA, Sparkwiki, √âtica en el NLP, Torchmeta,... &#8211; dair.ai</title>
<meta name="description" content="">
<meta name="keywords" content="nlp_newsletter">


<!-- Twitter Cards -->
<meta name="twitter:title" content="Bolet√≠n informativo NLP #5: GPT-2 Explicado, Entendiendo ‚ÄòSelf-Distillation‚Äô, Haiku, GANILLA, Sparkwiki, √âtica en el NLP, Torchmeta,...">
<meta name="twitter:description" content="">
<meta name="twitter:site" content="@dair_ai">
<meta name="twitter:creator" content="@nicolas_araque">

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://dair.ai/images/nlp_newsletter_5.png">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Bolet√≠n informativo NLP #5: GPT-2 Explicado, Entendiendo ‚ÄòSelf-Distillation‚Äô, Haiku, GANILLA, Sparkwiki, √âtica en el NLP, Torchmeta,...">
<meta property="og:description" content="">
<meta property="og:url" content="https://dair.ai/Bolet%C3%ADn_informativo_NLP_GPT-2_Explicado,_Entendie/">
<meta property="og:site_name" content="dair.ai">

<meta property="og:image" content="https://dair.ai/images/nlp_newsletter_5.png">







<link rel="canonical" href="https://dair.ai/Bolet%C3%ADn_informativo_NLP_GPT-2_Explicado,_Entendie/">
<link href="https://dair.ai/feed.xml" type="application/atom+xml" rel="alternate" title="dair.ai Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="https://dair.ai/assets/css/main.css">

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
	<script src="https://dair.ai/assets/js/vendor/html5shiv.min.js"></script>
	<script src="https://dair.ai/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="https://dair.ai/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<link href='//fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700%7CPT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="https://dair.ai/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="https://dair.ai/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="https://dair.ai/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://dair.ai/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://dair.ai/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://dair.ai/images/apple-touch-icon-144x144-precomposed.png">

</head>

<body class="post">

<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.8&appId=1537934899816329";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

<!-- Go to www.addthis.com/dashboard to customize your tools -->
<script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-4e43ef4f23bf37b0"></script>

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="https://dair.ai/">dair.ai</a>
	</div><!-- /.site-name -->
	<div class="top-navigation">
		<nav role="navigation" id="site-nav" class="nav">
		    <ul>
		        
				    
				    <li><a href="https://dair.ai/posts/" >Blog ‚úçÔ∏è</a></li>
				
				    
				    <li><a href="https://dair.ai/about/" >About ‚ÑπÔ∏è</a></li>
				
				    
				    <li><a href="https://dair.ai/newsletter/" >NLP Newsletter üóûÔ∏è</a></li>
				
				    
				    <li><a href="https://dair.ai/projects/" >Projects üí°</a></li>
				
				    
				    <li><a href="https://github.com/dair-ai" target="_blank">GitHub üìÅ</a></li>
				
				    
				    <li><a href="https://github.com/dair-ai/dair-ai.github.io/contribute" target="_blank">Contribute ‚ú®</a></li>
				
				    
				    <li><a href="https://medium.com/dair-ai" target="_blank">Medium üì∞</a></li>
				
				    
				    <li><a href="https://nlpoverview.com/" target="_blank">NLP Overview üìò</a></li>
				
				    
				    <li><a href="https://github.com/dair-ai/nlp_highlights" target="_blank">2019 NLP Highlights (PDF) üî•</a></li>
				
		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->



<div id="main" role="main">
  <div class="article-author-side">
    

<div itemscope itemtype="http://schema.org/Person">


	<img src="https://dair.ai/images/naraquev.png" class="bio-photo" alt="Nicolas Araque Volk bio photo">


  <h3 itemprop="name">Nicolas Araque Volk</h3>
  <p>Hardware Knight. Artificial Intelligence Advocate and Data Driven Geek.</p>

  <a href="http://twitter.com/nicolas_araque" class="author-social" target="_blank"><i class="fa fa-fw fa-twitter-square"></i> Twitter</a>
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
</div>

  </div>
  <article class="post">
    <div class="headline-wrap">
      
        
          <h1><a href="https://dair.ai/Bolet%C3%ADn_informativo_NLP_GPT-2_Explicado,_Entendie/" rel="bookmark" title="Bolet√≠n informativo NLP #5: GPT-2 Explicado, Entendiendo ‚ÄòSelf-Distillation‚Äô, Haiku, GANILLA, Sparkwiki, √âtica en el NLP, Torchmeta,...">Bolet√≠n informativo NLP #5: GPT-2 Explicado, Entendiendo ‚ÄòSelf-Distillation‚Äô, Haiku, GANILLA, Sparkwiki, √âtica en el NLP, Torchmeta,...</a></h1>
        
      
    </div><!--/ .headline-wrap -->

    
    <div class="article-wrap">
      <p><img src="https://cdn-images-1.medium.com/max/1200/1*YIhZsPaiBFkRMMWo5FAhGw.png" alt="" /></p>

<p><br />
Post Original en Ingles: <a href="https://dair.ai/NLP_Newsletter_The_Annotated_GPT-2,_Understanding/">https://dair.ai/NLP_Newsletter_The_Annotated_GPT-2,_Understanding/</a></p>

<p><br />
Primero que todo, no puedo agradecerles lo suficiente a todos por el increible animo y soporte para continuar con el bolet√≠n informativo de NLP. Este esfuerzo requiere una investigaci√≥n y edici√≥n tediosa, la cual encuentro gratificante y √∫til para proveer el mejor contenido. Espero que lo est√©n disfrutando tanto como yo.</p>

<p><br />
<a href="https://dair.ai/newsletter/">Suscr√≠bete</a> al Bolet√≠n Informativo de NLP para recibir futuras ediciones en tu email. Este boletin lo desarrolla Elvis Saravia, editor de dair.ai</p>

<h1 id="publicaciones">Publicacionesüìô</h1>

<p><strong><em>Un entendimiento te√≥rico del self-distillation</em></strong></p>

<p><br />
En el contexto del Deep Learning, self-distillation es el proceso de transferir conocimiento de una arquitectura a otra arquitectura id√©ntica. Las predicciones del modelo original son alimentadas como valores objetivo al otro modelo mientras se entrena. Adem√°s de tener propiedades deseables (como reducir el tama√±o del modelo) resultados emp√≠ricos demuestran que esta aproximaci√≥n trabaja bastante bien en sets de datos del tipo held-out. Un grupo de investigadores recientemente public√≥ una investigaci√≥n que provee un an√°lisis te√≥rico con foco en entender mejor qu√© est√° pasando en el proceso de destilaci√≥n del conocimiento y por que es efectivo. Los resultados muestran que unas pocas rondas de self-distillation amplifica la regularizaci√≥n (<a href="https://twitter.com/TheGradient/status/1228132843630387201?s=20">limitando progresivamente el n√∫mero de funciones b√°sicas que representan la soluci√≥n</a>) lo que tiende a reducir el over-fitting. (Lee la investigaci√≥n completa <a href="https://arxiv.org/abs/2002.05715">aqu√≠</a>)</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*nWYO71awfooL4MrGK-tFww.png" alt="" /></p>

<p><br />
<em>Traducci√≥n: Figura 1. Ilustraci√≥n esquem√°tica del proceso de self-distillation por dos iteraciones. Fuente: <a href="https://arxiv.org/abs/2002.05715">https://arxiv.org/abs/2002.05715</a></em></p>

<p><br />
<strong><em>Los 2010: La d√©cada del Deep Learning / Mirada al 2020</em></strong></p>

<p><br />
<a href="http://people.idsia.ch/~juergen/">J√ºrgen Schmidhuber,</a> un pionero en la inteligencia artificial, public√≥ recientemente un <a href="http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html">art√≠culo</a> titulado ‚ÄúFoco en proveer un resumen hist√≥rico del Deep Learning desde el 2010‚Äù. Algunos de los temas incluyen LSTMs, redes neuronales feedforward, GANs, DeepRL, Meta-Learning, World Models, distillings NNs, Attention Learning, etc. El art√≠culo concluye con una mirada al 2020, donde llama la atenci√≥n a temas como la privacidad y los mercados de datos.</p>

<p><br />
<strong><em>Utilizando Redes Neuronales para resolver ecuaciones matem√°ticas avanzadas.</em></strong></p>

<p><br />
Investigadores de Facebook AI publicaron una <a href="https://arxiv.org/abs/1912.01412">investigaci√≥n</a> que dice proponer un modelo entrenado en problemas matem√°ticos y sus soluciones para aprender a predecir posibles soluciones en problemas c√≥mo resolver integrales matematicas. El acercamiento est√° basado en un novedoso framework similar al usado en neural machine translation donde una expresi√≥n matem√°tica es representada como una especie de lenguaje y la soluci√≥n es tratada como su traducci√≥n. Por esto, en vez de que el resultado sea una traducci√≥n, es la soluci√≥n al problema matematico. Con esto, los investigadores concluyen que las redes neuronales profundas no solo son buenas para el razonamiento simb√≥lico sino tambi√©n para tareas m√°s diversas.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*P_JtxoC8pYkXuXCp3e3QeQ.png" alt="" /></p>

<p><em>Ecuaciones incluidas como entrada con su correspondiente soluci√≥n. <a href="https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/">Fuente</a></em></p>

<h1 id="creatividad-y-sociedad-">Creatividad y Sociedad üé®</h1>

<p><strong><em>IA para el descubrimiento cient√≠fico</em></strong></p>

<p><br />
Mattew Hutson <a href="https://www.sciencemag.org/news/2020/02/models-galaxies-atoms-simple-ai-shortcuts-speed-simulations-billions-times">reporta</a> c√≥mo la inteligencia artificial (IA) puede ser usada para producir emuladores con la capacidad de modelar fen√≥menos naturales complejos. Estos modelos pueden a su vez apuntar a diferentes tipos de descubrimientos cient√≠ficos. El desaf√≠o al construir estos emuladores es que necesitan una gran cantidad de datos y una b√∫squeda extensiva de par√°metros. Una <a href="https://arxiv.org/abs/2001.08055">investigaci√≥n</a> reciente propone una t√©cnica llamada DENSE, basada en <a href="https://en.wikipedia.org/wiki/Neural_architecture_search">neural architecture search</a> para construir emuladores precisos utilizando una cantidad de datos de entrenamiento limitada. Los investigadores hicieron distintas pruebas construyendo simuladores para caso como astrof√≠sica, ciencia clim√°tica, energ√≠a de fusi√≥n, etc.</p>

<p><br />
<strong><em>Mejorando la traducci√≥n imagen-a-ilustraci√≥n</em></strong></p>

<p><br />
GANILLA es una aproximaci√≥n que propone el uso de las GANs para mejorar la transferencia de estilo y contenido en la tarea de convertir una imagen en una ilustraci√≥n. En particular, un modelo para <a href="https://paperswithcode.com/task/image-to-image-translation">imagen-a-ilustraci√≥n</a> es propuesto (con una red generadora mejorada) y evaluado basado en un nuevo framework para evaluaci√≥n cuantitativa que considera tanto el contenido como el estilo de la imagen generada. La novedad de esta investigaci√≥n es en la red generadora propuesta que considera un balance entre estilo y contenido que anteriores intentos fallaron en alcanzar. El c√≥digo y modelo pre entrenado est√°n disponibles en <a href="https://github.com/giddyyupp/ganilla">l√≠nea</a>. <a href="https://arxiv.org/abs/2002.05638">Aqu√≠</a> se puede encontrar la investigaci√≥n completa.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*l_B4vfaHVkXDwzM7SldiqQ.png" alt="" /></p>

<p><em>Traducci√≥n: Ejemplos de salida de CycleGAN, DualGAN, y nuestro m√©todo (GANILLA) utilizando diferentes estilos. CycleGAN y GANILLA generan im√°genes en un estilo de ilustraci√≥n, pero DualGAN falla en transferir el estilo. Sin embargo, CycleGAN falle en preservar el contenido de la imagen original, por ejemplo, aleatoriamente coloca caras de animales en el aire. Nuestro m√©todo preserva contenido as√≠ como tambi√©n transfiere el estilo de la imagen original.</em></p>

<p><br />
<strong><em>Andrew Ng habla sobre su inter√©s en self-supervised learning</em></strong></p>

<p><br />
Andrew Ng, fundador de <a href="http://deeplearning.ai/">deeplearning.ai</a>, participa en el podcast Artificial Inteligence de Lex Friedman para <a href="https://www.youtube.com/watch?v=0jspaMLxBig">hablar</a> de diferentes temas como sus comienzos en el ML, el futuro del AI y la educaci√≥n, recomendaciones de uso para ML, sus metas personales y a cu√°les t√©cnicas de ML prestar atenci√≥n en el 2020.</p>

<p><br />
Andrew tambien explico por que est√° muy emocionado sobre el self-supervised representation learning. <a href="https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html">Self-supervised learning</a> se trata sobre enfocar un problema de aprendizaje que intenta obtener la supervisi√≥n de los datos en s√≠ mismos para hacer uso de grandes cantidades de datos no clasificados, escenario m√°s com√∫n que datos limpios y clasificados. Las representaciones aprendidas, opuesto al rendimiento de la tarea, son importantes y pueden ser usadas en tareas m√°s adelante, similar a lo que est√° siendo usado en modelos de lenguaje natural como <a href="https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#bert">BERT</a>.</p>

<p><br />
Hay mucho inter√©s en usar el self-supervised learning para aprender representaciones visuales generales que hagan al modelo m√°s preciso en situaciones de bajos recursos. Por ejemplo, un nuevo m√©todo llamado <a href="https://arxiv.org/abs/2002.05709">SimCLR</a> (Dirigido por Geoffrey Hinton) propone un framework para el <em>constrastive self-supervised learning</em> de representaciones visuales para mejorar el resultado de la clasificaci√≥n de im√°genes en diferentes contextos como el <em>transfer learning</em> y el <em>semi-supervised learning</em>.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*8zLzHFCyM3goc9y7KApHfg.png" alt="" /></p>

<p><em>Traducci√≥n: ImageNet top-1 accuracy de clasificadores lineales entrenados en representaciones aprendidas con diferentes m√©todos de auto-aprendizaje (pre entrenados en ImageNet). La cruz gris indica un modelo ResNet-50 supervisado. Nuestro m√©todo, SimCLR, es mostrado en negrita.</em></p>

<h1 id="herramientas-y-set-de-datos-Ô∏è">Herramientas y Set de Datos ‚öôÔ∏è</h1>

<p><strong><em>Librer√≠as JAX</em></strong></p>

<p><br />
<a href="https://github.com/google/jax">JAX</a> es una nueva librer√≠a que combina NumPy y diferenciaci√≥n autom√°tica para realizar investigaci√≥n de alto rendimiento en ML. Para simplificar procesos para construir redes neuronales usando JAX, DeepMind lanz√≥ <a href="https://github.com/deepmind/dm-haiku">Haiku</a> y <a href="https://github.com/deepmind/rlax">RLax</a>. RLAx simplifica la implementaci√≥n de agentes de reinforcement learning y Haiku simplifica la construcci√≥n de redes neuronales utilizando paradigmas familiares de programaci√≥n orientada a objetos.</p>

<p><br />
<strong><em>Una herramienta para procesar datos de Wikipedia</em></strong></p>

<p><br />
<a href="https://github.com/epfl-lts2/sparkwiki">Spakwiki</a> es una herramienta para procesar datos de Wikipedia. Este lanzamiento es parte de muchos esfuerzos para permitir an√°lisis interesantes de comportamiento como <a href="https://arxiv.org/abs/2002.06885">capturar tendencias y sesgos de lenguaje en ediciones de distinto idioma en Wikipedia</a>. Los autores descubrieron que independientemente del lenguaje, el comportamiento de b√∫squeda de los usuarios de Wikipedia es muy parecido en categor√≠as como pel√≠culas, m√∫sica y deportes pero que las diferencias se vuelven m√°s aparentes con eventos locales y particularidades de cultura.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*K7N9KbQlbuqowUeePjLtdw.jpeg" alt="" /></p>

<p><br />
<strong><em>Tokenizadores, caso DistilBERT y modelos</em></strong></p>

<p><br />
Una <a href="https://github.com/huggingface/transformers/releases/tag/v2.5.0">nueva entrega de Transformers</a> de Hugging Face ahora incluye la integraci√≥n de su r√°pida librer√≠a de tokenizaci√≥n que intenta mejorar el tiempo de ejecuci√≥n de modelos como BERT, RoBERTa, GPT2, y otro modelos construidos por la comunidad.</p>

<h1 id="√©tica-en-la-inteligencia-artificial-">√âtica en la Inteligencia Artificial üö®</h1>

<p><strong><em>Consideraciones √©ticas para modelos de NLP y ML.</em></strong></p>

<p><br />
En un nuevo <a href="https://soundcloud.com/nlp-highlights/106-ethical-considerations-in-nlp-research-emily-bender">episodio</a> de <a href="https://soundcloud.com/nlp-highlights">NLP Highlights</a>, Emily Bender habla sobre algunas consideraciones √©ticas cuando se desarrollan modelos de Procesamiento de Lenguaje Natural y tecnolog√≠as tanto en el contexto de la academia como de industria. Algunos de los t√≥picos discutidos incluyen consideraciones √©ticas cuando se dise√±an tareas, recolecci√≥n de datos, y eventualmente publicaci√≥n de resultados.</p>

<p><br />
Adicionalmente a todas las consideraciones mencionadas anteriormente, una preocupaci√≥n que siempre se discute en la comunidad de AI es enfocarse demasiado en optimizar una m√©trica, lo que va en contra de los cimientos de lo que el campo trata de alcanzar. Rachel Thomas y David Uminsky discuten c√≥mo esto puede salir mal a trav√©s de un <a href="https://arxiv.org/abs/2002.08512">an√°lisis</a> con diferentes casos de uso. Adem√°s, proponen un framework simple para mitigar el problema que involucra el uso y combinaci√≥n de m√∫ltiples m√©tricas, seguido del involucramiento directo de los usuarios afectados por la tecnolog√≠a que se est√° desarrollando.</p>

<h1 id="art√≠culos-y-blogs-Ô∏è">Art√≠culos y Blogs ‚úçÔ∏è</h1>

<p><strong><em>GPT-2 Explicado</em></strong></p>

<p><br />
Aman Arora recientemente public√≥ un art√≠culo excepcional titulado ‚Äú<a href="https://amaarora.github.io/2020/02/18/annotatedGPT2.html"><strong>El GPT-2 Explicado</strong></a>‚Äù explicando el funcionamiento interno del modelo basado en la t√©cnica del Transformer llamado GPT-2. Su aproximaci√≥n fue inspirada por el art√≠culo <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">El Transformer Explicado</a> que tom√≥ una aproximaci√≥n a explicar las partes m√°s importantes del modelo siguiendo ejemplos f√°ciles de seguir y c√≥digo comentado. Amant realiz√≥ un gran esfuerzo para re implementar GPT-2 de OpenAI utilizando PyTorch y la librer√≠a de Transformers de Hugging Face. Es un trabajo brillante.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*oRFMJTEojyQ-uocVES5GYA.png" alt="" /></p>

<p><em><a href="https://amaarora.github.io/2020/02/18/annotatedGPT2.html">Fuente</a></em></p>

<p><br />
<strong><em>M√°s all√° de BERT?</em></strong></p>

<p><br />
<a href="https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1">Opini√≥n</a> interesante de Sergi Castella en que hay m√°s all√° de BERT. El tema principal incluye mejorar m√©tricas, como la librer√≠a Transformer de Hugging Face empodera la investigaci√≥n, set de datos interesantes para revisar, utilizaci√≥n de modelos, etc.</p>

<p><br />
<strong><em>Operador para comprimir Matrices</em></strong></p>

<p><br />
El blog the TensorFlow publico un <a href="https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016">articulo</a> donde explican las t√©cnicas para compresi√≥n de matrices y su importancia en un modelo de red neuronal. La compresi√≥n de matrices puede ayudar a construir modelos m√°s eficientes y reducidos que pueden ser incorporados en dispositivos m√°s peque√±os como tel√©fonos y asistentes del hogar. Enfocarse en la comprensi√≥n por m√©todos como low-rank-approximation y quantization significa que no debemos comprometer la calidad del modelo</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*fpAdJvBIf4SKxF3gTIpe_g.png" alt="" /></p>

<p><em><a href="https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016">Fuente</a></em></p>

<h1 id="educaci√≥n">Educaci√≥nüéì</h1>

<p><strong><em>Fundamentos del NLP</em></strong></p>

<p><br />
Estoy emocionado de liberar un borrador del cap√≠tulo 1 de mi nueva serie llamado <a href="https://medium.com/dair-ai/fundamentals-of-nlp-chapter-1-tokenization-lemmatization-stemming-and-sentence-segmentation-b362c5d07684">Fundamentos del NLP</a>. Ense√±a conceptos comenzando desde lo m√°s b√°sico, compartiendo buenas pr√°cticas, referencias importantes, errores comunes a evitar, y cu√°l es el futuro del NLP. Un <a href="https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ">Colab Notebook</a> est√° incluido y el proyecto ser√° mantenido <a href="https://github.com/dair-ai/nlp_fundamentals">aqu√≠</a></p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*mS5NcoJ_c8hYTjiJsuu_8g.gif" alt="" /></p>

<p><br />
<strong><em>[En l√≠nea] Revisi√≥n/Discusi√≥n: Parte 1 Sesi√≥n de lectura de Fundamentos Matem√°ticos</em></strong></p>

<p><br />
Machine Learning Tokio est√° organizando una discusi√≥n en l√≠nea para revisar los cap√≠tulos que fueron cubiertos en su m√°s reciente sesi√≥n de estudio. El grupo ha estudiado previamente cap√≠tulos del libro <a href="https://mml-book.github.io/">Matem√°ticas para el Machine Learning</a> escrito por Marc Peter Deisenroth, A Aldo Faisal, y Cheng Soon Ong. El <a href="https://www.meetup.com/Machine-Learning-Tokyo/events/268817313/">evento</a> est√° pautado para el 8 de Marzo de 2020.</p>

<p><br />
<strong><em>Libros Recomendados</em></strong></p>

<p><br />
En un segmento previo discutimos la importancia de la compresi√≥n de matrices para construir modelos de ML compactos. Si est√°s interesado en aprender m√°s sobre como construir redes neuronales m√°s peque√±as para sistemas embebidos te recomiendo este gran libro llamado <a href="https://tinymlbook.com/?linkId=82595412">TinyML</a> por Pete Warden y Daniel Situnayake.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/0*omOa3aw2bfMzX2Qm.jpg" alt="" /></p>

<p><em><a href="https://www.amazon.com/TinyML-Learning-TensorFlow-Ultra-Low-Micro-Controllers/dp/1492052043">Fuente</a></em></p>

<p><br />
Otro libro interesante a tener en cuenta es ‚Äú<a href="https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527">Deep Learning para programadores con fastai y pyTorch: Aplicaciones AI sin un PhD</a>‚Äù por Jeremy Howard y Sylvain Gugger. El libro trata de proveer la matem√°tica necesaria para construir y entrenar modelos que resuelvan tareas en el √°rea de visi√≥n por computador y Entendimiento natural de texto.</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/0*i2OmtWOncatOYsZv.jpg" alt="" /></p>

<p><em><a href="https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527">Fuente</a></em></p>

<h1 id="menciones-a-considerar-Ô∏è">Menciones a considerar ‚≠êÔ∏è</h1>

<p>Puedes acceder a las versiones anteriores de este bolet√≠n <a href="https://medium.com/dair-ai/nlp-newsletter-pytorch3d-deepspeed-turing-nlg-question-answering-benchmarks-hydra-sparse-322f018ee096">aqu√≠</a>.</p>

<p><br />
<a href="https://arxiv.org/abs/1909.06576">Torchmeta</a> es una librer√≠a que permite el uso de data loaders para la investigaci√≥n en meta-learning. El creador es Tristan Deleu.</p>

<p><br />
Manuel Tonneau escribi√≥ una <a href="https://creatext.ai/blog-posts/machine-text-writing-gpt2-beam-search?utm_medium=newsletter">pieza</a> ofreciendo una mirada m√°s detallada a alguna de la maquinaria involucrada en el modelamiento del lenguaje. Algunos t√≥picos incluyen b√∫squedas greedy y beam, as√≠ como tambi√©n nucleus sampling.</p>

<p><br />
El MIT <a href="http://introtodeeplearning.com/">liber√≥</a> el programa de estudio completo del curso titulado ‚ÄúIntroducci√≥n al Deep Learning‚Äù, incluye videos de las clases que ya se dictaron. Est√°n apuntando a liberar videos y presentaciones todas las semanas.</p>

<p><br />
Aprende a entrenar un modelo para reconocimiento de entidades nombradas (NER) utilizando una aproximaci√≥n v√≠a Transformers en menos de <a href="https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py">300 l√≠neas de c√≥digo</a>. Puedes encontrar el Google Colab que lo acompa√±a <a href="https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn">aqu√≠</a>.</p>

      <hr />
      <footer role="contentinfo">
        <div class="social-share">
  <!-- Go to www.addthis.com/dashboard to customize your tools --> 
  <div class="addthis_inline_share_toolbox"></div>
  <!--
  <h4>Share on</h4>
  <ul>
    <li>
      <a href="https://twitter.com/intent/tweet?text=https://dair.ai/Bolet%C3%ADn_informativo_NLP_GPT-2_Explicado,_Entendie/" class="twitter" title="Share on Twitter"><i class="fa fa-twitter"></i><span> Twitter</span></a>
    </li>
    <li>
      <a href="https://www.facebook.com/sharer/sharer.php?u=https://dair.ai/Bolet%C3%ADn_informativo_NLP_GPT-2_Explicado,_Entendie/" class="facebook" title="Share on Facebook"><i class="fa fa-facebook"></i><span> Facebook</span></a>
    </li>
    <li>
      <a href="https://plus.google.com/share?url=https://dair.ai/Bolet%C3%ADn_informativo_NLP_GPT-2_Explicado,_Entendie/" class="google-plus" title="Share on Google Plus"><i class="fa fa-google-plus"></i><span> Google+</span></a>
    </li>
  </ul>-->
</div><!-- /.social-share -->
        <p class="byline"><strong>Bolet√≠n informativo NLP #5: GPT-2 Explicado, Entendiendo ‚ÄòSelf-Distillation‚Äô, Haiku, GANILLA, Sparkwiki, √âtica en el NLP, Torchmeta,...</strong> was published on <time datetime="2020-02-23T00:00:00-06:00">February 23, 2020</time>.</p>
        
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dair-ai'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<!--
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
-->

      </footer>
    </div><!-- /.article-wrap -->
  
    <section id="disqus_thread"></section><!-- /#disqus_thread -->
  
  </article>
</div><!-- /#main -->

<div class="footer-wrap">
  
  <div class="related-articles">
  <h4>You might also enjoy <small class="pull-right">(<a href="https://dair.ai/posts/">View all posts</a>)</small></h4>
    <ul>
    
      <li><a href="https://dair.ai/NLP_Newsletter_NLP_7-ZH-.md/" title="NLP ÁÆÄÊä•ÔºàIssue#7Ôºâ: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶">NLP ÁÆÄÊä•ÔºàIssue#7Ôºâ: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶</a></li>
    
      <li><a href="https://dair.ai/NLP_Newsletter_NLP_7/" title="NLP Newsletter: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶">NLP Newsletter: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶</a></li>
    
      <li><a href="https://dair.ai/NLP_Newsletter_-7_-FR/" title="NLP Newsletter [FR] #7: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶">NLP Newsletter [FR] #7: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,‚Ä¶</a></li>
    
    </ul>
    <hr />
  </div><!-- /.related-articles -->
  
  <footer>
    

<span>&copy; 2020 dair.ai. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="http://mademistakes.com/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a> theme.</span>

  </footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="https://dair.ai/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="https://dair.ai/assets/js/scripts.min.js"></script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  var _gaq = _gaq || [];
  var pluginUrl =
 '//www.google-analytics.com/plugins/ga/inpage_linkid.js';
  _gaq.push(['_require', 'inpage_linkid', pluginUrl]);
  _gaq.push(['_setAccount', 'UA-158959084-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
<script async defer src="https://buttons.github.io/buttons.js"></script>


  
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dair-ai'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<!--
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
-->




</body>
</html>
