<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">dair.ai</title>
<generator uri="https://github.com/jekyll/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="https://dair.ai/feed.xml" />
<link rel="alternate" type="text/html" href="https://dair.ai" />
<updated>2020-03-17T18:43:50-05:00</updated>
<id>https://dair.ai/</id>
<author>
  <name>dair.ai</name>
  <uri>https://dair.ai/</uri>
  <email>ellfae@gmail.com</email>
</author>


<entry>
  <title type="html"><![CDATA[NLP 简报（Issue#7）: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,…]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_NLP_7-ZH-.md/" />
  <id>https://dair.ai/NLP_Newsletter_NLP_7[ZH].md</id>
  <published>2020-03-16T00:00:00-05:00</published>
  <updated>2020-03-16T00:00:00-05:00</updated>
  <author>
    <name>kaiyuan</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*9gNslKwKiRaffDt2RSOgoQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
欢迎来到NLP简报第七期。
Welcome to the 7th issue of the NLP Newsletter.希望您今天过得愉快，并希望您和您的亲人在这个困难的日子里平安无事。我们决定发布此新闻通讯，以使我们的读者感到高兴，因此请在有空的时候阅读。现在，让我们继续关注最重要的事情-我们的家人和朋友。 ❤️ 💛 💚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;一些关于NLP简报以及dair.ai的更新&lt;/em&gt;&lt;/strong&gt;
之前每一期NLP简报的法语和中文翻译都已经完成，可以在 &lt;a href=&quot;https://github.com/dair-ai/nlp_newsletter&quot;&gt;这里&lt;/a&gt;查看. &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues/11&quot;&gt;在此&lt;/a&gt;了解如何为NLP新闻通讯的前期和即将发行的翻译做出贡献。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
我们最近建立了两个Github库： &lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;NLP paper summaries&lt;/a&gt; 以及&lt;a href=&quot;https://github.com/dair-ai/pytorch_notebooks&quot;&gt;PyTorch notebooks&lt;/a&gt; 。&lt;/p&gt;

&lt;h1 id=&quot;1research-and-publications-&quot;&gt;1、Research and Publications 📙&lt;/h1&gt;

&lt;h4 id=&quot;11-measuring-compositional-generalization&quot;&gt;1.1 Measuring Compositional Generalization&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
在机器学习的背景下，合成泛化（compositional generalization）是指机器学习从一组训练示例学习上下文表示。 迄今为止，尚不清楚如何正确地测量神经网络中的compositionality。Google AI研究者在 ICLR 2020 上的论文《&lt;a href=&quot;https://ai.googleblog.com/2020/03/measuring-compositional-generalization.html&quot;&gt;Measuring Compositonal Generalization: A Comprehensive Method on Realistic Data&lt;/a&gt;》，提出了使用问题解答和语义解析等任务进行compositional generalization的最大基准之一。 下图显示了该种新模型，使用原子（prodece，direct等）来产生新化合物（即原子的组合）的示例。 这项工作的想法是产生一个训练测试拆分，其中包含共享相似原子（生成示例的构造块）但具有不同化合物分布（原子组成）的示例。 作者声称这是测试compositional generalization的一种更可靠的方法。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*lXmUWOY8HJL7YVn1.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Credit: Google AI Blog&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;12-微调预训练语言模&quot;&gt;1.2 微调预训练语言模&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
研究人员进行了一系列全面的微调试验，以更好地了解权重初始化和早停对语言模型的效果，发表在论文《&lt;a href=&quot;https://arxiv.org/abs/2002.06305&quot;&gt;Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping&lt;/a&gt;》中。 通过涉及对BERT进行数百次微调的各种实验，发现不同的随机种子会产生截然不同的结果。特别是，该研究报告称，一些权重初始化在一组任务中确实表现良好。所有实验数据和试验均已公开发布，供有兴趣进一步了解微调过程中不同动态的其他研究人员使用。&lt;/p&gt;

&lt;h4 id=&quot;13-zoom-in-an-introduction-to-circuits&quot;&gt;1.3 Zoom In: An Introduction to Circuits&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
OpenAI研究人员发表了一篇文章，&lt;a href=&quot;https://distill.pub/2020/circuits/zoom-in/&quot;&gt;Zoom In: An Introduction to Circuits&lt;/a&gt;，讨论了神经网络的可解释性状态，并提出了一种解释神经网络的新方法的建议。受细胞生物学的启发，作者通过检查神经网络的权重深入了解了视觉模型以及他们学到了什么。本质上，该研究提出了他们认为可以为更好地解释神经网络铺平道路的一些主张以及证据。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*i0c-qpiire6dD4IqJVKlYg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;14-nlp-research-highlightsissue-1&quot;&gt;1.4 NLP Research Highlights — Issue #1&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
在dair.ai的新系列&lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-bertology-primer-fastpages-t5-data-science-education-pytorch-notebooks-slow-8ae5d499e040&quot;&gt;NLP Research Highlights&lt;/a&gt;中，详细介绍了当前有趣且重要的NLP研究。通过对这些工作的总性，这将成为跟踪NLP进展的一种方式。在第一季度中，主题涉及从改进语言模型到改进对话代理到最新的语音识别系统。 这些摘要也将保留在&lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;nlp_paper_summaries&lt;/a&gt;中。&lt;/p&gt;

&lt;h4 id=&quot;15用图网络模拟复杂物理&quot;&gt;1.5用图网络模拟复杂物理&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
在过去的几个月中，由于图神经网络（GNN）不仅在NLP中有效，而且在基因组学和材料等其他领域也非常有效，因此我们一直在关注它们。在最近的一篇论文中，《&lt;a href=&quot;https://arxiv.org/abs/2002.09405&quot;&gt;Learning to Simulate Complex Physics with Graph Networks&lt;/a&gt;》，研究人员提出了一种基于图网络的通用框架，该框架能够学习流体和可变形材料等不同领域的模拟。 作者声称他们在不同领域都实现了最先进的性能，他们的通用方法可能是迄今为止学得最好的物理模拟器。 实验包括对材料的模拟，例如在水上滑行以及其他与刚性障碍物的相互作用。 他们还测试了关于分发任务的预训练模型，并找到了可喜的结果，表明该框架已推广到更大的领域。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*48EolUDJoHpYRCTZxgn_qg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2002.09405.pdf&quot;&gt;&lt;em&gt;(Sanchez-Gonzalez et al., 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;16-特定语言bert模型&quot;&gt;1.6 特定语言BERT模型&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
Hugging Face Transformer库中现在提供阿拉伯语BERT（AraBERT）。 你可以访问&lt;a href=&quot;https://huggingface.co/aubmindlab/bert-base-arabert&quot;&gt;AraBERT模型&lt;/a&gt;以及对应的[AraBERT论文(https://arxiv.org/abs/2003.00104);&lt;/p&gt;

&lt;p&gt;最近还发布了&lt;a href=&quot;https://github.com/akirakubo/bert-japanese-aozora&quot;&gt;日语BERT&lt;/a&gt;以及波兰语BERT&lt;a href=&quot;https://github.com/kldarek/polbert&quot;&gt;Polbert&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&quot;2creativity-ethics-and-society-&quot;&gt;2、Creativity, Ethics, and Society 🌎&lt;/h1&gt;

&lt;h4 id=&quot;21-covid-19相关的蛋白质结构的计算预测&quot;&gt;2.1 COVID-19相关的蛋白质结构的计算预测&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind公开与COVID-19相关病毒相关的蛋白质的计算预测结构，&lt;a href=&quot;https://deepmind.com/research/open-source/computational-predictions-of-protein-structures-associated-with-COVID-19&quot;&gt;computational-predictions-of-protein-structures-associated-with-COVID-19&lt;/a&gt;。这些预测是直接从AlphaFold系统获得的，但尚未经过实验验证。该开源的初衷是鼓励为更好地了解该病毒及其功能做出贡献。&lt;/p&gt;

&lt;h4 id=&quot;22-court-cases-that-sound-like-the-weirdest-fights&quot;&gt;2.2 Court cases that sound like the weirdest fights&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
Janelle Shane分享了一个有趣实验的结果，&lt;a href=&quot;https://aiweirdness.com/post/612669075940900864/court-cases-that-sound-like-the-weirdest-fights&quot;&gt;court-cases-that-sound-like-the-weirdest-fights&lt;/a&gt;，其中对GPT-2模型进行了微调以生成针对无生命物体的案例。该模型喂入了一系列政府扣押违禁品或危险品的案例，并生成了如下图所示的案例。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*E5mHmkm1h4VQJ2Ni.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://aiweirdness.com/post/612669075940900864/court-cases-that-sound-like-the-weirdest-fights&quot;&gt;&lt;em&gt;Source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;23-以人为中心的ml框架设计&quot;&gt;2.3 以人为中心的ML框架设计&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
Google AI公布了对使用TensorFlow.js的645人的大规模调查结果，&lt;a href=&quot;https://ai.googleblog.com/2020/03/toward-human-centered-design-for-ml.html&quot;&gt;toward-human-centered-design-for-ml&lt;/a&gt;。 他们旨在从非ML软件开发人员那里了解最重要的功能是什么，以及他们在使用当前ML框架时的总体经验。研究发现包括“缺乏对ML的概念性理解”阻碍了ML框架针对此特定用户集的使用。该研究的参与者还报告了关于如何将ML模型应用于不同问题的需求&lt;/p&gt;

&lt;h4 id=&quot;24-在浏览器中进行面部和手部跟踪&quot;&gt;2.4 在浏览器中进行面部和手部跟踪&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
这篇很棒的TensorFlow文章，&lt;a href=&quot;https://blog.tensorflow.org/2020/03/face-and-hand-tracking-in-browser-with-mediapipe-and-tensorflowjs.html?linkId=83996111&quot;&gt;Toward Human-Centered Design for ML Frameworks&lt;/a&gt;，提供了如何使用TensorFlow.js和MediaPipe在浏览器上启用实时面部和手部跟踪的演练。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*XsRsB-tSOZo9yWOc.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Credit: TensorFlow Blog&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;tools-and-datasets-️&quot;&gt;Tools and Datasets ⚙️&lt;/h1&gt;

&lt;h4 id=&quot;31-nlp-paper-summaries&quot;&gt;3.1 NLP Paper Summaries&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
我们最近创建了一个&lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;nlp_paper_summaries库&lt;/a&gt;，其中包含经过精心挑选的NLP论文摘要列表，这些摘要是过去几年中一些最有趣和最重要的NLP论文。着重于精选重要论文的论文摘要和博客文章，以帮助提高NLP主题和研究的可及性。&lt;/p&gt;

&lt;h4 id=&quot;32-pytorch的计算机视觉库&quot;&gt;3.2 PyTorch的计算机视觉库&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/kornia/kornia&quot;&gt;Kornia&lt;/a&gt; 是建立在PyTorch之上的开源库，它使研究人员可以使用一组运算符来使用PyTorch执行不同的计算机视觉。某些功能包括图像转换，深度估计和低级图像处理等。它在很大程度上受到OpenCV的启发，但不同之处在于，它旨在用于研究，而不是构建可投入生产的应用程序。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*gN_-llcA4_3lIHYE.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;33-diet简介&quot;&gt;3.3 DIET简介&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://blog.rasa.com/introducing-dual-intent-and-entity-transformer-diet-state-of-the-art-performance-on-a-lightweight-architecture/&quot;&gt;DIET（Dual Intent and Entity Transformer）&lt;/a&gt;是Rasa提出的自然语言理解（NLU）多任务体系结构。 该框架着重于多任务训练，以改善意图分类和实体识别方面的结果。 DIET的其他好处包括能够使用任何当前的预训练嵌入，例如BERT和GloVe。重点是要提供一个模型，这些模型可以提高这些任务的当前最新性能，并且训练速度更快（据报道，速度提高了6倍）。 该模型在&lt;a href=&quot;https://rasa.com/docs/rasa/1.8.0/nlu/components/#dietclassifier&quot;&gt;Rasa开源python库&lt;/a&gt;中可用.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*R_8FOU-CVZabv7hJ.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.rasa.com/introducing-dual-intent-and-entity-transformer-diet-state-of-the-art-performance-on-a-lightweight-architecture/?utm_source=twitter&quot;&gt;&lt;em&gt;DIET framework&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;34-迷失在众多bert模型中&quot;&gt;3.4 迷失在众多BERT模型中？&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://bertlang.unibocconi.it/&quot;&gt;BERT Lang Street&lt;/a&gt;是一个简洁的网站，它能够搜索30种基于BERT的模型，其中包含18种语言和28个任务，共177个条目。 例如，如果你想使用BERT模型找出最新的情感分类结果，则可以在搜索栏中搜索“情感”（如下面的图片所示）。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*UuVno2eOAzYb_wlSSfukPA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;35-med7&quot;&gt;3.5 Med7&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
Andrey Kormilitzin发布了&lt;a href=&quot;https://github.com/kormilitzin/med7&quot;&gt;Med7&lt;/a&gt; ，这是一种用于在电子健康记录上执行临床NLP（特别是命名实体识别（NER）任务）的模型。该模型最多可以识别七个类别，并且可以与spaCy库一起使用。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*yOMqhvTwYnxB4LYXv2Mgjg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;36-量子机器学习开源库&quot;&gt;3.6 量子机器学习开源库&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://ai.googleblog.com/2020/03/announcing-tensorflow-quantum-open.html&quot;&gt;TensorFlow Quantum&lt;/a&gt;是一个开放源代码库，提供了用于快速进行量子ML研究原型的工具箱，该工具箱应用ML模型来解决从医学到材料的各种问题。&lt;/p&gt;

&lt;h4 id=&quot;37-快速简便的无限宽网络&quot;&gt;3.7 快速简便的无限宽网络&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/google/neural-tangents&quot;&gt;Neural Tangents&lt;/a&gt;是一个开放源代码库，允许研究人员使用JAX建立和训练无限宽模型和有限神经网络。可以阅读相应地博客获取更多信息，&lt;a href=&quot;https://ai.googleblog.com/2020/03/fast-and-easy-infinitely-wide-networks.html&quot;&gt;fast-and-easy-infinitely-wide-networks&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*CojgKJwB_n_7-j0DJZ0y7g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;articles-and-blog-posts-️&quot;&gt;Articles and Blog posts ✍️&lt;/h1&gt;

&lt;h4 id=&quot;41-从-pytorch-到jax&quot;&gt;4.1 从 PyTorch 到JAX&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
Sabrina J. Mielke发表了一篇文章，&lt;a href=&quot;https://sjmielke.com/jax-purify.htm&quot;&gt;From PyTorch to JAX: towards neural net frameworks that purify stateful code&lt;/a&gt;，其中提供了有关如何使用JAX构建和训练神经网络的演练。 本文着重于在构建神经网络时比较PyTorch和JAX的内部工作原理，这有助于更好地理解JAX的一些优点和区别。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*Nrw4UnmnIZ__elHu.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://sjmielke.com/jax-purify.htm&quot;&gt;&lt;em&gt;Source&lt;/em&gt;&lt;/a&gt; **&lt;/p&gt;

&lt;h4 id=&quot;42-why-do-we-still-use-18-year-old-bleu&quot;&gt;4.2 Why do we still use 18-year old BLEU?***&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
在博客&lt;a href=&quot;https://ehudreiter.com/2020/03/02/why-use-18-year-old-bleu/&quot;&gt; Why do we still use 18-year old BLEU?&lt;/a&gt;中，Ehud Reiter谈到了为什么我们仍然使用BLUE等旧的评估技术进行评估诸如机器翻译之类的任务的NLP模型。作为该领域的研究人员，他还表达了对对较新任务进行评估的技术的含义。&lt;/p&gt;

&lt;h4 id=&quot;43-bart简介&quot;&gt;4.3 BART简介&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1910.13461&quot;&gt;BART&lt;/a&gt;是Facebook提出的一种新模型，其中涉及一种用于对seq2seq模型进行预训练的降噪自动编码器，该模型可以改善下游文本生成任务（如抽象摘要）的性能。 Sam Shleifer提供了&lt;a href=&quot;https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html&quot;&gt;BART的摘要简介&lt;/a&gt;，以及他如何将其集成到Hugging Face Transformers代码库中。&lt;/p&gt;

&lt;h4 id=&quot;44-transformer长程上下文综述&quot;&gt;4.4 Transformer长程上下文综述&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
Madison May最近写了一篇有趣的综述，&lt;a href=&quot;https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/&quot;&gt;A Survey of Long-Term Context in Transformers&lt;/a&gt;，描述了改进基于Transformer的方法，其中包括Sparse Transformers, Adaptive Span Transformers, Transformer-XL, compressive Transformers, Reformer以及routing transformer。&lt;/p&gt;

&lt;h4 id=&quot;45-如何在自动文本编写中控制样式和内容&quot;&gt;4.5 如何在自动文本编写中控制样式和内容&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
尽管自动文本书写在过去的一年中展现了令人印象深刻的表现，但是控制诸如机器书写文本的结构或内容之类的属性仍然具有挑战性。 在最近的博客文章，&lt;a href=&quot;https://creatext.ai/blog-posts/controllable-text-generation&quot;&gt;“Mind your language, GPT-2”: how to control style and content in automatic text writing&lt;/a&gt;中，Manuel Tonneau从Hugging Face的GPT-2讨论了可控文本生成领域的最新进展和观点。 该模型在arXiv上与Google的T5进行了微调，并提到了Salesforce的CTRL和Uber AI的PPLM。&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education 🎓&lt;/h1&gt;

&lt;h4 id=&quot;51-python中nlp的未来发展&quot;&gt;5.1 Python中NLP的未来发展&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
在我们以前的NLP简报中，我们介绍了&lt;a href=&quot;https://thinc.ai/&quot;&gt;THiNC&lt;/a&gt;，这是一个功能深层学习库，致力于与其他现有库的兼容性。 Ines Montani在PyCon哥伦比亚的演讲使用的PPT&lt;a href=&quot;https://speakerdeck.com/inesmontani/the-future-of-nlp-in-python-keynote-pycon-colombia-2020?slide=9&quot;&gt;The Future of NLP in Python&lt;/a&gt;引入了更多的库。&lt;/p&gt;

&lt;h4 id=&quot;52-transformers-notebooks&quot;&gt;5.2 Transformers Notebooks&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
HuggingFace发布了一组&lt;a href=&quot;https://github.com/huggingface/transformers/tree/master/notebooks&quot;&gt;Colab notebooks&lt;/a&gt;，可帮助他们开始使用流行的Transformers库。 一些notebook包括使用令牌化，设置NLP管道以及在自定义数据上训练语言模型。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*0AYHYUsHbaqV2vqN2zCzLQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;53-tensorflow-20免费课程&quot;&gt;5.3 TensorFlow 2.0免费课程&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
在TensorFlow 2.0上查看此&lt;a href=&quot;https://www.freecodecamp.org/news/massive-tensorflow-2-0-free-course/&quot;&gt;〜7小时免费课程&lt;/a&gt; ，其中包含从基本神经网络到NLP到强化学习的介绍。&lt;/p&gt;

&lt;h4 id=&quot;54-deepmind播客&quot;&gt;5.4 DeepMind播客&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind已为其播客发布了所有剧集，&lt;a href=&quot;https://www.youtube.com/playlist?list=PLqYmG7hTraZBiUr6_Qf8YTS2Oqy3OGZEj&quot;&gt;DeepMind: The Podcast&lt;/a&gt;，其中有科学家，研究人员和工程师讨论主题涵盖了AGI 到 神经科学 到 机器人技术。&lt;/p&gt;

&lt;h4 id=&quot;55-mldl课程&quot;&gt;5.5 ML&amp;amp;DL课程&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://sites.google.com/view/berkeley-cs294-158-sp20/home&quot;&gt;Berkeley的“深度无监督学习”课程&lt;/a&gt;已经公开发布整个教学大纲，主要侧重于自我学习的理论方面 监督学习和生成模型。一些主题包括潜在变量模型，自回归模型，流模型和自我监督学习等等，已经有提供YouTube视频和幻灯片。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
我们还发现了有关机器学习，NLP和深度学习的高级在线课程的令人印象深刻的列表，&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/fdw0ax/d_advanced_courses_update/&quot;&gt;d_advanced_courses_update&lt;/a&gt; 。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
这是另一门名为&lt;a href=&quot;https://compstat-lmu.github.io/lecture_i2ml/index.html&quot;&gt;“机器学习入门”&lt;/a&gt;的课程，其中包括诸如监督回归，性能评估，随机森林，参数调整， 实用建议等。&lt;/p&gt;

&lt;h1 id=&quot;noteworthy-mentions-️&quot;&gt;Noteworthy Mentions ⭐️&lt;/h1&gt;

&lt;p&gt;上一期的NLP简报(Issue #6) 可以在&lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-bertology-primer-fastpages-t5-data-science-education-pytorch-notebooks-slow-8ae5d499e040&quot;&gt;这里&lt;/a&gt;查看。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Connon Shorten发表了解释&lt;a href=&quot;https://www.youtube.com/watch?v=QWu7j1nb_jI&amp;amp;feature=emb_logo&quot;&gt;ELECTRA模型的视频&lt;/a&gt;，该模型提出了一种称为 &lt;code class=&quot;highlighter-rouge&quot;&gt;replaced token detection&lt;/code&gt;的技术，可以更有效地对Transformers进行预训练。 如果您有兴趣，我们也在&lt;a href=&quot;https://medium.com/dair-ai/nlp-research-highlights-cd522b21b01a&quot;&gt;此处&lt;/a&gt;写了该模型的简短摘要。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Rachael Tatman正在研究一个名为&lt;a href=&quot;https://www.youtube.com/watch?v=-G36q8_cYsc&amp;amp;feature=emb_logo&quot;&gt;面向开发人员的NLP&lt;/a&gt; 的新系列，其目的是在何时使用NLP的方法进行更深入的讨论,使用它们并解释你可能遇到的常见问题。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind在YouTube上发布了&lt;a href=&quot;https://youtu.be/WXuK6gekU1Y&quot;&gt;AlphaGo-电影&lt;/a&gt;，以庆祝AlphaGo在Go游戏中击败Lee Sedol四周年。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
OpenMined为研究工程师和研究科学家&lt;a href=&quot;https://blog.openmined.org/introducing-openmined-research/&quot;&gt;开放职位&lt;/a&gt;，这是参与保护隐私的AI的好机会。&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;如果您希望在下一期NLP新闻通讯中共享任何数据集，项目，博客文章，教程或论文，请随时通过ellfae@gmail.com或&lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;&lt;strong&gt;DM on Twitter&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Subscribe&lt;/em&gt;&lt;/a&gt;&lt;em&gt;🔖至NLP新闻通讯，以在收件箱中接收以后的新闻。&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_NLP_7-ZH-.md/&quot;&gt;NLP 简报（Issue#7）: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,…&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 16, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,…]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_NLP_7/" />
  <id>https://dair.ai/NLP_Newsletter_NLP_7</id>
  <published>2020-03-16T00:00:00-05:00</published>
  <updated>2020-03-16T00:00:00-05:00</updated>
  <author>
    <name>Elvis Saravia</name>
    <uri>https://dair.ai</uri>
    <email>ellfae@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*9gNslKwKiRaffDt2RSOgoQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Welcome to the 7th issue of the NLP Newsletter. I hope you are having a wonderful day and that you and your loved ones are safe in these difficult times. We decided to publish this newsletter to bring some joy to our readers so please read when you have free time. For now, let’s keep focused on the things that are of top priority— our families and friends. ❤️ 💛 💚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;A few updates about the NLP Newsletter and dair.ai&lt;/em&gt;&lt;/strong&gt;
All French and Chinese translations for the previous issues of the NLP Newsletter are now &lt;a href=&quot;https://github.com/dair-ai/nlp_newsletter&quot;&gt;available&lt;/a&gt;. Find out how you can contribute to the translation of previous and upcoming issues of the NLP Newsletter at this &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues/11&quot;&gt;link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
We recently created two GitHub repositories that contain &lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;NLP paper summaries&lt;/a&gt; and &lt;a href=&quot;https://github.com/dair-ai/pytorch_notebooks&quot;&gt;PyTorch notebooks&lt;/a&gt; to get you started with neural networks.&lt;/p&gt;

&lt;h1 id=&quot;research-and-publications-&quot;&gt;Research and Publications 📙&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Measuring Compositional Generalization&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In the context of machine learning, compositional generalization is the ability to learn to represent meaning and in turn sequences (novel combinations) from what’s learned in the training set. To this date, it is not clear how to properly measure compositionality in neural networks. A Google AI team &lt;a href=&quot;https://ai.googleblog.com/2020/03/measuring-compositional-generalization.html&quot;&gt;proposes&lt;/a&gt; one of the largest benchmarks for compositional generalization using tasks such as question answering and semantic parsing. The picture below shows an example of the proposed model using atoms (produce, direct, etc.) to produce novel compounds, i.e., combinations of atoms. The idea of this work is to produce a train-test split that contains examples that share similar atoms (building blocks to generate examples) distribution but different compound distribution (the composition of atoms). The authors claim that is a more reliable way to test for compositional generalization.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*lXmUWOY8HJL7YVn1.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Credit: Google AI Blog&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Researchers ran a comprehensive &lt;a href=&quot;https://arxiv.org/abs/2002.06305&quot;&gt;set of fine-tuning trials&lt;/a&gt; to better understand the effect of weight initialization and early stopping in the performance of language models. Through various experiments that involved fine-tuning BERT hundreds of times, it was found that distinct random seeds produce very different results. In particular, the study reports that some weight initialization does perform well across a set of tasks. All the experimental data and trials were publicly released for other researchers that are interested in further understanding different dynamics during fine-tuning.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Zoom In: An Introduction to Circuits&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
OpenAI researchers published a &lt;a href=&quot;https://distill.pub/2020/circuits/zoom-in/&quot;&gt;piece&lt;/a&gt; discussing the state of interpretability of neural networks and the proposal of a new approach to interpreting them. Inspired by cellular biology, the authors delve deep into understanding vision models and what they learn by inspecting the weights of neural networks. Essentially, the study presented a few claims along with collected evidence that they believe could pave the way to better interpret neural networks.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*i0c-qpiire6dD4IqJVKlYg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;NLP Research Highlights — Issue #1&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In a new dair.ai series called &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-bertology-primer-fastpages-t5-data-science-education-pytorch-notebooks-slow-8ae5d499e040&quot;&gt;NLP Research Highlights&lt;/a&gt;, we provide detailed descriptions of current interesting and important NLP research. This will serve as a way to keep track of NLP progress via approachable summaries of these works. In the first quarterly issue, topics range from improving language models to improving conversational agents to state-of-the-art speech recognition systems. These summaries will also be maintained &lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Learning to Simulate Complex Physics with Graph Networks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In the past few months, we have been featuring a lot about Graph Neural Networks (GNNs) due to their effectiveness not only in NLP but in other areas such as genomics and materials. In a recent &lt;a href=&quot;https://arxiv.org/abs/2002.09405&quot;&gt;paper&lt;/a&gt;, researchers propose a general framework based on graph networks that is able to learn simulations in different domains such as fluids and deformable materials. The authors claim that they achieve state-of-the-art performance across different domains and that their general-purpose approach is potentially the best-learned physics simulator to date. Experiments include the simulation of materials such as goop over water and other interactions with rigid obstacles. They also tested a pre-trained model on out-of-distribution tasks and found promising results that show the generalization of the framework to larger domains.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*48EolUDJoHpYRCTZxgn_qg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2002.09405.pdf&quot;&gt;&lt;em&gt;(Sanchez-Gonzalez et al., 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Language-specific BERT models&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Arabic BERT (AraBERT) is now available in the Hugging Face Transformer library. You can access the model &lt;a href=&quot;https://huggingface.co/aubmindlab/bert-base-arabert&quot;&gt;here&lt;/a&gt; and the paper &lt;a href=&quot;https://arxiv.org/abs/2003.00104&quot;&gt;here&lt;/a&gt;. Recently, a Japanese version of BERT was also &lt;a href=&quot;https://github.com/akirakubo/bert-japanese-aozora&quot;&gt;released&lt;/a&gt;. And there is also a Polish version of BERT called &lt;a href=&quot;https://github.com/kldarek/polbert&quot;&gt;Polbert&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;creativity-ethics-and-society-&quot;&gt;Creativity, Ethics, and Society 🌎&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Computational predictions of protein structures associated with COVID-19&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind releases &lt;a href=&quot;https://deepmind.com/research/open-source/computational-predictions-of-protein-structures-associated-with-COVID-19&quot;&gt;computationally-predicted structures&lt;/a&gt; for proteins linked with the virus related to COVID-19. The predictions are directly obtained from the AlphaFold systems but haven’t been experimentally verified. The idea with this release is to encourage contributions that aim to better understand the virus and how it functions.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Court cases that sound like the weirdest fights&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Janelle Shane shares the &lt;a href=&quot;https://aiweirdness.com/post/612669075940900864/court-cases-that-sound-like-the-weirdest-fights&quot;&gt;results&lt;/a&gt; of a fun experiment where a GPT-2 model is fine-tuned to generate cases against inanimate objects. The model was fed a list of cases where the government was seizing contraband or dangerous goods and it generated cases like the ones shown in the picture below.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*E5mHmkm1h4VQJ2Ni.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://aiweirdness.com/post/612669075940900864/court-cases-that-sound-like-the-weirdest-fights&quot;&gt;&lt;em&gt;Source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Toward Human-Centered Design for ML Frameworks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Google AI &lt;a href=&quot;https://ai.googleblog.com/2020/03/toward-human-centered-design-for-ml.html&quot;&gt;published&lt;/a&gt; the results of a large-scale survey of 645 people who used TensorFlow.js. They aimed to find out from non-ML software developers what are the most important features and their overall experience with using current ML frameworks. Findings include that the “lack of conceptual understanding of ML” hinders the use of ML frameworks for this particular set of users. Participants in the study also reported the need for better instructions on how to apply the ML models to different problems and more explicit support for modification.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Face and hand tracking in the browser with MediaPipe and TensorFlow.js&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
This awesome &lt;a href=&quot;https://blog.tensorflow.org/2020/03/face-and-hand-tracking-in-browser-with-mediapipe-and-tensorflowjs.html?linkId=83996111&quot;&gt;TensorFlow article&lt;/a&gt; provides a walkthrough of how to enable real-time face and hand tracking on the browser using TensorFlow.js and MediaPipe.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*XsRsB-tSOZo9yWOc.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Credit: TensorFlow Blog&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;tools-and-datasets-️&quot;&gt;Tools and Datasets ⚙️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;NLP Paper Summaries&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
We recently created a &lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;repository&lt;/a&gt; containing a list of carefully curated NLP paper summaries for some of the most interesting and important NLP papers in the past few years. The focus is to feature paper summaries and blog posts of important papers to help improve the approachability and accessibility of NLP topics and research.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;A differentiable computer vision library for PyTorch.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/kornia/kornia&quot;&gt;Kornia&lt;/a&gt; is an open-source library built on top of PyTorch that allows researchers to use a set of operators for performing differentiable computer vision using PyTorch. Some capabilities include image transformations, depth estimation, and low-level image processing, to name a few. It is heavily inspired by OpenCV but the difference is that it is meant to be used for research as opposed to building production-ready applications.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*gN_-llcA4_3lIHYE.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Introducing DIET: state-of-the-art architecture that outperforms fine-tuning BERT and is 6X faster to train&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DIET (Dual Intent and Entity Transformer) is a natural language understanding (NLU) multitask architecture &lt;a href=&quot;https://blog.rasa.com/introducing-dual-intent-and-entity-transformer-diet-state-of-the-art-performance-on-a-lightweight-architecture/&quot;&gt;proposed&lt;/a&gt; by Rasa. The framework focuses on multitask training to improve results on both intent classification and entity recognition. Other benefits of DIET include the ability to use any of the current pre-trained embeddings such as BERT and GloVe. However, the focus was to provide a model that improves the current state-of-the-art performance on those tasks and is faster to train (6X speedup reported). The model is available in the &lt;a href=&quot;https://rasa.com/docs/rasa/1.8.0/nlu/components/#dietclassifier&quot;&gt;Rasa Open Source python library&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*R_8FOU-CVZabv7hJ.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.rasa.com/introducing-dual-intent-and-entity-transformer-diet-state-of-the-art-performance-on-a-lightweight-architecture/?utm_source=twitter&quot;&gt;&lt;em&gt;DIET framework&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Lost in (language-specific) BERT models?&lt;/em&gt;&lt;/strong&gt;
&lt;a href=&quot;https://bertlang.unibocconi.it/&quot;&gt;BERT Lang Street&lt;/a&gt; is a neat website that provides the ability to search over 30 BERT-based models with 18 languages and 28 tasks with a total of 177 entries. For instance, if you wanted to find out the state-of-the-art results for sentiment classification using BERT models, you can just search for “sentiment” in the search bar (example shown in the screenshot below).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*UuVno2eOAzYb_wlSSfukPA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Med7&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrey Kormilitzin releases &lt;a href=&quot;https://github.com/kormilitzin/med7&quot;&gt;Med7&lt;/a&gt; which is a model for performing clinical NLP (in particular named entity recognition (NER) tasks) on electronic health records. The model can identify up to seven categories and is available for use with the spaCy library.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*yOMqhvTwYnxB4LYXv2Mgjg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;An Open Source Library for Quantum Machine Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://ai.googleblog.com/2020/03/announcing-tensorflow-quantum-open.html&quot;&gt;TensorFlow Quantum&lt;/a&gt; is an open-source library that provides a toolbox for rapid prototyping of quantum ML research that allows the application of ML models to approach problems ranging from medicine to materials.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Fast and Easy Infinitely Wide Networks with Neural Tangents&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Neural Tangents is an open-source library that allows researchers to build and train infinite-width models and finite neural networks using JAX. Read the blog post of the release &lt;a href=&quot;https://ai.googleblog.com/2020/03/fast-and-easy-infinitely-wide-networks.html&quot;&gt;here&lt;/a&gt; and get access to the library &lt;a href=&quot;https://github.com/google/neural-tangents&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*CojgKJwB_n_7-j0DJZ0y7g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;articles-and-blog-posts-️&quot;&gt;Articles and Blog posts ✍️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;From PyTorch to JAX: towards neural net frameworks that purify stateful code&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sabrina J. Mielke published an &lt;a href=&quot;https://sjmielke.com/jax-purify.htm&quot;&gt;article&lt;/a&gt; that provides a walkthrough of how to build and train neural networks using JAX. The article focuses on comparing the inner workings of PyTorch and JAX when building neural networks, which helps to better understand some of the benefits and differences of JAX.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*Nrw4UnmnIZ__elHu.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://sjmielke.com/jax-purify.htm&quot;&gt;&lt;em&gt;Source&lt;/em&gt;&lt;/a&gt; **&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Why do we still use 18-year old BLEU?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In this &lt;a href=&quot;https://ehudreiter.com/2020/03/02/why-use-18-year-old-bleu/&quot;&gt;blog post&lt;/a&gt;, Ehud Reiter talks about why we still use old evaluation techniques like BLUE for evaluating NLP models for tasks like machine translation. As a researcher in the space, he also expresses the implications for techniques that perform the evaluation on more recent tasks.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Introducing BART&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1910.13461&quot;&gt;BART&lt;/a&gt; is a new model proposed by Facebook that involves a denoising autoencoder for pretraining seq2seq models that improve performance on downstream text generation tasks such as abstractive summarization. Sam Shleifer provides a &lt;a href=&quot;https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html&quot;&gt;nice summary&lt;/a&gt; of BART and how he integrated it into the Hugging Face Transformers repo.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;A Survey of Long-Term Context in Transformers&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Madison May recently wrote an interesting &lt;a href=&quot;https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/&quot;&gt;survey&lt;/a&gt; describing ways to improve Transformer based approaches, which include Sparse Transformers, Adaptive Span Transformers, Transformer-XL, compressive Transformers, Reformer, and routing transformer. We also touched on some of these topics in the dair.ai &lt;a href=&quot;https://medium.com/dair-ai&quot;&gt;publication&lt;/a&gt; and in this list of &lt;a href=&quot;https://medium.com/dair-ai/nlp-research-highlights-cd522b21b01a&quot;&gt;paper summaries&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;“Mind your language, GPT-2”: how to control style and content in automatic text writing&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Despite the impressive fluency automatic text writing has exhibited in the past year, it is still challenging to control attributes like structure or content of the machine-written text. In a &lt;a href=&quot;https://creatext.ai/blog-posts/controllable-text-generation&quot;&gt;recent blog post&lt;/a&gt;, Manuel Tonneau discusses the recent progress and the perspectives in the field of controllable text generation, from Hugging Face’s GPT-2 model fine-tuned on arXiv to Google’s T5, with mentions of Salesforce’s CTRL and Uber AI’s PPLM.&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education 🎓&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Talk: The Future of NLP in Python&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In one of our previous newsletters, we featured &lt;a href=&quot;https://thinc.ai/&quot;&gt;THiNC&lt;/a&gt; which is a functional deep learning library focused on compatibility with other existing libraries. This &lt;a href=&quot;https://speakerdeck.com/inesmontani/the-future-of-nlp-in-python-keynote-pycon-colombia-2020?slide=9&quot;&gt;set of slides&lt;/a&gt; introduces a bit more of the library which was used in the talk by Ines Montani for PyCon Colombia.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Transformers Notebooks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
HuggingFace published a set of &lt;a href=&quot;https://github.com/huggingface/transformers/tree/master/notebooks&quot;&gt;Colab notebooks&lt;/a&gt; that help to get started with their popular Transformers library. Some notebooks include using tokenization, setting up NLP pipelines, and training a language model on custom data.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*0AYHYUsHbaqV2vqN2zCzLQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;TensorFlow 2.0 in 7 hours&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Check out this &lt;a href=&quot;https://www.freecodecamp.org/news/massive-tensorflow-2-0-free-course/&quot;&gt;~7-hour free course&lt;/a&gt; on TensorFlow 2.0 containing topics that range from basic neural networks to NLP with RNNs to an introduction to reinforcement learning.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;DeepMind: The Podcast&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind has released all episodes (in the form of a &lt;a href=&quot;https://www.youtube.com/playlist?list=PLqYmG7hTraZBiUr6_Qf8YTS2Oqy3OGZEj&quot;&gt;YouTube playlist&lt;/a&gt;) for their podcast which features scientists, researchers, and engineers discussing topics that range from AGI to neuroscience to robotics.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Machine Learning and Deep Learning Courses&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Berkeley is publicly releasing the &lt;a href=&quot;https://sites.google.com/view/berkeley-cs294-158-sp20/home&quot;&gt;entire syllabus&lt;/a&gt; for its course on “Deep Unsupervised Learning” mainly focusing on the theoretical aspects of self-supervised learning and generative models. Some topics include latent variable models, autoregressive models, flow models, and self-supervised learning, to name a few. Youtube videos and slides are available.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
We also found this &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/fdw0ax/d_advanced_courses_update/&quot;&gt;impressive list&lt;/a&gt; of advanced online courses on machine learning, NLP and deep learning.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
And here is another course called &lt;a href=&quot;https://compstat-lmu.github.io/lecture_i2ml/index.html&quot;&gt;“Introduction to Machine Learning&lt;/a&gt;” which includes topics such as supervised regression, performance evaluation, random forests, parameter tuning, practical advice, and much more.&lt;/p&gt;

&lt;h1 id=&quot;noteworthy-mentions-️&quot;&gt;Noteworthy Mentions ⭐️&lt;/h1&gt;

&lt;p&gt;The previous NLP Newsletter (Issue #6) is available &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-bertology-primer-fastpages-t5-data-science-education-pytorch-notebooks-slow-8ae5d499e040&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Connon Shorten published a &lt;a href=&quot;https://www.youtube.com/watch?v=QWu7j1nb_jI&amp;amp;feature=emb_logo&quot;&gt;video&lt;/a&gt; explaining the ELECTRA model which proposes a technique called replaced token detection to pre-train Transformers more efficiently. If you are interested, we also wrote a short summary of the model &lt;a href=&quot;https://medium.com/dair-ai/nlp-research-highlights-cd522b21b01a&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Rachael Tatman is working on a new series called &lt;a href=&quot;https://www.youtube.com/watch?v=-G36q8_cYsc&amp;amp;feature=emb_logo&quot;&gt;NLP for Developers&lt;/a&gt; where the idea is to talk more in-depth about different NLP methods, when to use them and explaining common issues that you may run into.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind releases &lt;a href=&quot;https://youtu.be/WXuK6gekU1Y&quot;&gt;AlphaGo — The Movie&lt;/a&gt; on YouTube to celebrate the 4th anniversary of AlphaGo beating Lee Sedol at the game of Go.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
OpenMined has &lt;a href=&quot;https://blog.openmined.org/introducing-openmined-research/&quot;&gt;open positions&lt;/a&gt; for Research Engineer and Research Scientist roles which is a good opportunity to get involved with privacy-preserving AI.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;If you have any datasets, projects, blog posts, tutorials, or papers that you wish to share in the next iteration of the NLP Newsletter, please free to reach out to me at ellfae@gmail.com or &lt;em&gt;**&lt;/em&gt;&lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;&lt;strong&gt;DM on Twitter&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Subscribe&lt;/em&gt;&lt;/a&gt; &lt;em&gt;🔖 to the NLP Newsletter to receive future issues in your inbox.&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_NLP_7/&quot;&gt;NLP Newsletter: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,…&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 16, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter [FR] #7: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,…]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_-7_-FR/" />
  <id>https://dair.ai/NLP_Newsletter_#7_[FR]</id>
  <published>2020-03-16T00:00:00-05:00</published>
  <updated>2020-03-16T00:00:00-05:00</updated>
  <author>
    <name>Loïck BOURDOIS</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*9gNslKwKiRaffDt2RSOgoQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;avant-propos-delvis&quot;&gt;Avant-propos d’Elvis&lt;/h1&gt;
&lt;p&gt;&lt;br /&gt;
Bienvenue au 7e numéro de la lettre d’information consacrée au NLP. J’espère que vous passez une merveilleuse journée et que vous et vos proches êtes en sécurité en ces temps difficiles. Nous avons décidé de publier ce bulletin pour apporter un peu de joie à nos lecteurs, alors n’hésitez pas à le lire quand vous aurez du temps libre. Pour l’instant, concentrons-nous sur les choses qui sont de la plus haute priorité : nos familles et nos amis. ❤️ 💛 💚&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Quelques mises à jour sur la lettre d’information sur le NLP et sur dair.ai.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Les traductions françaises et chinoises de tous les numéros précédents de la newsletter sont désormais &lt;a href=&quot;https://github.com/dair-ai/nlp_newsletter&quot;&gt;disponibles&lt;/a&gt;. Découvrez comment vous pouvez contribuer à la traduction des numéros précédents et à venir en cliquant sur ce &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues/11&quot;&gt;lien&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Nous avons récemment créé deux nouveaux dépôts GitHub qui contiennent des &lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;résumés de publications sur le NLP&lt;/a&gt; et des &lt;a href=&quot;https://github.com/dair-ai/pytorch_notebooks&quot;&gt;notebooks PyTorch&lt;/a&gt; pour vous aider à démarrer avec les réseaux de neurones.&lt;/p&gt;

&lt;h1 id=&quot;publications-&quot;&gt;Publications 📙&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Mesure de la généralisation de la composition&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans le contexte de l’apprentissage machine, la généralisation de la composition est la capacité d’apprendre à représenter le sens et des séquences (combinaisons inédites) à partir de ce qui est appris dans le jeu d’entraînement. À ce jour, la manière de mesurer correctement la composition dans les réseaux neuronaux n’est pas claire. Une équipe de Google AI &lt;a href=&quot;https://ai.googleblog.com/2020/03/measuring-compositional-generalization.html&quot;&gt;propose&lt;/a&gt; l’un des plus grands benchmarks pour la généralisation de la composition en utilisant des tâches telles que le question/ answering et l’analyse sémantique. L’image ci-dessous montre un exemple du modèle proposé utilisant des atomes (produire, diriger, etc.) pour produire de nouveaux composés, c’est-à-dire des combinaisons d’atomes. L’idée de ce travail est de produire des échantillons entraînement/test qui contiennent des exemples qui partagent des atomes similaires (blocs de construction pour générer des exemples) de distribution mais une distribution de composés différente (la composition des atomes). Les auteurs affirment que c’est une façon plus fiable de tester la généralisation de la composition.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*lXmUWOY8HJL7YVn1.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Credit: Google AI Blog&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Fine-Tuning de modèles linguistiques pré-entraînés : Initialisation des poids, ordonnancement des données et arrêt anticipé&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Des chercheurs ont mené une &lt;a href=&quot;https://arxiv.org/abs/2002.06305&quot;&gt;série d’essais de fine-tuning&lt;/a&gt; pour mieux comprendre l’effet de l’initialisation du poids et de l’arrêt précoce dans la performance des modèles. Au cours de diverses expériences qui ont nécessité des centaines de tunage de BERT, il a été constaté que des graines aléatoires distinctes donnent des résultats très différents. En particulier, l’étude indique qu’une certaine initialisation des poids donne de bons résultats pour un ensemble de tâches. Toutes les données expérimentales et les essais ont été rendus publics pour les autres chercheurs qui souhaitent mieux comprendre les différentes dynamiques lors de la mise au point.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Une introduction aux circuits&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Les chercheurs d’OpenAI ont publié un &lt;a href=&quot;https://distill.pub/2020/circuits/zoom-in/&quot;&gt;article&lt;/a&gt; sur l’interprétabilité des réseaux de neurones et proposent une nouvelle approche pour les interpréter. Inspirés par la biologie cellulaire, les auteurs approfondissent la compréhension des modèles de vision et de ce qu’ils apprennent en inspectant le poids des réseaux neuronaux. Essentiellement, l’étude présente quelques affirmations ainsi que des preuves recueillies qui, selon eux, pourraient ouvrir la voie à une meilleure interprétation des réseaux neuronaux.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*i0c-qpiire6dD4IqJVKlYg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;NLP Research Highlights — Issue #1&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans une nouvelle série de dair.ai intitulée &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-bertology-primer-fastpages-t5-data-science-education-pytorch-notebooks-slow-8ae5d499e040&quot;&gt;NLP Research Highlights&lt;/a&gt;, nous fournissons des descriptions détaillées des recherches actuelles intéressantes et importantes sur le NLP. Dans le premier numéro trimestriel, les sujets vont de l’amélioration des modèles de langage à l’amélioration des agents conversationnels en passant par les systèmes de reconnaissance vocale de pointe. Ces résumés seront également mis à jour &lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Apprendre à simuler la physique complexe avec les réseaux de graphes&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ces derniers mois, nous avons beaucoup parlé des réseaux de neurones de graphes (GNN) en raison de leur efficacité non seulement en NLP mais aussi dans d’autres domaines tels que la génomique et les matériaux. Dans un récent &lt;a href=&quot;https://arxiv.org/abs/2002.09405&quot;&gt;article&lt;/a&gt;, des chercheurs proposent un cadre général basé sur les réseaux de graphes qui est capable d’apprendre des simulations dans différents domaines tels que les fluides et les matériaux déformables. Les auteurs affirment qu’ils obtiennent des performances de pointe dans différents domaines et que leur approche générale est potentiellement le simulateur de physique le mieux appris à ce jour. Les expériences comprennent la simulation de matériaux tels que le « goop over water » (je ne sais pas comment cela se traduit en français) et d’autres interactions avec des obstacles rigides. Ils ont également testé un modèle pré-entraîné sur des tâches hors distribution et ont trouvé des résultats prometteurs qui montrent la généralisation du framework à des domaines plus vastes.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*48EolUDJoHpYRCTZxgn_qg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2002.09405.pdf&quot;&gt;&lt;em&gt;(Sanchez-Gonzalez et al., 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Modèles BERT spécifiques à chaque langue&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Le BERT en arabe (AraBERT) est maintenant disponible dans la librairie Transformer. Vous pouvez accéder au modèle &lt;a href=&quot;https://huggingface.co/aubmindlab/bert-base-arabert&quot;&gt;ici&lt;/a&gt; et au document &lt;a href=&quot;https://arxiv.org/abs/2003.00104&quot;&gt;ici&lt;/a&gt;. Récemment, une version japonaise de BERT a également été &lt;a href=&quot;https://github.com/akirakubo/bert-japanese-aozora&quot;&gt;publiée&lt;/a&gt;. Il existe également une version polonaise de BERT appelée &lt;a href=&quot;https://github.com/kldarek/polbert&quot;&gt;Polbert&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;créativité-éthique-et-société-&quot;&gt;Créativité, éthique et société 🌎&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Prévisions des structures protéiques associées au COVID-19&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind dévoile &lt;a href=&quot;https://deepmind.com/research/open-source/computational-predictions-of-protein-structures-associated-with-COVID-19&quot;&gt;des structures prédites par calcul&lt;/a&gt; pour les protéines liées au COVID-19. Les prédictions sont directement obtenues à partir des systèmes AlphaFold mais n’ont pas été vérifiées expérimentalement. L’idée de cette publication est d’encourager les contributions qui visent à mieux comprendre le virus et son fonctionnement.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Utilisation du GPT2 pour une expérience sur des cas judiciaires&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Janelle Shane partage les &lt;a href=&quot;https://aiweirdness.com/post/612669075940900864/court-cases-that-sound-like-the-weirdest-fights&quot;&gt;résultats&lt;/a&gt; d’une expérience amusante où un modèle GPT-2 est fine-tuné pour générer des cas contre des objets inanimés. Le modèle a été alimenté par une liste de cas où le gouvernement saisissait des marchandises de contrebande ou dangereuses. Cela a généré des cas comme ceux présentés dans l’image ci-dessous.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*E5mHmkm1h4VQJ2Ni.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://aiweirdness.com/post/612669075940900864/court-cases-that-sound-like-the-weirdest-fights&quot;&gt;&lt;em&gt;Source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Vers une conception centrée sur l’homme des frameworks de ML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Google AI &lt;a href=&quot;https://ai.googleblog.com/2020/03/toward-human-centered-design-for-ml.html&quot;&gt;a publié&lt;/a&gt; les résultats d’une enquête à grande échelle menée auprès de 645 personnes ayant utilisé TensorFlow.js. L’objectif était de connaître les caractéristiques les plus importantes ainsi que l’expérience générale des développeurs de logiciels non ML testant des frameworks de ML actuels.
Les résultats montrent notamment que le “manque de compréhension conceptuelle du ML” entrave l’utilisation des frameworks de ML pour cet ensemble d’utilisateurs. Les participants à l’étude ont également signalé le besoin de meilleures instructions sur la façon d’appliquer les modèles de ML à différents problèmes et d’un soutien plus explicite pour les modifications.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Tracking du visage et de la main avec MediaPipe et TensorFlow.js&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Cet &lt;a href=&quot;https://blog.tensorflow.org/2020/03/face-and-hand-tracking-in-browser-with-mediapipe-and-tensorflowjs.html?linkId=83996111&quot;&gt;article sur TensorFlow&lt;/a&gt; explique comment activer le suivi des visages et des mains en temps réel à l’aide de TensorFlow.js et de MediaPipe.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*XsRsB-tSOZo9yWOc.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Credit: TensorFlow Blog&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;outils-et-jeux-de-données-️&quot;&gt;Outils et jeux de données ⚙️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;NLP Paper Summaries&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Nous avons récemment créé un &lt;a href=&quot;https://github.com/dair-ai/nlp_paper_summaries&quot;&gt;dépôt&lt;/a&gt; contenant des résumés des publications de NLP les intéressantes et les plus importantes de ces dernières années. L’objectif est d’améliorer l’accessibilité de ces recherches et sujets.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Une librairie de vision par ordinateur pour PyTorch&lt;/em&gt;&lt;/strong&gt;
&lt;br /&gt;
&lt;a href=&quot;https://github.com/kornia/kornia&quot;&gt;Kornia&lt;/a&gt; est une librairie open-source construite sur PyTorch qui permet aux chercheurs d’utiliser un ensemble d’opérateurs pour réaliser une vision informatique différenciée en utilisant PyTorch. Parmi les fonctionnalités, on trouve les transformations d’images, l’estimation de la profondeur et le traitement d’images de bas niveau, pour n’en citer que quelques-unes. Elle est fortement inspirée d’OpenCV, mais la différence est qu’elle est destinée à la recherche plutôt qu’à la création d’applications prêtes à la production.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*gN_-llcA4_3lIHYE.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Présentation de DIET : une architecture qui surpasse le fine-tuning de BERT et qui est 6X plus rapide à entraîner&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DIET (Dual Intent and Entity Transformer) est une architecture multitâche de compréhension du langage naturel (NLU) &lt;a href=&quot;https://blog.rasa.com/introducing-dual-intent-and-entity-transformer-diet-state-of-the-art-performance-on-a-lightweight-architecture/&quot;&gt;proposée&lt;/a&gt; par Rasa. Le framework se concentre sur l’entraînement multitâche afin d’améliorer les résultats en matière de classification des intentions et de reconnaissance des entités. Un autre avantage de DIET est que l’on peut utiliser n’importe quel élément pré entrainé tels que BERT et GloVe. L’objectif principale de cette librairie est de fournir un modèle qui améliore les performances actuelles de ces tâches et qui est plus rapide à entraîner (accélération de 6X). Le modèle est disponible dans la &lt;a href=&quot;https://rasa.com/docs/rasa/1.8.0/nlu/components/#dietclassifier&quot;&gt;librairie python Rasa Open Source&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*R_8FOU-CVZabv7hJ.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.rasa.com/introducing-dual-intent-and-entity-transformer-diet-state-of-the-art-performance-on-a-lightweight-architecture/?utm_source=twitter&quot;&gt;&lt;em&gt;DIET framework&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Perdu parmi toutes les langues disponibles pour BERT ?&lt;/em&gt;&lt;/strong&gt;
&lt;a href=&quot;https://bertlang.unibocconi.it/&quot;&gt;BERT Lang Street&lt;/a&gt; est un site web qui offre la possibilité de rechercher plus de 30 modèles basés sur le BERT, en 18 langues et 28 tâches, soit un total de 177 entrées. Par exemple, si vous souhaitez connaître les résultats de la classification des sentiments à l’aide des modèles de BERT, il vous suffit de rechercher “sentiment” dans la barre de recherche (exemple illustré dans la capture d’écran ci-dessous).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*UuVno2eOAzYb_wlSSfukPA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Med7&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrey Kormilitzin publie &lt;a href=&quot;https://github.com/kormilitzin/med7&quot;&gt;Med7&lt;/a&gt; qui est un modèle pour du NLP à usage clinique (en particulier les tâches de reconnaissance d’entités nommées (NER)) sur les dossiers médicaux électroniques. Le modèle peut identifier jusqu’à sept catégories et est disponible pour être utilisé avec la bibliothèque spaCy.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*yOMqhvTwYnxB4LYXv2Mgjg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Une bibliothèque open source pour le ML quantique&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://ai.googleblog.com/2020/03/announcing-tensorflow-quantum-open.html&quot;&gt;TensorFlow Quantum&lt;/a&gt; est une bibliothèque open-source qui fournit une boîte à outils pour le prototypage rapide de la recherche en ML quantique qui permet l’application de modèles de ML pour aborder des problèmes allant de la médecine aux matériaux.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Des réseaux infiniment larges, rapides et faciles, avec Neural Tangents&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Neural Tangents est une librairie open-source qui permet aux chercheurs de d’entraîner des modèles de largeur finie ou infinie en utilisant JAX. Lisez l’article du blog de la version &lt;a href=&quot;https://ai.googleblog.com/2020/03/fast-and-easy-infinitely-wide-networks.html&quot;&gt;ici&lt;/a&gt; et accédez à la librairie &lt;a href=&quot;https://github.com/google/neural-tangents&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*CojgKJwB_n_7-j0DJZ0y7g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;articles-et-blog-️&quot;&gt;Articles et Blog ✍️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;De PyTorch à JAX&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sabrina J. Mielke a publié un &lt;a href=&quot;https://sjmielke.com/jax-purify.htm&quot;&gt;article&lt;/a&gt; qui explique comment construire et entraîner des réseaux de neurones en utilisant JAX. L’article se concentre sur la comparaison du fonctionnement interne de PyTorch et de JAX lors de la construction de réseaux neuronaux, ce qui permet de mieux comprendre certains des avantages et des différences de JAX.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*Nrw4UnmnIZ__elHu.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://sjmielke.com/jax-purify.htm&quot;&gt;&lt;em&gt;Source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Pourquoi utilisons-nous encore des jeux de données test créés il y a plus de 18 ans ?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans cet &lt;a href=&quot;https://ehudreiter.com/2020/03/02/why-use-18-year-old-bleu/&quot;&gt;article de blog&lt;/a&gt;, Ehud Reiter explique pourquoi nous utilisons encore de vieilles techniques d’évaluation comme BLUE pour évaluer les modèles de NLP pour des tâches comme la traduction automatique&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Introduction à BART&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1910.13461&quot;&gt;BART&lt;/a&gt; est un modèle proposé par Facebook qui implique un autoencodeur de débruitage pour le pré-entraînement de modèles seq2seq, ce qui améliorent les performances sur les tâches de génération de texte telles que le résumé abstrait. Sam Shleifer fournit un &lt;a href=&quot;https://sshleifer.github.io/blog_v2/jupyter/2020/03/12/bart.html&quot;&gt;résumé&lt;/a&gt; de BART et explique comment il l’a intégré dans la librairie Transformers de Hugging Face.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Améliorations des Transformers&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Madison May a récemment rédigé une &lt;a href=&quot;https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/&quot;&gt;enquête&lt;/a&gt; décrivant les moyens d’améliorer les approches basées sur les Transformers. L’article aborde ainsi les Sparse Transformers, les Adaptive Span Transformers, le Transformer-XL, les compressive Transformers, le Reformer, et les routing transformers.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Contrôler le style et le contenu dans la rédaction automatique de textes&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Malgré l’impressionnante fluidité dont a fait preuve l’écriture automatique des textes l’année dernière, il est toujours difficile de contrôler des attributs comme la structure ou le contenu du texte. Dans un &lt;a href=&quot;https://creatext.ai/blog-posts/controllable-text-generation&quot;&gt;récent article de blog&lt;/a&gt;, Manuel Tonneau évoque les progrès récents et les perspectives dans le domaine de la génération de texte contrôlable, du modèle GPT-2 de Hugging Face, fine-tuné sur arXiv, au T5 de Google, en passant par CTRL de Salesforce et PPLM de Uber AI.&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education 🎓&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;L’avenir du NLP en Python&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans l’un de nos numéros précédents, nous avons présenté &lt;a href=&quot;https://thinc.ai/&quot;&gt;THiNC&lt;/a&gt;, qui est une librairie de DL fonctionnelle axée sur la compatibilité avec d’autres librairies existantes. Ces &lt;a href=&quot;https://speakerdeck.com/inesmontani/the-future-of-nlp-in-python-keynote-pycon-colombia-2020?slide=9&quot;&gt;diapositives&lt;/a&gt; présentent un peu plus cette livrairie qui a été utilisée par Ines Montani pour la conférence PyCon Colombia.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Les notebooks de Transformers&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
HuggingFace a publié un ensemble de &lt;a href=&quot;https://github.com/huggingface/transformers/tree/master/notebooks&quot;&gt;notebooks&lt;/a&gt; qui aident à démarrer avec leur librairie Transformers. Certains notebooks comprennent l’utilisation de la tokenisation, la mise en place de pipelines et l’entraînement d’un modèle sur des données personnalisées.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*0AYHYUsHbaqV2vqN2zCzLQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;TensorFlow 2.0 en 7 heures&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Jetez un oeil à ce &lt;a href=&quot;https://www.freecodecamp.org/news/massive-tensorflow-2-0-free-course/&quot;&gt;cours gratuity d’environ 7h&lt;/a&gt; consacré à TensorFlow 2.0 abordant des sujets qui vont des réseaux neuronaux de base au NLP avec les RNN, en passant par une introduction à l’apprentissage par renforcement.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;DeepMind: le podcast&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind a publié tous les épisodes (sous la forme d’une &lt;a href=&quot;https://www.youtube.com/playlist?list=PLqYmG7hTraZBiUr6_Qf8YTS2Oqy3OGZEj&quot;&gt;playlist YouTube&lt;/a&gt;) de son podcast qui présente des scientifiques, des chercheurs et des ingénieurs discutant de sujets allant de l’IAG aux neurosciences en passant par la robotique.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Cours de Machine Learning et de Deep Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Berkeley rend public le &lt;a href=&quot;https://sites.google.com/view/berkeley-cs294-158-sp20/home&quot;&gt;programme complet&lt;/a&gt; de son cours sur “l’apprentissage profond non supervisé”, principalement axé sur les aspects théoriques de l’apprentissage autosupervisé ainsi que sur les modèles générateurs. Parmi les sujets abordés, citons les modèles de variables latentes, les modèles autorégressifs et les modèles de flux pour n’en citer que quelques-uns. Des vidéos et des diapositives sont disponibles sur Youtube.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Nous avons également trouvé cette importante &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/fdw0ax/d_advanced_courses_update/&quot;&gt;liste&lt;/a&gt; de cours en ligne sur l’apprentissage machine, le NLP et l’apprentissage approfondi.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Et voici un autre cours intitulé &lt;a href=&quot;https://compstat-lmu.github.io/lecture_i2ml/index.html&quot;&gt;“Introduction to Machine Learning”&lt;/a&gt; qui comprend des sujets tels que la régression supervisée, l’évaluation des performances, les forêts aléatoires, le réglage des paramètres, des conseils pratiques, et bien plus encore.&lt;/p&gt;

&lt;h1 id=&quot;mentions-spéciales-️&quot;&gt;Mentions spéciales ⭐️&lt;/h1&gt;

&lt;p&gt;Connon Shorten a publié une &lt;a href=&quot;https://www.youtube.com/watch?v=QWu7j1nb_jI&amp;amp;feature=emb_logo&quot;&gt;vidéo&lt;/a&gt; expliquant le modèle ELECTRA qui propose une technique appelée “remplacement de la détection de jeton” pour prétraiter les Transformers plus efficacement. Si vous êtes intéressé, nous avons également rédigé un bref résumé du modèle &lt;a href=&quot;https://medium.com/dair-ai/nlp-research-highlights-cd522b21b01a&quot;&gt;ici&lt;/a&gt; (en anglais).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Rachael Tatman travaille sur une nouvelle série intitulée &lt;a href=&quot;https://www.youtube.com/watch?v=-G36q8_cYsc&amp;amp;feature=emb_logo&quot;&gt;NLP for Developers&lt;/a&gt; où l’idée est de parler plus en profondeur des différentes méthodes de NLP du moment et d’expliquer les problèmes communs que vous pourriez rencontrer.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind diffuse &lt;a href=&quot;https://youtu.be/WXuK6gekU1Y&quot;&gt;AlphaGo - The Movie&lt;/a&gt; sur YouTube pour célébrer le 4ème anniversaire de la victoire d’AlphaGo sur Lee Sedol au jeu de Go.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
OpenMined &lt;a href=&quot;https://blog.openmined.org/introducing-openmined-research/&quot;&gt;évoque&lt;/a&gt; les rôles d’ingénieur de recherche et de chercheur scientifique, ce qui est une bonne occasion de s’impliquer dans la préservation de la vie privée en matière d’IA.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Vous pouvez retrouver la précédente newsletter &lt;a href=&quot;https://dair.ai/NLP_Newsletter_-6_-FR/&quot;&gt;ici&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous avez des jeux de données, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine édition de la newletter, n’hésitez pas à contacter Elvis à ellfae@gmail.com ou par message sur &lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;Abonnez-vous&lt;/a&gt; pour recevoir les prochains numéros dans votre boîte mail.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_-7_-FR/&quot;&gt;NLP Newsletter [FR] #7: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,…&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 16, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter [FR] #6: BERTology Primer, fastpages, T5, Data Science Education, PyTorch Notebooks, Slow Science in ML]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_-6_-FR/" />
  <id>https://dair.ai/NLP_Newsletter_#6_[FR]</id>
  <published>2020-03-09T00:00:00-05:00</published>
  <updated>2020-03-09T00:00:00-05:00</updated>
  <author>
    <name>Loïck BOURDOIS</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*vWICxAehSy3xOnqGIXtpoQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;avant-propos&quot;&gt;Avant-propos&lt;/h1&gt;
&lt;p&gt;Bienvenue au sixième numéro de la lettre d’information consacrée au NLP. Merci pour votre soutien et pour avoir pris le temps de lire les dernières nouvelles sur le ML et le NLP. Ce numéro traite de sujets allant de l’extension du modèle Transformer au ralentissement de la publication en ML, en passant par une série de livres et de lancements de projets en ML et en NLP.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Quelques mises à jour sur la lettre d’information sur le NLP et sur dair.ai.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Nous avons traduit la lettre d’information dans d’autres langues telles que le portugais brésilien, le chinois, l’arabe, l’espagnol, entre autres. Merci aux personnes qui ont aidé à la traduction. Vous pouvez également contribuer &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues/11&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Il y a un mois, nous avons officiellement lancé notre nouveau &lt;a href=&quot;https://dair.ai/&quot;&gt;site web&lt;/a&gt;. Vous pouvez consulter notre &lt;a href=&quot;https://github.com/dair-ai&quot;&gt;organisation GitHub&lt;/a&gt; pour plus d’informations sur dair.ai et ses projets. Si vous souhaitez voir comment d’autres personnes contribuent déjà à dair.ai ou si vous souhaitez contribuer à la démocratisation de la recherche, de l’éducation et des technologies en matière d’intelligence artificielle, consultez notre &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues&quot;&gt;section&lt;/a&gt; sur les questions d’actualité.&lt;/p&gt;

&lt;h1 id=&quot;publications--&quot;&gt;Publications  📙&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Une introduction à la BERTologie : Ce que nous savons sur le fonctionnement de BERT&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Les modèles basés sur le Transformer se sont avérés efficaces pour aborder différents types de tâches de NLP allant de la classification de séquences à la réponse aux questions. L’un de ces modèles, appelé BERT (&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;Devlin et al. 2019&lt;/a&gt;), est largement utilisé mais comme d’autres modèles qui utilisent des réseaux de neurones profonds, nous savons très peu de choses sur leur fonctionnement interne. Un nouvel &lt;a href=&quot;https://arxiv.org/abs/2002.12327&quot;&gt;article&lt;/a&gt; intitulé “ A Primer in BERTology: What we know about how BERT works “ vise à répondre à certaines des interrogations portant sur les raisons pour lesquelles BERT est performant dans un si grand nombre de tâches de NLP. Parmi les sujets abordés dans cet article, on trouve le type de connaissances acquises par BERT ainsi que leur représentation, la manière dont ces connaissances sont acquises et les autres méthodes utilisées par les chercheurs pour les améliorer.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Explorer les limites de l’apprentissage par transfert avec un Transformer de texte à texte&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Google AI a récemment publié une &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;méthode&lt;/a&gt; qui rassemble tous les enseignements et les améliorations tirés des modèles de NLP basés sur l’apprentissage par transfert.  Les auteurs l’ont appelé Text-to-Text Transfer Transformer (T5). Ce travail propose que la plupart des tâches de NLP puissent être formulées dans un format texte-texte, suggérant que les entrées et les sorties sont des textes. Les auteurs affirment que ce “ cadre fournit un objectif d’entraînement cohérent à la fois pour le pré-entraînement et le fine-tuning”. 			
Le T5 est essentiellement un Transformer encoder-decoder qui applique diverses améliorations, en particulier aux composantes d’attention qui composent le modèle. Le modèle a été pré-entraîné sur un ensemble de données récemment publié, le &lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/c4&quot;&gt;Colossal Clean Crawled Corpus&lt;/a&gt; et a été appliqué sur SOTA sur des tâches de NLP telles que le résumé, la réponse aux questions et la classification de textes.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*T9MXxcDOd2fX6xblbu7VdQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;&lt;em&gt;(Raffel et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;12 en 1 : Apprentissage multitâche de la représentation de la vision et des langues&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
La recherche actuelle utilise des tâches et des ensembles de données indépendants pour effectuer des recherches sur la vision et le langage même lorsque les “compétences de compréhension du langage fondées sur la vision” requises pour effectuer ces tâches se chevauchent. Une nouvelle &lt;a href=&quot;https://arxiv.org/abs/1912.02315&quot;&gt;publication&lt;/a&gt; (qui sera présentée à la CVPR) propose une approche multitâche à grande échelle pour mieux modéliser et entraîner conjointement les tâches de vision et du langage afin de générer un modèle de vision et de langue plus générique. Le modèle réduit la taille des paramètres et fonctionne bien pour des tâches telles que la recherche d’images basée sur des légendes et la réponse visuelle à des questions.
&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*yyvN4bK0K2iykyJ2-QVBjw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.02315&quot;&gt;&lt;em&gt;(Lu et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;BERT peut voir à l’extérieur de la boîte : Sur la transférabilité intermodale des représentations textuelles&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Les chercheurs et collaborateurs de reciTAL ont publié un &lt;a href=&quot;https://arxiv.org/abs/2002.10832&quot;&gt;article&lt;/a&gt; qui vise à répondre à la question de savoir si un modèle BERT peut produire des représentations qui se généralisent à d’autres modalités que le texte, comme par exemple la vision. Ils proposent un modèle appelé BERT-gen qui exploite des représentations mono ou multimodales et qui obtient de meilleurs résultats sur les ensembles de données de génération de questions visuelles.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*2NgR7yBuVLDcEza9UT41dw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.10832&quot;&gt;&lt;em&gt;(Scialom et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;créativité-et-société-&quot;&gt;Créativité et société 🎨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;La prochaine décennie en IA : quatre étapes vers une intelligence artificielle robuste&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Gary Marcus a récemment publié un &lt;a href=&quot;https://arxiv.org/abs/2002.06177&quot;&gt;article&lt;/a&gt; dans lequel il explique une série de mesures que nous devrions prendre selon lui afin de construire des systèmes d’IA plus robustes. L’idée centrale dans ce papier est de se concentrer sur la construction de systèmes hybrides et axés sur la connaissance, guidés par des modèles cognitifs, plutôt que de se concentrer sur la construction de systèmes plus importants qui nécessitent plus de données et de puissance de calcul.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;10 technologies de pointe pour 2020&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
La revue technologique du MIT a publié une liste des &lt;a href=&quot;https://www.technologyreview.com/lists/technologies/2020/&quot;&gt;10 percées&lt;/a&gt; qu’elle a identifiées et qui feront la différence dans la résolution de problèmes susceptibles de changer notre façon de vivre et de travailler. La liste - sans ordre particulier - comprend l’internet non piratable, la médecine hyper-personnalisée, l’argent numérique, les médicaments anti-âge, les molécules découvertes par l’IA, les méga-constellations de satellites, la suprématie quantique, l’IA minuscule, la confidentialité différentielle et l’attribution du climat.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Il est temps de repenser le processus de publication en ML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Yoshua Bengio a récemment fait part de ses &lt;a href=&quot;https://yoshuabengio.org/2020/02/26/time-to-rethink-the-publication-process-in-machine-learning/&quot;&gt;préoccupations&lt;/a&gt; concernant les cycles rapides des publications du ML. La principale est qu’en raison de la rapidité de la publication, beaucoup d’articles publiés contiennent des erreurs et sont juste incrémentiels. A contrario, ceux sur lesquels plus de temps est consacré afin d’en assurer la rigueur, semble disparaître. De plus, ce sont les étudiants qui doivent faire face aux conséquences négatives de cette pression et de ce stress. Pour remédier à cette situation, Bengio parle de ses actions pour aider à ralentir le processus de publication des recherches pour le bien de la science.&lt;/p&gt;

&lt;h1 id=&quot;outils-et-jeux-de-données-️&quot;&gt;Outils et jeux de données ⚙️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Mise en œuvre du réseau PointerGenerator dans AllenNLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Les réseaux « Pointer-Generator » visent à augmenter les modèles d’attention utilisés pour améliorer &lt;a href=&quot;https://arxiv.org/abs/1704.04368&quot;&gt;la synthèse abstraite&lt;/a&gt;. Si vous souhaitez utiliser cette technique en utilisant AllenNLP, Kundan Krishna a développé une &lt;a href=&quot;https://github.com/kukrishna/pointer-generator-pytorch-allennlp&quot;&gt;librairie&lt;/a&gt; qui vous permet d’exécuter un modèle pré-entraîné (fourni) ou d’entraîner votre propre modèle.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Questions/réponses pour différentes langues&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Avec la prolifération des modèles de Transformer et leur efficacité pour les tâches de NLP, des efforts impressionnants ont été déployés pour publier différents types de jeux de données dans différentes langues. Par exemple, Sebastian Ruder a &lt;a href=&quot;https://twitter.com/seb_ruder/status/1231713840502657025?s=20&quot;&gt;partagé une liste&lt;/a&gt; de jeux de données qui peuvent être utilisés pour des tâches de réponses aux questions dans différentes langues : &lt;a href=&quot;https://www.aclweb.org/anthology/W18-2605/&quot;&gt;DuReader&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1909.07005&quot;&gt;KorQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1912.09723&quot;&gt;SberQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2002.06071&quot;&gt;FQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1906.05394&quot;&gt;Arabic-SQuAD&lt;/a&gt;, &lt;a href=&quot;https://github.com/crux82/squad-it&quot;&gt;SQuAD-it&lt;/a&gt; et &lt;a href=&quot;https://arxiv.org/abs/1912.05200v2&quot;&gt;Spanish SQuAD&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;PyTorch Lightning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
PyTorch Lightning est un &lt;a href=&quot;https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09&quot;&gt;outil&lt;/a&gt; qui vous permet de réaliser un entraînement abstrait qui nécessiterait l’utilisation de GPU/TPU d’une précision de 16 bits. PyTorch Lightning permet d’entraîner des modèles sur des plusieurs GPU et TPU sans avoir besoin de changer votre code PyTorch actuel.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Graph Neural Networks dans TensorFlow 2&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Une équipe de recherche de Microsoft publie une &lt;a href=&quot;https://github.com/microsoft/tf2-gnn&quot;&gt;librairie&lt;/a&gt; qui donne accès aux implémentations de nombreuses architectures de réseaux neuronaux en graphes (GNN). Cette librairie est basée sur TensorFlow 2 et fournit également des modules de manipulation de données qui peuvent être directement utilisés dans des boucles d’entrainement/évaluation.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Pré-entraînement de SmallBERTa - Un petit modèle pour s’entraîner sur un petit jeu de données&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Avez-vous déjà voulu entraîner votre propre modèle linguistique à partir de zéro mais n’avez pas eu assez de ressources pour le faire ? Si c’est le cas, Aditya Malte vous propose un &lt;a href=&quot;https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b#file-smallberta_pretraining-ipynb&quot;&gt;notebook&lt;/a&gt; qui vous apprend à entrainer un modèle linguistique à partir de zéro avec un ensemble de données plus restreint.&lt;/p&gt;

&lt;h1 id=&quot;ethique-en-ia-&quot;&gt;Ethique en IA 🚨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Pourquoi les visages ne disent pas toujours la vérité sur les sentiments&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Depuis un certain temps, de nombreux chercheurs et entreprises ont tenté de construire des modèles d’IA qui comprennent et peuvent reconnaître les émotions dans un contexte textuel ou visuel. Un nouvel &lt;a href=&quot;https://www.nature.com/articles/d41586-020-00507-5&quot;&gt;article&lt;/a&gt; relance le débat sur le fait que les techniques d’IA qui visent à reconnaître les émotions à partir des images de visages ne le font pas correctement. L’argument principal, soulevé par des psychologues, est qu’il n’existe aucune preuve d’expressions universelles pouvant être utilisées pour la détection d’émotions basées uniquement sur des images de visages. Il faudrait qu’un modèle comprenne mieux par exemple les traits de personnalité ou encore les mouvements du corps, afin de se rapprocher réellement d’une détection plus précise des émotions affichées par les humains.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Differential Privacy and Federated Learning Explained&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
L’une des considérations éthiques à prendre en compte lors de la construction de systèmes d’IA est la garantie du respect de la vie privée. Actuellement, cela peut être réalisé de deux manières, soit en utilisant une intimité différentielle, soit par un apprentissage fédéré. Si vous voulez en savoir plus sur ces sujets, Jordan Harrod nous fournit une excellente introduction dans cette &lt;a href=&quot;https://www.youtube.com/watch?v=MOcTGM_UteM&quot;&gt;vidéo&lt;/a&gt; qui comprend également une session de pratique avec l’utilisation d’un notebook.&lt;/p&gt;

&lt;h1 id=&quot;articles-et-blog-️&quot;&gt;Articles et Blog ✍️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Plongée dans le Reformer&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Madison May a écrit un nouvel &lt;a href=&quot;https://www.pragmatic.ml/reformer-deep-dive/&quot;&gt;article sur son blog&lt;/a&gt; consacré au &lt;a href=&quot;https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html&quot;&gt;Reformer&lt;/a&gt;, proposé par Google AI. Nous avons également présenté le Reformer dans un &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-reformer-deepmath-electra-tinybert-for-search-vizseq-open-sourcing-ml-68d5b6eed057&quot;&gt;précédent numéro&lt;/a&gt; de la newsletter. Pour une explication en français, vous pouvez consulter l’illustration &lt;a href=&quot;https://lbourdois.github.io/blog/nlp/Reformer/&quot;&gt;suivante&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Une plateforme de blogging gratuite&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://fastpages.fast.ai/fastpages/jupyter/2020/02/21/introducing-fastpages.html&quot;&gt;fastpages&lt;/a&gt; vous permet de créer automatiquement et gratuitement un blog en utilisant les pages GitHub. Cette solution simplifie le processus de publication d’un blog et prend également en charge l’utilisation de documents Word et de notebook Jupyter.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Conseils pour un entretien chez Google&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Pablo Castro, de l’équipe Google Brain, a publié un &lt;a href=&quot;https://psc-g.github.io/interviews/google/2020/02/25/interviewing-at-google.html&quot;&gt;article sur son blog&lt;/a&gt; mettant en avant une liste de conseils pour les personnes intéressées par un entretien d’embauche chez Google. Parmi les sujets abordés figurent des conseils sur la façon de se préparer à l’entretien, sur ce à quoi il faut s’attendre pendant l’entretien et sur ce qui se passe après l’entretien.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Les Transformers sont des Graph Neural Networks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Les réseaux neuronaux de graphes (GNN) et les Transformers se sont avérés efficaces pour différentes tâches de NLP. Pour mieux comprendre le fonctionnement interne de ces approches et leurs relations, Chaitanya Joshi a écrit un &lt;a href=&quot;https://graphdeeplearning.github.io/post/transformers-are-gnns/&quot;&gt;article&lt;/a&gt; expliquant la connexion entre les GNN et les Transformers, ainsi que les différentes façons dont ces méthodes peuvent être combinées dans une sorte de modèle hybride.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;CNNs et Equivariance&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Fabian Fuchs et Ed Wagstaff &lt;a href=&quot;https://fabianfuchsml.github.io/equivariance1of2/&quot;&gt;discutent&lt;/a&gt; de l’importance de l’équivariance et de la manière dont les CNN la font respecter. Le concept d’équivariance est d’abord défini, puis discuté dans le contexte de CNN appliqués à la traduction.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;L’auto-apprentissage avec les images&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
L’auto-apprentissage a été beaucoup évoqué dans les précédents numéros de la newsletter en raison du rôle qu’il a joué dans les techniques modernes de modélisation des langues. Ce &lt;a href=&quot;https://datasciencecastnet.home.blog/2020/02/22/self-supervised-learning-with-image%e7%bd%91/&quot;&gt;billet&lt;/a&gt; de Jonathan Whitaker fournit une explication de l’auto-apprentissage dans le contexte des images. Si le sujet vous intéresse, Amit Chaudhary a également écrit un &lt;a href=&quot;https://amitness.com/2020/02/illustrated-self-supervised-learning/&quot;&gt;article&lt;/a&gt; décrivant le concept de manière visuelle.&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education 🎓&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Stanford CS330: Deep Multi-Task et Meta-Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Stanford a récemment publié une &lt;a href=&quot;https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&quot;&gt;playlist vidéo YouTube&lt;/a&gt; sur son nouveau cours consacré à l’apprentissage multi-tâches et au méta-apprentissage. Parmi les sujets abordés, citons le méta-apprentissage bayésien, le lifelong apprentissage, une introduction à l’apprentissage par renforcement, etc…&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Notebooks PyTorch&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
dair.ai publie une &lt;a href=&quot;https://github.com/dair-ai/pytorch_notebooks&quot;&gt;série de notebook&lt;/a&gt; qui visent à vous faire découvrir les réseaux neuronaux profonds à l’aide de PyTorch. Il s’agit d’un travail en cours et certains sujets d’actualité comprennent la façon de mettre en œuvre un modèle de régression logistique à partir de zéro et la façon de programmer un NN ou un RNN à partir de zéro. Les notebooks sont également disponibles dans le dépôt GitHub.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;em&gt;**Le livre de fastai book (ébauche) **&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Jeremy Howard et Sylvain Gugger ont publié une &lt;a href=&quot;https://github.com/fastai/fastbook&quot;&gt;liste complète&lt;/a&gt; de notebooks (non terminés) en vue de leur prochain cours qui présentera des concepts de deep learning et différentes méthodes d’utilisation de PyTorch et la librairie fastai.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Cours gratuits sur la datascience&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Au cas où vous l’auriez manqué, Kaggle propose une série de &lt;a href=&quot;https://www.kaggle.com/learn/overview&quot;&gt;petits cours gratuits&lt;/a&gt; sur les outils de datascience. Certains de ces cours comprennent, entre autres, l’explication de l’apprentissage machine, une introduction à l’apprentissage machine et à Python, la visualisation de données, l’ingénierie des fonctionnalités et l’apprentissage approfondi.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Un autre &lt;a href=&quot;https://lewtun.github.io/dslectures/&quot;&gt;cours de datascience en ligne&lt;/a&gt; propose un programme, des diapositives et des notebooks sur l’analyse exploratoire des données, l’interprétation des modèles ou encore le NLP.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;8 créateurs et contributeurs parlent de leurs librairies PyTorch&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
nepture.ai a publié un vaste &lt;a href=&quot;https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem?utm_source=twitter&amp;amp;utm_medium=tweet&amp;amp;utm_campaign=blog-model-training-libraries-pytorch-ecosystem&quot;&gt;article&lt;/a&gt; qui contient des discussions détaillées avec les principaux créateurs et contributeurs de PyTorch (parcours, philosophie du projet, outils qui l’entourent, etc…).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Visualisation des modèles adaptatifs d’attention réduite&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sasha Rush partage un &lt;a href=&quot;https://colab.research.google.com/drive/1EB7MI_3gzAR1gFwPPO27YU9uYzE_odSu&quot;&gt;notebook&lt;/a&gt; qui explique et montre les détails techniques sur la manière de produire des sorties softmax sparses. Il aborde également la façon d’induire la sparcité dans la composante d’attention d’un modèle de Transformer, ce qui aide à produire une probabilité nulle pour les mots non pertinents dans un contexte donné, améliorant ainsi la performance et l’interprétabilité.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*7BB322LlVgt1zzk-cviSoA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;mentions-spéciales-️&quot;&gt;Mentions spéciales ⭐️&lt;/h1&gt;

&lt;p&gt;Conor Bell a codé un &lt;a href=&quot;https://gist.github.com/connorbell/9269401d127f1e507cc9aaf2803067c4&quot;&gt;script python&lt;/a&gt; qui vous permet de visualiser et de préparer facilement un jeu de données pouvant être utilisé pour un modèle StyleGAN.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Manu Romero &lt;a href=&quot;https://github.com/huggingface/transformers/tree/master/model_cards/mrm8488/bert-spanish-cased-finetuned-pos&quot;&gt;a contribué&lt;/a&gt; au fine-tuning d’un modèle POS pour l’espagnol. Le modèle est sur la librairie Transfomers d’Hugging Face. Il sera intéressant de voir cet effort dans d’autres langues.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ce &lt;a href=&quot;https://github.com/tomohideshibata/BERT-related-papers&quot;&gt;répertoire Github&lt;/a&gt; contient une longue liste de documents rédigés sur BERT qui abordent différents problèmes tels que la compression de modèles, les domaines spécifiques, le multi-modèle, la génération, les tâches en aval, etc.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Connor Shorten a publié une courte &lt;a href=&quot;https://www.youtube.com/watch?time_continue=79&amp;amp;v=-Bh_7tzyoR4&amp;amp;feature=emb_logo&quot;&gt;vidéo&lt;/a&gt; de 15 minutes expliquant un framework à aborder pour réduire l’effet des “raccourcis” dans l’auto-apprentissage de la représentation. C’est important car, s’il n’est pas bien fait, le modèle peut ne pas apprendre des représentations sémantiques utiles et se révéler potentiellement inefficace dans un contexte d’apprentissage par transfert.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sebastian Ruder a publié un nouveau numéro de sa newsletter qui met en lumière des sujets et des ressources allant d’analyses d’articles sur le NLP et le ML de 2019 à des diapositives sur l’apprentissage par transfert et des éléments essentiels sur le deep learning. Vous pouvez le consulter &lt;a href=&quot;http://newsletter.ruder.io/issues/accelerating-science-memorizing-vs-learning-to-look-things-up-schmidhuber-s-2010s-greek-bert-arc-illustrated-reformer-annotated-gpt-2-olmpics-223195&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Vous pouvez retrouver la précédente newsletter &lt;a href=&quot;https://dair.ai/NLP_Newsletter_-5_-FR/&quot;&gt;ici&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous avez des jeux de données, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine édition de la newletter, n’hésitez pas à me contacter à ellfae@gmail.com ou par message sur &lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;Abonnez-vous&lt;/a&gt; pour recevoir les prochains numéros dans votre boîte mail.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_-6_-FR/&quot;&gt;NLP Newsletter [FR] #6: BERTology Primer, fastpages, T5, Data Science Education, PyTorch Notebooks, Slow Science in ML&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 09, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter [FR] #5: The Annotated GPT-2, Understanding self-distillation, Haiku, GANILLA, Sparkwiki, Ethics in NLP, Torchmeta,…]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_-5_-FR/" />
  <id>https://dair.ai/NLP_Newsletter_#5_[FR]</id>
  <published>2020-03-09T00:00:00-05:00</published>
  <updated>2020-03-09T00:00:00-05:00</updated>
  <author>
    <name>Loïck BOURDOIS</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*YIhZsPaiBFkRMMWo5FAhGw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;avant-propos&quot;&gt;Avant-propos&lt;/h1&gt;
&lt;p&gt;Tout d’abord, je ne saurais trop tous vous remercier pour le soutien et les encouragements incroyables que vous avez apportés à cette newsletter. Son élaboration nécessite des recherches et une rédaction fastidieuse que je trouve à la fois enrichissantes et utiles, afin de vous fournir le meilleur contenu. J’espère que vous les appréciez, car c’est le cas pour moi. 😉&lt;/p&gt;

&lt;h1 id=&quot;publications-&quot;&gt;Publications 📙&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Une compréhension théorique de l’autodistillation&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
L’&lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot;&gt;autodistillation&lt;/a&gt; est le processus de transfert de connaissances d’une architecture à une seconde qui est identique. Les prédictions du modèle original sont transmises comme valeurs cibles au second modèle pendant la phase d’entraînement. Outre les propriétés souhaitables, comme la réduction de la taille du modèle, les résultats empiriques montrent que cette approche fonctionne bien sur des ensembles de données maintenus. Un groupe de chercheurs a récemment publié un article qui fournit une analyse théorique visant à mieux comprendre ce qui se passe dans ce processus et pourquoi il est efficace. Les résultats montrent que quelques cycles d’autodistillation amplifient la régularisation (&lt;a href=&quot;https://twitter.com/TheGradient/status/1228132843630387201?s=20&quot;&gt;en limitant progressivement le nombre de fonctions de base qui représentent la solution&lt;/a&gt;), ce qui tend à réduire le sur-apprentissage. (Lire l’article complet &lt;a href=&quot;https://arxiv.org/abs/2002.05715&quot;&gt;ici&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*nWYO71awfooL4MrGK-tFww.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.05715&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Les années 2010 : Une décennie d’apprentissage approfondi / Perspectives pour les années 2020&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;http://people.idsia.ch/~juergen/&quot;&gt;Jürgen Schmidhuber&lt;/a&gt;, un pionnier de l’intelligence artificielle, a récemment publié un &lt;a href=&quot;http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html&quot;&gt;article sur son blog&lt;/a&gt; qui se concentre sur un aperçu historique de l’apprentissage profond depuis 2010. Parmi les sujets abordés, citons les LSTM, les feedforward NN, les GAN, l’apprentissage par renforcement, le méta-apprentissage, la distillation, l’apprentissage par l’attention, etc. L’article se termine par une perspective sur les années 2020, encourageant l’attention sur des questions urgentes telles que la vie privée et les marchés des données.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Utilisation des réseaux de neurones pour résoudre des équations mathématiques&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Les chercheurs du FAIR de Facebook ont publié un &lt;a href=&quot;https://arxiv.org/abs/1912.01412&quot;&gt;article&lt;/a&gt; qui propose un modèle entraîné sur des problèmes mathématiques ainsi que leurs solutions associées, afin d’apprendre à prédire les solutions possibles pour des tâches telles que la résolution de problèmes d’intégration. L’approche est basée sur une approche similaire à celle utilisée dans la traduction automatique où les expressions mathématiques sont représentées comme une sorte de langage et les solutions sont traitées comme le problème de traduction. Ainsi, au lieu que le modèle produise une traduction, le résultat est la solution elle-même. Les chercheurs affirment ainsi que les réseaux neuronaux profonds ne sont pas seulement bons pour le raisonnement symbolique, mais aussi pour des tâches plus diverses.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*P_JtxoC8pYkXuXCp3e3QeQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Équations fournies en entrée avec la solution correspondante fournie par le modèle –&lt;/em&gt; &lt;a href=&quot;https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;créativité-et-société-&quot;&gt;Créativité et société 🎨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;L’IA au service de la découverte scientifique&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Mattew Hutson &lt;a href=&quot;https://www.sciencemag.org/news/2020/02/models-galaxies-atoms-simple-ai-shortcuts-speed-simulations-billions-times&quot;&gt;rapporte&lt;/a&gt; comment l’IA peut être utilisée pour produire des émulateurs qui ont une utilité importante dans la modélisation de phénomènes naturels complexes qui pourraient, à leur tour, conduire à différents types de découvertes scientifiques. Le défi avec la construction de ces émulateurs est qu’ils nécessitent souvent d’importantes données et une exploration approfondie des paramètres. Un &lt;a href=&quot;https://arxiv.org/abs/2001.08055&quot;&gt;article&lt;/a&gt; récent propose DENSE, une approche basée sur la &lt;a href=&quot;https://en.wikipedia.org/wiki/Neural_architecture_search&quot;&gt;recherche d’architecture neurale&lt;/a&gt; pour construire des émulateurs précis tout en ne s’appuyant que sur une quantité limitée de données d’entraînement. Ils l’ont testée en effectuant des simulations pour des cas tels que l’astrophysique, la climatologie et la fusion, entre autres.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Améliorer la « traduction » de l’image à l’illustration&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
GANILLA est une approche qui propose l’utilisation de GAN pour améliorer le transfert à la fois du style et du contenu dans la &lt;a href=&quot;https://paperswithcode.com/task/image-to-image-translation&quot;&gt;tâche de traduction d’image à image&lt;/a&gt; non appariée. En particulier, un modèle d’illustration d’image à image est proposé (avec un réseau générateur amélioré) et évalué sur la base d’un nouveau cadre d’évaluation quantitative qui prend en compte à la fois le contenu et le style. La nouveauté de ce travail réside dans le réseau générateur proposé qui tient compte d’un équilibre entre le style et le contenu, ce que les modèles précédents n’ont pas réussi à atteindre. Des codes et modèles pré-entrainés sont mis à &lt;a href=&quot;https://github.com/giddyyupp/ganilla&quot;&gt;disposition&lt;/a&gt;. Lisez le document complet &lt;a href=&quot;https://arxiv.org/abs/2002.05638&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*l_B4vfaHVkXDwzM7SldiqQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Andrew Ng parle de l’intérêt pour l’auto-apprentissage&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew Ng, le fondateur de deeplearning.ai, est intervenu dans un podcast sur l’intelligence artificielle pour &lt;a href=&quot;https://www.youtube.com/watch?v=0jspaMLxBig&quot;&gt;parler&lt;/a&gt; de sujets tels que ses débuts dans le ML, l’avenir de l’IA, l’enseignement de l’IA, ses recommandations pour une bonne utilisation du ML, ses objectifs personnels et les techniques de ML auxquelles il faudra prêter attention dans les années 2020.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew a expliqué pourquoi il est très enthousiaste à l’idée de s’initier à l’auto-apprentissage. &lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html&quot;&gt;L’auto-apprentissage supervisé&lt;/a&gt; consiste à formuler un problème d’apprentissage dont le but est d’obtenir une supervision à partir des données elles-mêmes. L’intérêt est d’utiliser de grandes quantités de données non labélisées, ce qui est plus disponibles en plus grande quantité que les données labélisées. Les représentations, par opposition à l’exécution de la tâche, sont importantes et peuvent être utilisées pour traiter des tâches en aval, comme c’est le cas dans les modèles linguistiques tels que le &lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#bert&quot;&gt;BERT&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Il y a également beaucoup d’intérêt à utiliser l’auto-apprentissage supervisé](pour apprendre des représentations visuelles généralisées qui rendent les modèles plus précis dans des environnements à faibles ressources. Par exemple, une méthode récente appelée &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;SimCLR&lt;/a&gt; (dirigée par Geoffrey Hinton) propose un cadre pour améliorer les résultats de la classification des images dans différents contextes tels que l’apprentissage par transfert et l’apprentissage semi-supervisé.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*8zLzHFCyM3goc9y7KApHfg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;outils-et-jeux-de-données-️&quot;&gt;Outils et jeux de données ⚙️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Les libraries liées à JAX&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/google/jax&quot;&gt;JAX&lt;/a&gt; est une nouvelle bibliothèque qui combine NumPy et la différenciation automatique pour mener des recherches de ML de haut niveau. Afin de simplifier les pipelines utilisant JAX, DeepMind a publié &lt;a href=&quot;https://github.com/deepmind/dm-haiku&quot;&gt;Haiku&lt;/a&gt; et &lt;a href=&quot;https://github.com/deepmind/rlax&quot;&gt;RLax&lt;/a&gt;. RLax simplifie l’implémentation de modèles basés sur l’apprentissage par renforcement et Haiku simplifie la construction de réseaux neuronaux en utilisant des modèles de programmation orientés objet.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Un outil de traitement des données Wikipédia&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/epfl-lts2/sparkwiki&quot;&gt;Sparkwiki&lt;/a&gt; est un outil permettant de traiter les données de Wikipédia. Cette version s’inscrit dans le cadre de nombreux efforts visant à permettre des recherches intéressantes en matière d’analyse comportementale, telles que la &lt;a href=&quot;https://arxiv.org/abs/2002.06885&quot;&gt;capture des tendances et des biais linguistiques dans les différentes éditions linguistiques de Wikipédia&lt;/a&gt;. Les auteurs ont découvert qu’indépendamment de la langue, le comportement de navigation des utilisateurs de Wikipédia montre qu’ils ont tendance à partager des intérêts communs comme par exemples les films, la musique et le sport, mais que les différences deviennent plus apparentes avec les événements locaux et les particularités culturelles.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*K7N9KbQlbuqowUeePjLtdw.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Mise à jour de la librairie Transformers&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Une &lt;a href=&quot;https://github.com/huggingface/transformers/releases/tag/v2.5.0&quot;&gt;nouvelle version&lt;/a&gt; de la librairie Transformers d’Hugging Face est disponible. Elle comprend l’intégration de leur librairie Tokenizer qui vise à accélérer des modèles comme BERT, RoBERTa, GPT2, et d’autres modèles construits par la communauté.&lt;/p&gt;

&lt;h1 id=&quot;ethique-en-ia-&quot;&gt;Ethique en IA 🚨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Considérations éthiques pour les modèles de NLP et de ML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans un nouvel &lt;a href=&quot;https://soundcloud.com/nlp-highlights/106-ethical-considerations-in-nlp-research-emily-bender&quot;&gt;épisode&lt;/a&gt; des &lt;a href=&quot;https://soundcloud.com/nlp-highlights&quot;&gt;NLP Highlights&lt;/a&gt;, Emily Bender et les intervenants parlent de certaines considérations éthiques qui peuvent se poser lors du développement de modèles de NLP dans un contexte d’utilisation universitaire et grand public. Parmi les sujets abordés, citons les considérations éthiques lors de la conception des tâches de NLP, les approches de collecte de données et, finalement, la publication des résultats.
En plus de toutes les considérations ci-dessus, une préoccupation qui est toujours discutée dans la communauté de l’IA est de se concentrer trop sur l’optimisation d’une mesure, ce qui va à l’encontre des fondements de ce que l’IA vise à atteindre (càd une IA générale). Rachel Thomas et David Uminsky discutent des erreurs possibles en &lt;a href=&quot;https://arxiv.org/abs/2002.08512&quot;&gt;analysant de manière approfondie&lt;/a&gt; différents cas d’utilisation. Ils proposent également un cadre simple pour atténuer le problème, qui implique l’utilisation et la combinaison de plusieurs mesures, suivies par l’implication des personnes directement concernées par la technologie.&lt;/p&gt;

&lt;h1 id=&quot;articles-et-blog-️&quot;&gt;Articles et Blog ✍️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Le GPT2 annoté&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Aman Arora a récemment publié un article sur son blog, intitulé le “&lt;a href=&quot;https://amaarora.github.io/2020/02/18/annotatedGPT2.html&quot;&gt;The Annotated GPT-2&lt;/a&gt;”, qui explique le fonctionnement interne du GPT-2. Son approche s’inspire de “&lt;a href=&quot;https://nlp.seas.harvard.edu/2018/04/03/attention.html&quot;&gt;The Annotated Transformer&lt;/a&gt;” qui a adopté une approche d’annotation pour expliquer les parties importantes du modèle par le biais de code et d’explications faciles à suivre. Aman a fait de gros efforts pour réimplémenter le GPT-2 d’OpenAI en utilisant PyTorch et la bibliothèque Transformers de Hugging Face.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*oRFMJTEojyQ-uocVES5GYA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amaarora.github.io/2020/02/18/annotatedGPT2.html&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Au-delà de BERT ?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sergi Castella expose son &lt;a href=&quot;https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1&quot;&gt;point de vue&lt;/a&gt; sur ce qui se trouve au-delà de BERT. Les principaux sujets abordés sont l’amélioration des mesures, la façon dont la librairie Transformers d’HuggingfFace permet de faire des recherches, les jeux de données intéressants à consulter, etc…&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Opérateur de compression matricielle&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
TensorFlow blog a publié un &lt;a href=&quot;https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&quot;&gt;article&lt;/a&gt; expliquant les techniques et l’importance de la compression des matrices dans un modèle de réseau neuronal profond. La compression des matrices peut aider à construire des modèles petits plus efficaces qui peuvent être incorporés dans des appareils tels que les téléphones et les assistants vocaux. En se concentrant sur la compression des modèles par des méthodes telles que la low-rank-approximation et la quantization, nous n’avons pas besoin de compromettre la qualité du modèle.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*fpAdJvBIf4SKxF3gTIpe_g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education 🎓&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Les bases du NLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Elvis a publié une ébauche du chapitre 1 de sa nouvelle série intitulée “&lt;a href=&quot;https://medium.com/dair-ai/fundamentals-of-nlp-chapter-1-tokenization-lemmatization-stemming-and-sentence-segmentation-b362c5d07684&quot;&gt;Les bases du NLP&lt;/a&gt;”. Il enseigne les concepts du NLP en partant des bases tout en partageant les meilleures pratiques, les références importantes, et les erreurs courantes à éviter. Un &lt;a href=&quot;https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ&quot;&gt;Google Colab&lt;/a&gt; est disponible et le projet sera maintenu &lt;a href=&quot;https://github.com/dair-ai/nlp_fundamentals&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;[Online] Review/Discussion: Part I Mathematical Foundations Reading Session&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Machine Learning Tokyo organise une discussion en ligne sur les chapitres qui ont été couverts lors de leurs récentes sessions d’étude en ligne. Le groupe avait auparavant étudié des chapitres basés sur le livre intitulé &lt;a href=&quot;https://mml-book.github.io/&quot;&gt;Mathematics For Machine Learning&lt;/a&gt;, écrit par Marc Peter Deisenroth, A Aldo Faisal et Cheng Soon Ong. L’événement est prévu pour le 8 mars 2020.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Recommandations de livres&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans une partie précédente, nous avons discuté de l’importance de la compression matricielle pour la construction de petits modèles de ML. Si vous souhaitez en savoir plus sur la façon de construire des réseaux neuronaux profonds plus petits pour les systèmes embarqués, consultez cet excellent livre intitulé &lt;a href=&quot;https://tinymlbook.com/?linkId=82595412&quot;&gt;TinyML&lt;/a&gt;, écrit par Pete Warden et Daniel Situnayake.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*omOa3aw2bfMzX2Qm.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.amazon.com/TinyML-Learning-TensorFlow-Ultra-Low-Micro-Controllers/dp/1492052043&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Un autre livre intéressant à surveiller et qui est à paraître est “&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;Deep Learning for Coders with fastai and PyTorch” : AI Applications Without a PhD&lt;/a&gt;” de Jeremy Howard et Sylvain Gugger. Ce livre vise à fournir les bases mathématiques nécessaires pour construire et former des modèles permettant d’aborder des tâches dans les domaines de computer vision et du NLP.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*i2OmtWOncatOYsZv.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;mentions-spéciales-️&quot;&gt;Mentions spéciales ⭐️&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1909.06576&quot;&gt;Torchmeta&lt;/a&gt; est une librairie qui permet d’utiliser facilement des chargeurs de données connexes pour la recherche sur le méta-apprentissage. Elle a été rédigée par Tristan Deleu.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Manuel Tonneau a écrit un &lt;a href=&quot;https://creatext.ai/blog-posts/machine-text-writing-gpt2-beam-search?utm_medium=newsletter&quot;&gt;article&lt;/a&gt; offrant un regard plus approfondi sur certains des mécanismes impliqués dans la modélisation du langage. Parmi les sujets abordés, citons la greedy recherche, la beam recherche et l’échantillonnage de noyaux.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Le MIT &lt;a href=&quot;http://introtodeeplearning.com/&quot;&gt;publie&lt;/a&gt; le programme complet et le calendrier du cours intitulé “Introduction to Deep Learning”. L’objectif est de publier chaque semaine des vidéos et des diapositives.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Apprenez comment entraîner un modèle de reconnaissance d’entités nommées (NER) en utilisant une approche basée sur &lt;a href=&quot;https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py&quot;&gt;Transformers&lt;/a&gt; en moins de 300 lignes de code. Vous pouvez trouver le programme Google Colab qui l’accompagne &lt;a href=&quot;https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Vous pouvez retrouver la précédente newsletter &lt;a href=&quot;https://dair.ai/NLP_Newsletter_-4_-FR/&quot;&gt;ici&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous avez des jeux de données, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine édition de la newletter, n’hésitez pas à me contacter à ellfae@gmail.com ou par message sur &lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;Abonnez-vous&lt;/a&gt; pour recevoir les prochains numéros dans votre boîte mail.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_-5_-FR/&quot;&gt;NLP Newsletter [FR] #5: The Annotated GPT-2, Understanding self-distillation, Haiku, GANILLA, Sparkwiki, Ethics in NLP, Torchmeta,…&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 09, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter [FR] #4: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,…]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_-4_-FR/" />
  <id>https://dair.ai/NLP_Newsletter_#4_[FR]</id>
  <published>2020-03-09T00:00:00-05:00</published>
  <updated>2020-03-09T00:00:00-05:00</updated>
  <author>
    <name>Loïck BOURDOIS</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*3vNKhz6K-oGQ8aLi3mo84Q.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;publications-&quot;&gt;Publications 📙&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Turing-NLG: Un modèle linguistique de 17 milliards de paramètres par Microsoft&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Turing Natural Language Generation (T-NLG) est un modèle de 17 milliards de paramètres &lt;a href=&quot;https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/&quot;&gt;proposé&lt;/a&gt; par les chercheurs en IA de Microsoft. Il est à ce jour, le plus grand modèle de langage connu (illustré dans la figure ci-dessous) et est basé sur un Transformer à 78 couches qui surpasse les résultats précédents (détenus par Megatron-LM de NVIDIA) sur la perplexité de WikiText-103. Il a été testé sur des tâches telles que la réponse à des questions et le résumé abstrait. Le modèle est rendu possible par une librairie d’optimisation de l’entraînement appelée DeepSpeed avec ZeRO, qui est également présentée plus loin dans cette newsletter.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*CAZm7uj8EaupnvnJ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Neural based Dependency Parsing&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Miryam de Lhoneux a publié sa thèse de doctorat intitulée “&lt;a href=&quot;http://uu.diva-portal.org/smash/record.jsf?pid=diva2%3A1357373&amp;amp;dswid=7905&quot;&gt;Linguistically Informed Neural Dependency Parsing for Typologically Diverse Languages&lt;/a&gt;”. Ce travail porte sur l’utilisation d’approches neurales pour l’&lt;a href=&quot;http://nlpprogress.com/english/dependency_parsing.html&quot;&gt;analyse des dépendances&lt;/a&gt; dans les langues typologiquement diverses (c’est-à-dire les langues qui construisent et expriment le sens de manière structurellement différente). Ce travail rapporte que les RNN et les couches récursives pourraient être utiles pour l’incorporation dans les parsers car elles aident à informer les modèles avec des connaissances linguistiques importantes nécessaires pour l’analyse. D’autres idées comprennent l’utilisation de l’analyse syntaxique polyglotte et des stratégies de partage de paramètres pour l’analyse syntaxique dans des langues apparentées et non apparentées.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Extraction d’informations de bout en bout dans le cloud avec BERT&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Une équipe de chercheurs a publié un &lt;a href=&quot;https://arxiv.org/abs/2002.01861&quot;&gt;article&lt;/a&gt; décrivant comment des modèles de Transformers comme BERT peuvent aider à l’extraction d’informations de bout en bout dans des documents commerciaux spécifiques à un domaine, tels que les dépôts réglementaires et les contrats de location de propriété. Non seulement ce type de travail peut aider à optimiser les opérations commerciales, mais il montre également l’applicabilité et l’efficacité des modèles basés sur BERT sur des régimes avec très peu de données annotées. Une application, et ses détails de mise en œuvre, qui fonctionne sur le cloud est également proposée (voir figure ci-dessous).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*KqViSLhP0otleDY-XFy3Bg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.01861&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Question Answering Benchmark&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/2001.11770v1&quot;&gt;Wolfson et al. (2020)&lt;/a&gt; ont publié un benchmark pour la compréhension des questions ainsi qu’une méthode pour décomposer une question qui est nécessaire pour calculer une réponse appropriée. Ils s’appuient sur le crowdsourcing pour annoter les étapes nécessaires à la décomposition des questions. Pour montrer la faisabilité et l’applicabilité de l’approche, ils améliorent la réponse aux questions du domaine ouvert en utilisant l’ensemble de données HotPotQA.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*AztG-Inqt6LGQ87lSufRcw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt; &lt;a href=&quot;https://arxiv.org/pdf/2001.11770v1.pdf&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Données radioactives : le traçage par l’entraînement&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Les chercheurs de Facebook AI ont récemment publié un &lt;a href=&quot;https://ai.facebook.com/blog/using-radioactive-data-to-detect-if-a-data-set-was-used-for-training/&quot;&gt;travail&lt;/a&gt; qui vise à marquer les images (appelées données radioactives) afin de vérifier si un jeu de données particulier a été utilisé pour l’entraînement d’un modèle de ML. Ils ont découvert qu’il est possible d’utiliser un marqueur intelligent qui déplace les caractéristiques dans une direction, que le modèle utilise pour aider à détecter l’utilisation de données radioactives même si seulement 1 % des données d’entraînement sont radioactives. C’est un défi car tout changement dans les données peut potentiellement dégrader la précision du modèle. Selon les auteurs, ce travail peut « aider les chercheurs et les ingénieurs à savoir quel jeu de données a été utilisé pour entraîner un modèle, afin de mieux comprendre comment les différents ensembles de données affectent les performances des différents réseaux neuronaux ». Cette approche semble importante pour les applications critiques de ML. Consultez le document complet &lt;a href=&quot;https://arxiv.org/pdf/2002.00937.pdf&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;REALM: Retrieval-Augmented Language Model Pre-Training&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://kentonl.com/pub/gltpc.2020.pdf&quot;&gt;REALM&lt;/a&gt; est une approche d’extraction à grande échelle qui utilise un corpus de connaissances textuelles pour pré-entraîner un modèle de langue de manière non supervisée. Les tâches abordées et évaluées à l’aide de REALM comprennent des questions ouvertes répondant à des critères de référence. Outre l’amélioration de la précision du modèle, les autres avantages comprennent les composantes de modularité et d’interprétabilité.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*MJO-yzCwsB5ydKGz7hKHVA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://kentonl.com/pub/gltpc.2020.pdf&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;créativité-et-société-&quot;&gt;Créativité et société 🎨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Permettre la présentation à distance de documents et d’affiches lors de conférences scientifiques&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
La semaine dernière, une &lt;a href=&quot;https://www.change.org/p/organizers-of-data-science-and-machine-learning-conferences-neurips-icml-aistats-iclr-uai-allow-remote-paper-poster-presentations-at-conferences&quot;&gt;pétition&lt;/a&gt; a été lancée pour permettre la présentation à distance de documents et d’affiches lors de conférences scientifiques comme celles liées au ML. Il semble que Yoshua Bengio, plaide pour que les gens aillent signer la pétition. Il l’a clairement indiqué dans son nouveau &lt;a href=&quot;https://yoshuabengio.org/2020/02/10/fusce-risus/&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Défi de l’abstraction et du raisonnement&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
François Chollet a récemment mis en ligne un &lt;a href=&quot;https://www.kaggle.com/c/abstraction-and-reasoning-challenge/overview&quot;&gt;concours Kaggle&lt;/a&gt; où il a publié le Corpus d’abstraction et de raisonnement (ARC). Il vise à encourager les utilisateurs à créer des systèmes d’IA capables de résoudre des tâches de raisonnement auxquelles ils n’ont jamais été exposés. L’espoir est de commencer à construire des systèmes d’IA plus robustes, capables de résoudre mieux et rapidement de nouveaux problèmes par eux-mêmes, ce qui pourrait aider à résoudre les applications du monde réel les plus difficiles, comme l’amélioration des voitures autonomes.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Publications de ML et NLP en 2019&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Marek Rei publie son &lt;a href=&quot;https://www.marekrei.com/blog/ml-and-nlp-publications-in-2019/&quot;&gt;analyse annuelle&lt;/a&gt; sur les statistiques en lien avec l’apprentissage machine et le NLP pour l’année 2019. Les conférences incluses dans l’analyse sont ACL, EMNLP, NAACL, EACL, COLING, TACL, CL, CoNLL, NeurIPS, ICML, ICLR, et AAAI.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;La croissance d’automates cellulaires neuronaux&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
La morphogenèse est un processus d’auto-organisation par lequel certaines créatures comme les salamandres peuvent se régénérer ou réparer des dommages corporels. Ce processus est robuste aux perturbations et de nature adaptative. Inspirés par ce phénomène biologique et par le besoin de mieux comprendre le processus, les chercheurs ont publié un &lt;a href=&quot;https://distill.pub/2020/growing-ca/&quot;&gt;article&lt;/a&gt; intitulé “Growing Neural Cellular Automata”, qui adopte un modèle différenciable pour la morphogenèse visant à reproduire les comportements et les propriétés des systèmes d’autoréparation. L’espoir est de pouvoir construire des machines autoréparatrices qui possèdent la même robustesse et plasticité que la vie biologique. En outre, cela permettrait de mieux comprendre le processus de régénération lui-même. Les applications qui peuvent en bénéficier comprennent la médecine régénératrice et la modélisation des systèmes sociaux et biologiques.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*2p62h1RaHD6d11LX8olnTA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://distill.pub/2020/growing-ca/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Visualiser l’attention des Transformers&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hendrik Strobelt a partagé ce &lt;a href=&quot;https://github.com/SIDN-IAP/attnvis&quot;&gt;repertoire&lt;/a&gt; qui montre comment construire rapidement une visualisation interactive simple de l’attention d’un Transformer à travers une application web en utilisant la bibliothèque HuggingFace et d3.js.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;SketchTransfer : Une nouvelle tâche stimulante pour explorer l’invariance des détails et les abstractions apprises par les réseaux&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/pdf/1912.11570.pdf&quot;&gt;SketchTransfer&lt;/a&gt; propose une nouvelle tâche pour tester la capacité des réseaux neuronaux à supporter l’invariance en présence/absence de détails. On a longtemps débattu du fait que les réseaux ne peuvent pas se généraliser à des variations qui n’ont pas encore été observées pendant l’entraînement, comme par exemple traiter les détails visuels manquants lorsqu’ils regardent des dessins animés. Le document examine et publie un ensemble de données pour aider les chercheurs à étudier attentivement le problème de « l’invariance des détails » en fournissant des croquis non étiquetés et des exemples étiquetés d’images réelles.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*jdYuMoHiu2yya5rHzZyjwQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1912.11570.pdf&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;outils-et-jeux-de-données-️&quot;&gt;Outils et jeux de données ⚙️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;DeepSpeed + ZeRO&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Microsoft a dévoilée une librairie d’optimisation pour l’entrainement appelée DeepSpeed. Elle est compatible avec PyTorch et peut permettre l’entrainement d’un modèle de 100 milliards de paramètres. La librairie se concentre sur quatre aspects importants de l’entrainement d’un modèle : l’échelle, la vitesse, le coût et la convivialité. DeepSpeed a été &lt;a href=&quot;https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/&quot;&gt;lancé&lt;/a&gt; en même temps que ZeRO. ZeRO est une technologie d’optimisation de la mémoire qui permet de faire du deep learning distribué à grande échelle sur GPU tout en améliorant le débit de trois à cinq fois plus que le meilleur système actuel.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*MXDI1f3cSBrY5w2g.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Une librairie pour mener des recherches rapides et efficaces sur le DL en 3D&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://ai.facebook.com/blog/-introducing-pytorch3d-an-open-source-library-for-3d-deep-learning/&quot;&gt;PyTorch3D&lt;/a&gt; est une boîte à outils open-source pour la recherche sur le DL en 3D. La librairie consiste en des implémentations rapides et optimisées d’opérateurs 3D et de fonctions de perte fréquemment utilisés. Elle est également dotée d’un moteur de rendu modulaire et différenciable qui permet de mener des recherches sur des entrées 3D complexes et de faire des prévisions 3D de haute qualité.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*VbspKMmPBUsgpdnIkd5jYA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ai.facebook.com/blog/-introducing-pytorch3d-an-open-source-library-for-3d-deep-learning/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Gestion de la configuration de projets de ML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hydra est un outil pour gérer plus efficacement les projets de ML complexes. Il est destiné à aider les chercheurs de PyTorch en offrant une réutilisation fonctionnelle des configurations. Son principal avantage est qu’il permet au programmeur de gérer la configuration comme du code de composition, ce qui signifie que le fichier de configuration peut être facilement écrasé. Hydra peut également aider à gérer automatiquement le répertoire de travail des résultats de votre projet de ML, ce qui est utile lorsque vous avez besoin de sauvegarder et d’accéder aux résultats de plusieurs expériences pour des travaux multiples. Pour en savoir plus, cliquez &lt;a href=&quot;https://medium.com/pytorch/hydra-a-fresh-look-at-configuration-for-machine-learning-projects-50583186b710&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Une boîte à outils pour l’inférence causale avec les réseaux bayésiens&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://causalnex.readthedocs.io/en/latest/01_introduction/01_introduction.html&quot;&gt;CausalNex&lt;/a&gt; est une boîte à outils pour “l’inférence causale avec les réseaux bayésiens”. L’outil vise à combiner l’apprentissage machine et le raisonnement causal pour découvrir des relations structurelles dans les données. Les auteurs ont également préparé un guide d’introduction sur le pourquoi et le comment de l’inférence causale avec les réseaux bayésiens en utilisant la librairie Python proposée.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*EYwKhdnscR7ZLuNkTqCS2Q.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://causalnex.readthedocs.io/en/latest/01_introduction/01_introduction.html&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Google Colab Pro est maintenant disponible&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Google Colab propose désormais une édition Pro, qui offre des avantages tels qu’un accès exclusif à des GPU et TPU plus rapides, des durées d’exécution plus longues et plus de mémoire.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;TyDi QA: Un benchmark pour le Question/Anwsering multilingues&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Google AI publie &lt;a href=&quot;https://ai.googleblog.com/2020/02/tydi-qa-multilingual-question-answering.html&quot;&gt;TyDi QA&lt;/a&gt;, un ensemble de données multilingues qui peut encourager les chercheurs à répondre à des questions dans des langues plus typologiquement diverses. C’est à dire qui construisent et expriment le sens de différentes manières. L’idée est de motiver les chercheurs à construire des modèles plus robustes sur des langues typologiquement éloignées, telles que l’arabe, le bengali, le coréen, le russe, le télougou et le thaï, afin de généraliser à encore plus de langues.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*1dZv5you3jigdrQ2uAKzUw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ai.googleblog.com/2020/02/tydi-qa-multilingual-question-answering.html&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Question Answering pour Node.js&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hugging Face publie une &lt;a href=&quot;https://github.com/huggingface/node-question-answering&quot;&gt;librairie&lt;/a&gt; de questions/réponses basée sur DistilBERT. Ce modèle peut fonctionner en production en utilisant Node.js avec seulement 3 lignes de code. Le modèle tire parti de la mise en œuvre rapide de Tokenizers, et de TensorFlow.js (une bibliothèque populaire pour l’utilisation de modèles d’apprentissage machine avec Javascript).&lt;/p&gt;

&lt;h1 id=&quot;ethique-en-ia-&quot;&gt;Ethique en IA 🚨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Identifier les biais subjectifs dans les textes&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ce &lt;a href=&quot;https://podcasts.apple.com/us/podcast/will-ai-help-identify-bias-or-perpetuate-it-with-diyi-yang/id1435564422?i=1000464141922&quot;&gt;podcast&lt;/a&gt; présente Diyi Yang, chercheur en sciences sociales computationnelles, qui explique comment les systèmes d’IA peuvent aider à identifier les biais subjectifs dans les informations textuelles. Il s’agit d’un domaine de recherche important impliquant les systèmes d’IA et de NLP. En particulier lorsque nous discutons de la consommation de médias textuels tels que les news qui peuvent être facilement encadrés pour biaiser les consommateurs alors qu’en réalité ils devraient viser à être plus objectifs. Du point de vue de l’application, il devient essentiel d’identifier automatiquement le biais subjectif présent dans les médias textuels afin d’aider les consommateurs à devenir plus conscients du contenu qu’ils consomment. L’épisode traite également de la manière dont l’IA peut également perpétuer le biais.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Intelligence artificielle, valeurs et alignement&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
L’essor des systèmes d’IA et la manière dont ils s’alignent sur les valeurs humaines est un domaine de recherche actif qui implique l’éthique dans les systèmes d’IA. DeepMind a récemment publié un &lt;a href=&quot;https://deepmind.com/research/publications/Artificial-Intelligence-Values-and-Alignment&quot;&gt;papier&lt;/a&gt; qui examine plus en profondeur les questions philosophiques entourant l’alignement de l’IA. Le rapport se concentre sur deux parties :  technique (c’est-à-dire comment coder les valeurs qui rendent les résultats des agents d’IA fiables) et normative (quels principes seraient justes à coder dans l’IA). Le document préconise une approche fondée sur des principes visant à préserver à préserver un traitement équitable malgré la différence de croyances et d’opinions.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Sur l’audit des systèmes d’IA&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
VentureBeat rapporte que Google Researchers, en collaboration avec d’autres groupes, a créé un framework appelé SMACTR qui permet aux ingénieurs de vérifier les systèmes d’IA. La raison de ce travail est de combler le fossé de responsabilité qui existe avec les systèmes d’IA actuels qui sont mis dans la nature pour être utilisés par les consommateurs. Pour plus d’informations, lire les deux documents suivants :  &lt;a href=&quot;https://venturebeat.com/2020/01/30/google-researchers-release-audit-framework-to-close-ai-accountability-gap/&quot;&gt;ici&lt;/a&gt; et &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3351095.3372873&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;articles-et-blog-️&quot;&gt;Articles et Blog ✍️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;La distillation de modèle en NLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans un &lt;a href=&quot;https://soundcloud.com/nlp-highlights/104-model-distillation-with-victor-sanh-and-thomas-wolf&quot;&gt;podcast&lt;/a&gt; de NLP Highlights, Thomas Wolf et Victor Sanh parlent de la distillation de modèles et de la façon dont elle peut être utilisée comme une approche réalisable pour comprimer de grands modèles comme BERT. Ce concept est discuté plus en détail dans la méthode qu’ils proposent, appelée &lt;a href=&quot;https://arxiv.org/abs/1910.01108&quot;&gt;DistilBERT&lt;/a&gt;, dans laquelle ils construisent des modèles plus petits (basés sur la même architecture qu’un modèle plus grand) pour essayer d’imiter le comportement du modèle plus grand. En substance, le petit modèle (l’étudiant) essaie de s’adapter à la distribution de probabilité de l’enseignant.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;em&gt;**BERT, ELMo, &amp;amp; GPT-2: Dans quelle mesure les représentations contextuelles des mots sont-elles contextualisées ? **&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
On a beaucoup parlé du succès des méthodes contextualisées comme BERT pour aborder une grande variété de tâches complexes de NLP. Dans cet &lt;a href=&quot;https://kawine.github.io/blog/nlp/2020/02/03/contextual.html&quot;&gt;article&lt;/a&gt;, Kawin Ethayarajh tente de répondre à la question qui consiste à savoir comment sont contextualisés les mots dans les modèles comme BERT, ELMo et le GPT-2. Les sujets abordés comprennent les mesures de la contextualité, la spécificité du contexte et les comparaisons entre les embeddings statiques et les représentations contextualisées.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*70aIv1Fkkz4rnHgQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://kawine.github.io/blog/nlp/2020/02/03/contextual.html&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Sparsity in Neural Networks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
François Lagunas, a écrit cet &lt;a href=&quot;https://medium.com/huggingface/is-the-future-of-neural-networks-sparse-an-introduction-1-n-d03923ecbd70&quot;&gt;article Medium&lt;/a&gt; pour discuter de son optimisme quant à l’adoption de tenseurs clairsemés dans les modèles de réseaux de neurones. L’espoir est d’utiliser une forme de rareté pour réduire la taille des modèles actuels qui, à un moment donné, deviennent peu pratiques en raison de leur taille et de leur vitesse. Ce concept pourrait être intéressant à explorer en ML en raison de la taille même des modèles actuels comme les Transformers. Cependant, les détails de mise en œuvre ne sont pas aussi clairs du point de vue des outils de développement disponibles, et c’est quelque chose sur lequel la communauté travaille déjà.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Entraîner votre propre modèle linguistique&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous souhaitez apprendre à entrainer un modèle de zéro, consultez ce &lt;a href=&quot;https://huggingface.co/blog/how-to-train&quot;&gt;tutoriel&lt;/a&gt; d’Hugging Face. Ils utilisent évidemment leurs propres bibliothèques Transformers et Tokenizers pour entraîner le modèle.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Tokenizers: Comment les machines lisent&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Cathal Horan a publié un &lt;a href=&quot;https://blog.floydhub.com/tokenization-nlp/&quot;&gt;article&lt;/a&gt; sur la manière dont les modèles de NLP les plus récents utilisent les tokenizers. Il explique également pourquoi la tokenisation est un domaine de recherche actif, passionnant et important. L’article vous montre même comment entraîner vos propres tokenizers en utilisant des méthodes de tokenisation comme SentencePiece et WordPiece.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Vkjw5n9Sz0Was43haVNJMg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.floydhub.com/tokenization-nlp/%27&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education 🎓&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;ML à l’université d’Amsterdam&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Vous pouvez désormais suivre en ligne le &lt;a href=&quot;https://mlvu.github.io/&quot;&gt;cours d’apprentissage machine 2020 MLVU&lt;/a&gt;, qui comprend des diapositives, des &lt;a href=&quot;https://www.youtube.com/watch?v=excCZSTJEPs&amp;amp;feature=youtu.be&quot;&gt;vidéos&lt;/a&gt; et le programme. Il s’agit d’une introduction au ML, mais il comporte également d’autres sujets liés à l’apprentissage approfondi, tels que les VAE et les GAN.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*zFpU2rQL5Fby7X3boJyQNg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mlvu.github.io/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Ressources mathématiques pour le ML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Suzana Ilić et le Machine Learning Tokyo (MLT) ont fait un travail remarquable en termes de démocratisation de l’éducation au ML. Par exemple, consultez ce &lt;a href=&quot;https://github.com/Machine-Learning-Tokyo/Math_resources&quot;&gt;répertoire&lt;/a&gt; qui présente une collection de ressources en ligne gratuites pour apprendre les fondements des concepts mathématiques utilisés en ML.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Introduction au Deep Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Suivez le cours “Introduction to Deep Learning” du MIT sur ce &lt;a href=&quot;http://introtodeeplearning.com/&quot;&gt;site&lt;/a&gt;. De nouveaux cours seront publiés chaque semaine et toutes les diapositives, vidéos et codes seront publiés.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Deep Learning avec PyTorch&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Alfredo Canziani a publié les diapositives et les notebooks pour son mini-cours sur l’apprentissage profond avec PyTorch. Le dépôt contient également un &lt;a href=&quot;https://atcold.github.io/pytorch-Deep-Learning/&quot;&gt;site web complémentaire&lt;/a&gt; qui comprend des descriptions textuelles des concepts enseignés dans le cours.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Missing Semester of Your CS&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Le “&lt;a href=&quot;https://missing.csail.mit.edu/&quot;&gt;Missing Semester of Your CS&lt;/a&gt;” est un cours en ligne pouvant être utile aux spécialistes des données ayant une formation autre que celle du développement. Il comprend des sujets tels que les outils shell, les scripts et le contrôle de version. Le cours a été publié par des membres du corps enseignant du MIT.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*weUnTXxmHxYf-B2DDaslvw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://missing.csail.mit.edu/2020/shell-tools/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Deep Learning avancé&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
La CMU a publié les diapositives et le programme du cours “&lt;a href=&quot;https://andrejristeski.github.io/10707-S20/syllabus.html&quot;&gt;Advanced Deep Learning&lt;/a&gt;” qui comprend des sujets tels que les modèles autorégressifs, les modèles générateurs et l’apprentissage autosurveillé/prédictif. Le cours s’adresse aux étudiants de master ou de doctorat ayant une formation avancée en ML.&lt;/p&gt;

&lt;h1 id=&quot;mentions-spéciales-️&quot;&gt;Mentions spéciales ⭐️&lt;/h1&gt;

&lt;p&gt;Xu et ses collaborateurs (2020) ont proposé une &lt;a href=&quot;https://arxiv.org/abs/2002.02925&quot;&gt;méthode&lt;/a&gt; pour remplacer et compresser progressivement un modèle BERT en le divisant en ses composantes d’origine. Le modèle proposé surpasse les autres approches de distillation sur le référentiel GLUE.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Le cours “&lt;a href=&quot;https://compstat-lmu.github.io/lecture_i2ml/index.html&quot;&gt;Introduction à l’apprentissage machine&lt;/a&gt;” couvre les bases du ML, la régression supervisée, les forêts aléatoires, le tuning des paramètres et bien d’autres sujets fondamentaux du ML.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Le modèle grec de BERT (&lt;a href=&quot;https://huggingface.co/nlpaueb/bert-base-greek-uncased-v1&quot;&gt;GreekBERT&lt;/a&gt;) est maintenant disponible sur Transformers.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Jeremy Howard publie un &lt;a href=&quot;https://arxiv.org/abs/2002.04688&quot;&gt;article&lt;/a&gt; décrivant la librairie fastai. Il s’agit d’une lecture recommandée aux développeurs de logiciels qui travaillent à la création et à l’amélioration des librairies d’apprentissage profond et de ML.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Deeplearning.ai complète la publication des quatre cours de TensorFlow : &lt;a href=&quot;https://www.coursera.org/specializations/tensorflow-data-and-deployment&quot;&gt;Data and Deployment Specialization&lt;/a&gt;. Cette spécialisation vise principalement à apprendre aux développeurs comment déployer efficacement des modèles dans différents scénarios et utiliser les données de manière intéressante.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sebastian Raschka a récemment publié un &lt;a href=&quot;https://arxiv.org/abs/2002.04803&quot;&gt;article&lt;/a&gt; intitulé “Machine Learning in Python” : Principaux développements et tendances technologiques en matière de science des données, d’apprentissage automatique et d’intelligence artificielle”. Ce document est un examen complet du paysage des outils d’apprentissage machine. Il permet de comprendre les avantages de certaines librairies et les concepts utilisés dans l’ingénierie ML. En outre, un mot sur l’avenir des bibliothèques d’apprentissage machine basées sur Python est fourni.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Vous pouvez retrouver la précédente newsletter &lt;a href=&quot;https://dair.ai/NLP_Newsletter_-3_-FR/&quot;&gt;ici&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous avez des jeux de données, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine édition de la newletter, n’hésitez pas à me contacter à ellfae@gmail.com ou par message sur &lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;Abonnez-vous&lt;/a&gt; pour recevoir les prochains numéros dans votre boîte mail.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_-4_-FR/&quot;&gt;NLP Newsletter [FR] #4: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,…&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 09, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter [FR] #3: Flax, Thinc, Language-specific BERT models, Meena, Flyte, LaserTagger,…]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_-3_-FR/" />
  <id>https://dair.ai/NLP_Newsletter_#3_[FR]</id>
  <published>2020-03-09T00:00:00-05:00</published>
  <updated>2020-03-09T00:00:00-05:00</updated>
  <author>
    <name>Loïck BOURDOIS</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2400/1*qaOM0D2tfy3chvnWRdycGA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;avant-propos&quot;&gt;Avant-propos&lt;/h1&gt;
&lt;p&gt;Bienvenue à la Newsletter sur le NLP ! Le troisième numéro traite de sujets tels que l’amélioration des agents conversationnels, les versions de BERT spécifiques à chaque langue, les ensembles de données gratuits, les versions des bibliothèques d’apprentissage approfondi, et bien plus encore.&lt;/p&gt;

&lt;h1 id=&quot;publications-&quot;&gt;Publications 📙&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Versions de BERT spécifiques à chaque langue&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
J’ai perdu le compte du nombre de modèles BERT spécifiques à chaque langue, mais voici quelques-unes des versions récentes :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;BERT néerlandais (&lt;a href=&quot;https://arxiv.org/abs/2001.06286&quot;&gt;RobBERT&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1912.09582&quot;&gt;BERTje&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://deepset.ai/german-bert&quot;&gt;BERT allemand&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/neuralmind-ai/portuguese-bert&quot;&gt;BERT portugais&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;BERT français (&lt;a href=&quot;https://arxiv.org/abs/1911.03894&quot;&gt;CamemBERT&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1912.05372&quot;&gt;FlauBERT&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;BERT italien (&lt;a href=&quot;http://ceur-ws.org/Vol-2481/paper57.pdf&quot;&gt;AlBERTo&lt;/a&gt;, &lt;a href=&quot;https://github.com/musixmatchresearch/umberto&quot;&gt;UmBERTo&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;BERT espagnol (&lt;a href=&quot;https://github.com/dccuchile/beto&quot;&gt;BETO&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;BERT arabe (&lt;a href=&quot;https://colab.research.google.com/drive/1KSy89fAkWt6EGfnFQElDjXrBror9lIZh&quot;&gt;araBERT&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
Notez que la plupart de ces modèles sont également disponibles par le biais de la librairie de Transformers d’Hugging Face, qui a récemment été mise à jour avec la version &lt;a href=&quot;https://github.com/huggingface/transformers/releases&quot;&gt;2.4.1&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Résultats trop optimistes de prédictions sur des données déséquilibrées : défauts et avantages de l’application du sur-échantillonnage&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Cette &lt;a href=&quot;https://arxiv.org/abs/2001.06296&quot;&gt;publication&lt;/a&gt; révèle et examine en détail certains des défauts et des avantages de l’application du sur-échantillonnage pour traiter les jeux de données déséquilibrés avant de les partitionner. En outre, le travail reproduit des études antérieures et identifie cette faille méthodologique qui produit des résultats trop optimistes.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Encoder, étiqueter et réaliser : une approche contrôlable et efficace pour la génération de texte&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Afin de réduire l’effet d’&lt;a href=&quot;https://arxiv.org/abs/1910.08684&quot;&gt;hallucination&lt;/a&gt; (production de sorties non supportées par le texte d’entrée) commun aux méthodes de génération de texte basées sur seq2seq, un groupe d’ingénieurs de Google a ouvert une méthode de génération de texte appelée &lt;a href=&quot;https://ai.googleblog.com/2020/01/encode-tag-and-realize-controllable-and.html&quot;&gt;LaserTagger&lt;/a&gt;.
L’idée principale de cette méthode est de produire des sorties en marquant des mots avec des opérations d’édition prédites (par exemple, KEEP, DELETE-ADD,, etc.) et en les appliquant aux mots d’entrée dans une étape dite de réalisation. Cette méthode remplace celle de génération de texte courante qui ne fait que générer des sorties à partir de zéro, ce qui est généralement lent et sujet à des erreurs. Le modèle offre d’autres avantages en plus de générer moins d’erreurs, tels que la possibilité de prévoir en parallèle les opérations d’édition tout en conservant une bonne précision et en surpassant une base de référence BERT dans des scénarios avec un nombre réduit d’exemples d’entraînement.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/0*OJN4pNgrQoS2STAX.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt; &lt;a href=&quot;https://ai.googleblog.com/2020/01/encode-tag-and-realize-controllable-and.html&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Les réseaux neuronaux convolutifs comme modèle du système visuel : passé, présent et futur&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Grace Lindsay a publié ce &lt;a href=&quot;https://arxiv.org/abs/2001.07092&quot;&gt;rapport&lt;/a&gt; sur l’histoire des CNN et sur la manière dont ils sont évalués en tant que modèles de vision biologique, c’est-à-dire comment les représentations des CNN se comparent à celles du cerveau ? La discussion sur les nouvelles possibilités d’utilisation des CNN pour la recherche sur la vision est vivement recommandée aux lecteurs.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*SngMqzPQJigR5A3AzeJGDQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2001.07092&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Multilingual Denoising Pre-training for Neural Machine Translation&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Facebook AI a publié &lt;a href=&quot;https://arxiv.org/pdf/2001.08210.pdf&quot;&gt;mBART&lt;/a&gt;, une méthode basée sur un auto-encodeur de débruitage multilingue seq2seq pré-entrainé sur des corpus monolingues à grande échelle pour la traduction automatique dans 25 langues.
Le texte d’entrée implique le masquage des phrases et la permutation des phrases (bruits). Un modèle basé sur un Transformer est appris pour reconstruire le texte dans plusieurs langues. Le modèle autorégressif complet n’est entraîné qu’une seule fois et peut être ajusté sur n’importe quelle paire de langues sans impliquer de modifications spécifiques à la tâche ou à la langue. Les problèmes de traduction au niveau du document et de la phrase sont abordés. En plus de montrer des gains de performance, les auteurs affirment que la méthode fonctionne bien sur la traduction automatique à faible ressource.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*aigX70Om2rEaI7OoTcpyGA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2001.08210.pdf&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Sur l’amélioration des agents conversationnels&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html&quot;&gt;Meena&lt;/a&gt; est un agent conversationnel qui vise à mener des conversations plus sensibles et plus spécifiques ;  des mesures définies pour saisir les attributs importants d’une conversation humaine (par exemple, la fluidité). Le modèle apprend le contexte de la conversation via un encodeur et formule une réponse sensible via le décodeur. Il est signalé que l’amélioration de la qualité des conversations a été possible en considérant des décodeurs plus puissants.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Vous pouvez également prendre connaissance des &lt;a href=&quot;https://venturebeat.com/2020/01/31/with-googles-meena-are-ai-assistants-about-to-get-a-lot-smarter/&quot;&gt;réflexions&lt;/a&gt; d’Alan Nichol (co-fondateur du siège de Rasa) sur ce travail.&lt;/p&gt;

&lt;h1 id=&quot;créativité-et-société-&quot;&gt;Créativité et société 🎨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Test de compréhension de la lecture et analyseur de sentiments&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ming Cheuk a conçu cette &lt;a href=&quot;https://littlealbert.now.sh/#/&quot;&gt;application&lt;/a&gt; qui permet de tester les capacités de compréhension de lecture d’&lt;a href=&quot;https://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html&quot;&gt;ALBERT&lt;/a&gt;. ALBERT est une version plus petite de BERT pour l’apprentissage des représentations des langues. L’auteur explique plus en détail le projet et les approches utilisées dans ce &lt;a href=&quot;https://www.spark64.com/post/machine-comprehension&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hendrik Strobelt a également publié un petit &lt;a href=&quot;https://github.com/HendrikStrobelt/sentimenter_minimal_hai&quot;&gt;projet&lt;/a&gt; dans lequel il montre comment réaliser un prototype d’un analyseur de sentiments interactif.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*kgKeL3svHqScr0Wjnfe0Cg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://littlealbert.now.sh/#/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Le parcours d’un chercheur autodidacte sur l’IA chez Google&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans cet &lt;a href=&quot;https://blog.floydhub.com/emils-story-as-a-self-taught-ai-researcher/&quot;&gt;entretien&lt;/a&gt; Emil, un chercheur de ML à Google Art &amp;amp; Culture, parle de son parcours en tant que chercheur autodidacte.&lt;/p&gt;

&lt;h1 id=&quot;outils-et-jeux-de-données-️&quot;&gt;Outils et jeux de données ⚙️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Jeux de données en libre accès&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://blog.google/products/search/discovering-millions-datasets-web/&quot;&gt;Google Dataset Search&lt;/a&gt; fournit désormais jusqu’à 25 millions de jeux de données. Il s’agit essentiellement d’un moteur de recherche pour les ensembles de données.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
La base de données &lt;a href=&quot;https://quantumstat.com/dataset/dataset.html&quot;&gt;Big Bad NLP&lt;/a&gt; est un site web où vous pouvez rechercher une base de données dédiée de plus de 200 jeux de données de NLP de tous types pour des tâches telles que le raisonnement, l’analyse des sentiments, la réponse aux questions, l’inférence d’implication, etc…&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*uYwA0snqOdKYyTJ56edtyA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Librairie d’apprentissage par renforcement&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Chris Nota a développé et publié une &lt;a href=&quot;https://github.com/cpnota/autonomous-learning-library&quot;&gt;librairie PyTorch&lt;/a&gt; pour la construction d’agents d’apprentissage par renforcement basés sur des algorithmes populaires tels que DQN, PPO et DDPG. L’accent de la librairie est mis sur la conception orientée objet et sur la mise en œuvre et l’évaluation rapides de nouveaux agents d’apprentissage par renforcement.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Explicabilité et interprétabilité du ML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous travaillez actuellement avec des modèles linguistiques basés sur des textes et que vous souhaitez comprendre comment les interpréter plus facilement lorsqu’ils sont appliqués à différentes tâches linguistiques, alors vous pourriez être intéressé par &lt;a href=&quot;https://captum.ai/&quot;&gt;Captum&lt;/a&gt;. Captum est une librairie d’interprétabilité qui peut être utilisée pour analyser l’importance des caractéristiques, interpréter des modèles de texte et de vision, interpréter des modèles multimodaux, et d’autres modèles tels que BERT utilisé pour la réponse aux questions.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous vous intéressez à l’explicabilité des modèles, cet &lt;a href=&quot;https://www.kaggle.com/learn/machine-learning-explainability&quot;&gt;tutoriels&lt;/a&gt; peuvent également vous intéresser.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Libraries de ML et DL&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
L’équipe de Google Research a publié &lt;a href=&quot;https://github.com/google-research/flax/tree/prerelease&quot;&gt;Flax&lt;/a&gt;, une libririe de réseaux neuronaux flexible et puissante basée sur &lt;a href=&quot;https://github.com/google/jax&quot;&gt;JAX&lt;/a&gt; qui fournit un framework pour le calcul rapide et l’entraînement de modèles de ML en utilisant les API typiques de Numpy.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*LSWFZM-xMV-GnvGl_lC-sg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Flax syntax&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://thinc.ai/&quot;&gt;Thinc&lt;/a&gt; est une librairie légère de deep learning développée par les créateurs de spaCy. Elle offre des API de programmation fonctionnelle pour composer, configurer et déployer des modèles personnalisés construits avec des librairies comme PyTorch et TensorFlow.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Lyft lance &lt;a href=&quot;https://eng.lyft.com/introducing-flyte-cloud-native-machine-learning-and-data-processing-platform-fb2bb3046a59&quot;&gt;Flyte&lt;/a&gt;, une plateforme, prête pour la production et sans serveur, pour le déploiement dde travail de traitement de données et de ML.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Un outil pour l’IA conversationnelle&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/deepmipt/DeepPavlov&quot;&gt;DeepPavlov&lt;/a&gt; offre une solution gratuite et facile à utiliser pour la construction de systèmes de dialogue et de systèmes conversationnels complexes. DeepPavlov est livré avec plusieurs composants prédéfinis pour résoudre les problèmes liés au NLP. Il intègre BERT (y compris le BERT conversationnel) dans trois tâches : la classification de textes, la reconnaissance d’entités nommées (et le marquage des séquences en général) et la réponse aux questions. En conséquence, il a permis d’améliorer considérablement toutes ces tâches. (&lt;a href=&quot;https://colab.research.google.com/github/deepmipt/dp_notebooks/blob/master/DP_tf.ipynb&quot;&gt;Google Colab&lt;/a&gt; | &lt;a href=&quot;https://medium.com/tensorflow/deeppavlov-an-open-source-library-for-end-to-end-dialog-systems-and-chatbots-31cf26849e37&quot;&gt;Blog&lt;/a&gt; | &lt;a href=&quot;https://demo.deeppavlov.ai/#/en/textqa&quot;&gt;Démo&lt;/a&gt;).&lt;/p&gt;

&lt;h1 id=&quot;ethique-en-ia-&quot;&gt;Ethique en IA 🚨&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Reconnaissance faciale et vie privée&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Le New York Times a rédigé un article sur les différentes perspectives concernant la vie privée impliquant la technologie de reconnaissance faciale. Ce reportage porte sur une “société secrète” appelée Clearview qui utiliserait la technologie d’IA pour construire une reconnaissance faciale universelle à partir d’images récupérées sur les réseaux sociaux tels que Twitter, Facebook et YouTube, etc… Cette technologie soulève des inquiétudes quant au respect de la vie privée, mais elle serait également utilisée principalement pour l’application de la loi. Pour en savoir plus, cliquez &lt;a href=&quot;https://www.nytimes.com/2020/01/18/technology/clearview-privacy-facial-recognition.html&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Progrès de l’IA au niveau humain&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans cet &lt;a href=&quot;https://fortune.com/longform/ai-artificial-intelligence-big-tech-microsoft-alphabet-openai/&quot;&gt;article&lt;/a&gt;, Jeremy Kahn discute en détail de la différence entre « l’IA étroite » et « l’IA générale » dans le contexte des progrès actuels de la technologie. Outre les nombreux sujets abordés, de nombreuses questions se posent sur les bénéfices de la réalisation de l’IA générale. L’article mentionne également l’intérêt récent des grandes entreprises technologiques qui investissent dans ces efforts. L’article inclut plusieurs préoccupations soulevées par des chercheurs respectés qui dénoncent le comportement “irresponsable et contraire à l’éthique” de certains organismes de recherche qui tentent de manipuler les récits sur l’IA à leur profit.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Technologies d’IA préservant la vie privée&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
L’un des efforts mené afin de promouvoir une IA ethique, responsable et respectant la vie privée est celui de la communauté [OpenMined] https://twitter.com/OpenMinedOrg). Si vous voulez en savoir plus, vous pouvez écouter Andrew Trask, parler de cette initiative dans cette &lt;a href=&quot;https://www.youtube.com/watch?v=4zrU54VIK6k&quot;&gt;vidéo&lt;/a&gt; faisant partie de la série de conférences du MIT sur l’apprentissage profond.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Comprendre l’éthique et la sécurité&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Le Dr David Leslie a publié ce &lt;a href=&quot;https://www.turing.ac.uk/sites/default/files/2019-06/understanding_artificial_intelligence_ethics_and_safety.pdf&quot;&gt;rapport&lt;/a&gt; très détaillé sur des sujets qui aident à mieux comprendre l’IA dans le contexte de l’éthique et de la sécurité. Il vise à aider les développeurs et les chercheurs à mieux concevoir et mettre en œuvre des systèmes d’IA pour le secteur public.
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*Ye09aVDP93RKsLc12PXqNQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.turing.ac.uk/sites/default/files/2019-06/understanding_artificial_intelligence_ethics_and_safety.pdf&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;articles-et-blog-️&quot;&gt;Articles et Blog ✍️&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Tutoriel sur l’accélération de la tokenisation&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Steven van de Graaf a écrit cet [article] https://towardsdatascience.com/a-small-timing-experiment-on-the-new-tokenizers-library-a-write-up-7caab6f80ea6) à propos des performances de la librairie &lt;a href=&quot;https://github.com/huggingface/tokenizers&quot;&gt;Tokenizers&lt;/a&gt; d’Hugging Face par rapport au tokenizer standard intégré utilisé dans la librairie &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;Transformers&lt;/a&gt;. Steven constate qu’une implémentation prend 10,6 secondes pour tokenizer 1 million de phrases et que le temps d’exécution est divisé par 9.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Les modèles linguistiques peuvent-ils vraiment comprendre ?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
The Gradient a récemment publié ce &lt;a href=&quot;https://thegradient.pub/gpt2-and-the-nature-of-intelligence/&quot;&gt;post&lt;/a&gt; de Gary Marcus où il discute de ce qu’il croit être des défauts fondamentaux derrière des modèles de langage comme GPT-2. L’argument principal de Gary Marcus est qu’un modèle entraîné pour pouvoir prédire le mot suivant n’est pas nécessairement un modèle capable de comprendre ou de raisonner. C’est-à-dire que “la prédiction est une composante de la compréhension, pas l’ensemble”.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Curriculum for Reinforcement Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Lillian Weng &lt;a href=&quot;https://lilianweng.github.io/lil-log/2020/01/29/curriculum-for-reinforcement-learning.html&quot;&gt;résume&lt;/a&gt; plusieurs approches basées sur les curriculum et la façon dont cela peut être utilisé pour entraîner de façon efficace des agents d’apprentissage par renforcement. Weng aborde les défis de la conception d’une telle approche, qui nécessite généralement de trier la complexité des tâches et de fournir au modèle une séquence de tâches dont le niveau de difficulté augmente au cours de l’entraînement.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/0*B-t_sNMjKiOb_Y3Z.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://lilianweng.github.io/lil-log/2020/01/29/curriculum-for-reinforcement-learning.html&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Introduction à NumPy&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Anne Bonner a récemment publié ce tutoriel détaillé présentant les bases de &lt;a href=&quot;https://numpy.org/devdocs/user/absolute_beginners.html&quot;&gt;NumPy&lt;/a&gt;. Il est constitue une base solide pour les personnes souhaitant s’initier à cette librairie.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/0*FmUSU_dh-_cqGUk_.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://numpy.org/devdocs/user/absolute_beginners.html&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education 🎓&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Fondements de l’apprentissage machine et de l’inférence statistique&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Anima Anandkumar, du Caltech, a publié un cours intitulé “Fondements de l’apprentissage machine et de l’inférence statistique”. Le cours se concentre sur les concepts de ML tels que les matrices, les tenseurs, l’optimisation, les modèles probabilistes, les réseaux de neurones et bien plus encore. Ce cours aborde les aspects théoriques du ML. (&lt;a href=&quot;https://www.youtube.com/playlist?list=PLVNifWxslHCDlbyitaLLYBOAEPbmF1AHg&quot;&gt;la playlist vidéo&lt;/a&gt; | &lt;a href=&quot;http://tensorlab.cms.caltech.edu/users/anima/cms165-2020.html&quot;&gt;le programme du cours&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Série de conférences sur le DL&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind s’est associé à l’UCL pour lancer une &lt;a href=&quot;https://www.eventbrite.co.uk/o/ucl-x-deepmind-deep-learning-lecture-series-general-29078980901&quot;&gt;série de 12 conférences&lt;/a&gt; sur l’apprentissage profond données par des chercheurs de DeepMind.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;~7 million de programmes d’études&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://opensyllabus.org/&quot;&gt;Open Syllabus&lt;/a&gt; est une organisation à but non lucratif qui utilise le crowdsourcing pour mettre en place un programme d’enseignement supérieur dans une base de données en ligne. Elle contient actuellement environ sept millions de programmes.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*fwQIhfb2VWuwQJM_LaLehg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://opensyllabus.org/results-list/titles?size=50&amp;amp;fields=Computer%20Science&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Discuter, partager et apprendre sur le ML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.reddit.com/r/ResearchML/&quot;&gt;r/ResearchML&lt;/a&gt; est une nouvelle sous-rubrique de reddit consacré au ML. Celui-ci est davantage axé sur la recherche et encourage des discussions plus approfondies.&lt;/p&gt;

&lt;h1 id=&quot;mentions-spéciales-️&quot;&gt;Mentions spéciales ⭐️&lt;/h1&gt;

&lt;p&gt;Découvrez comment &lt;a href=&quot;https://github.blog/2020-01-22-how-we-built-good-first-issues/&quot;&gt;GitHub&lt;/a&gt; exploite l’apprentissage machine pour repérer les problèmes faciles et personnalisés des développeurs afin qu’ils puissent s’attaquer aux questions qui correspondent à leurs intérêts. Cela encourage des contributions plus rapides et plus nombreuses de la part des contributeurs de logiciels libres.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sebastian Ruder a publié une nouvelle &lt;a href=&quot;http://newsletter.ruder.io/issues/nlp-progress-restrospectives-and-look-ahead-new-nlp-courses-independent-research-initiatives-interviews-lots-of-resources-217744&quot;&gt;newsletter&lt;/a&gt;. On y trouve une mise à jour des progrès du NLP, des rétrospectives sur la dernière décennie, de nouveaux cours de NLP, et d’autres sujets.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Jetez un coup d’œil à &lt;a href=&quot;https://github.com/NERSC/dl4sci-tf-tutorials&quot;&gt;ces notebooks TensorFlow 2.0&lt;/a&gt; qui vont de CycleGAN à Transformers en passant par les tâches de sous-titrage d’images. Ils ont été rendus publics par l’école d’apprentissage profond pour la science du LBNL.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Un &lt;a href=&quot;https://engineering.papercup.com/posts/bayesian-neural-nets/&quot;&gt;blog&lt;/a&gt; pour s’initier aux réseaux neuronaux bayésiens.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
John Schulman &lt;a href=&quot;http://joschu.net/blog/opinionated-guide-ml-research.html&quot;&gt;partage&lt;/a&gt; quelques conseils pour les futurs chercheurs du LBNL sur la façon de mieux choisir les problèmes de recherche et d’être plus stratégique dans la réalisation des tâches de recherche. John partage également des conseils pour le développement personnel et le progrès continu.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Vous pouvez retrouver la précédente newsletter &lt;a href=&quot;https://dair.ai/NLP_Newsletter_-2_-FR/&quot;&gt;ici&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous avez des jeux de données, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine édition de la newletter, n’hésitez pas à me contacter à ellfae@gmail.com ou par message sur &lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;Abonnez-vous&lt;/a&gt; pour recevoir les prochains numéros dans votre boîte mail.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_-3_-FR/&quot;&gt;NLP Newsletter [FR] #3: Flax, Thinc, Language-specific BERT models, Meena, Flyte, LaserTagger,…&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 09, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter [FR] #2: Reformer, DeepMath, ELECTRA, TinyBERT, VizSeq, Open-Sourcing ML,…]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_-2_-FR/" />
  <id>https://dair.ai/NLP_Newsletter_#2_[FR]</id>
  <published>2020-03-09T00:00:00-05:00</published>
  <updated>2020-03-09T00:00:00-05:00</updated>
  <author>
    <name>Loïck BOURDOIS</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*mgWc3FhHPRfCxdPir6wSeg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;avant-propos&quot;&gt;Avant-propos&lt;/h1&gt;
&lt;p&gt;Bienvenue à cette nouvelle newsletter consacrée au NLP ! Ce deuxième numéro aborde des sujets qui vont de l’interprétabilité des modèles au repliement des protéines en passant par l’apprentissage par transfert actif.&lt;/p&gt;

&lt;h1 id=&quot;publications-&quot;&gt;Publications 📙&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Sur l’incertitude de la confiance dans un modèle&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Un article récent de Google AI, publié au NeurIPS, examine si les probabilités sorties par un modèle reflètent sa capacité à prévoir les données décalées et hors distribution. Ils ont constaté que les ensembles profonds ont de meilleures performances (c’est-à-dire une meilleure incertitude du modèle) sur le décalage de l’ensemble de données, tandis que d’autres modèles ne sont pas devenus de plus en plus incertains sur le décalage de l’ensemble de données, mais se sont plutôt trompés avec confiance. (Lire l’article &lt;a href=&quot;https://arxiv.org/abs/1906.02530&quot;&gt;ici&lt;/a&gt; et le résumé &lt;a href=&quot;https://ai.googleblog.com/2020/01/can-you-trust-your-models-uncertainty.html&quot;&gt;ici&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*NrsUnHS1thKq3ChK.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;image corruption —&lt;/em&gt; &lt;a href=&quot;https://ai.googleblog.com/2020/01/can-you-trust-your-models-uncertainty.html&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Généralisation systématique&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Un &lt;a href=&quot;https://www.semanticscholar.org/paper/Systematic-Generalization%3A-What-Is-Required-and-Can-Bahdanau-Murty/6c7494a47cc5421a7b636c244e13586dc2dab007&quot;&gt;travail&lt;/a&gt; intéressant publié dans ICLR présente une comparaison entre les modèles modulaires et les modèles génériques concernant leur efficacité pour la généralisation systématique dans la compréhension des langues. Sur la base d’une évaluation effectuée des questions/réponses en lien avec une &lt;a href=&quot;https://arxiv.org/abs/1909.01860&quot;&gt;tâche visuelle&lt;/a&gt;, les auteurs concluent qu’il peut être nécessaire d’utiliser des régularisateurs et des antécédents explicites pour parvenir à une généralisation systématique.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Le Reformer&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Un Transformer est limité au niveau de la fenêtre de contexte qu’il peut couvrir en raison des calculs coûteux effectués dans la couche d’attention. Ainsi, il est possible d’appliquer le Transformer qu’à des tailles de texte limitées ou de générer que de courtes phrases / morceaux de musique. GoogleAI a récemment publié une variante efficace du modèle Transformer, appelée &lt;a href=&quot;https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html&quot;&gt;Reformer&lt;/a&gt;. L’objectif principal de cette méthode est de pouvoir traiter des séquences de contexte beaucoup plus grandes tout en réduisant les besoins de calcul et en améliorant l’efficacité de la mémoire. Reformer utilise le “locality-sensitive-hashing” (&lt;a href=&quot;https://fr.wikipedia.org/wiki/Locality_sensitive_hashing&quot;&gt;LSH&lt;/a&gt;) pour regrouper des vecteurs similaires et créer des segments à partir de ceux-ci. Cela permet ainsi un traitement en parallèle. L’attention est ensuite portée sur ces segments plus petits et sur les parties voisines correspondantes, réduisant la charge de calcul. L’efficacité de la mémoire est obtenue grâce à des couches réversibles qui permettent de recalculer à la demande les informations d’entrée de chaque couche tout en s’entraînant par rétropropagation. C’est une technique simple qui évite au modèle de devoir stocker en mémoire les activations. Une description de ce modèle est disponible en langue française sur ce &lt;a href=&quot;https://lbourdois.github.io/blog/nlp/Reformer/&quot;&gt;site&lt;/a&gt;. Pour voir comment le Reformer peut être appliqué à une tâche de génération d’images, je vous invite à consulter ce &lt;a href=&quot;https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/image_generation.ipynb&quot;&gt;Google Colab&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*Q6FHJ5bqZRCrBAp9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
 &lt;strong&gt;&lt;em&gt;Adaptation non supervisée de domaines pour la classification de textes&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ce &lt;a href=&quot;https://arxiv.org/abs/2001.04362&quot;&gt;travail&lt;/a&gt; propose une combinaison de mesures de distance qui incorporées dans une fonction de perte lors de l’entraînement d’un modèle, permet d’améliorer l’adaptation du domaine non supervisé. Le modèle est étendu à un modèle « DistanceNet Bandit ». Le problème clé abordé par cette méthode est de comprendre comment traiter la dissimilitude entre les données de différents domaines.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Amélioration des représentations contextualisées&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Ce &lt;a href=&quot;https://openreview.net/forum?id=r1xMH1BtvB&quot;&gt;document&lt;/a&gt; propose une tâche de pré-entraînement, appelée token detection, qui se révèle plus efficace pour entraîner un modèle linguistique que les méthodes de basées sur un pré-entaînement avec des masques telles que BERT par exemple. Le modèle est baptisé ELECTRA et ses représentations contextualisées surpassent celles de BERT et XLNET à données identiques et à taille de modèle identique. La méthode fonctionne particulièrement bien sur des machines à faible capacité de calcul. Il s’agit d’un effort pour construire des modèles de langage plus petits et moins chers.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Interprétabilité des modèles&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Distill a publié un document intitulé “&lt;a href=&quot;https://distill.pub/2020/attribution-baselines/&quot;&gt;Visualizing the Impact of Feature Attribution Baselines&lt;/a&gt;” qui traite des &lt;a href=&quot;https://medium.com/@kartikeyabhardwaj98/integrated-gradients-for-deep-neural-networks-c114e3968eae&quot;&gt;gradients intégrés&lt;/a&gt; utilisés pour interpréter les réseaux neuronaux dans divers problèmes. Dans le contexte de l’interprétabilité du modèle, le défi consiste à ce que la méthode puisse garantir que le modèle ne considère pas les caractéristiques manquantes comme étant importantes mais aussi que le modèle évite de donner aux entrées de la baseline une importance nulle (ce qui peut facilement arriver). L’auteur propose d’évaluer quantitativement les différents effets de certains choix précédemment utilisés et propose des choix de baseline qui préservent mieux la notion de manque.&lt;/p&gt;

&lt;h1 id=&quot;créativité-et-société-&quot;&gt;Créativité et société 🎨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;L’inadéquation des sentiments&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Cette &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8952437&quot;&gt;étude&lt;/a&gt; longitudinale révèle que les émotions extraites via l’utilisation d’algorithmes basés sur le texte ne sont souvent pas les mêmes que les émotions autodéclarées.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Compréhension de la dopamine et repliement des protéines&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind a récemment publié deux articles intéressants dans Nature. Le &lt;a href=&quot;https://deepmind.com/blog/article/Dopamine-and-temporal-difference-learning-A-fruitful-relationship-between-neuroscience-and-AI&quot;&gt;premier&lt;/a&gt; vise à mieux comprendre le fonctionnement de la dopamine dans le cerveau grâce à l’apprentissage par renforcement. Le &lt;a href=&quot;https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery&quot;&gt;second&lt;/a&gt; est lié au repliement des protéines et tente de mieux comprendre ce fonctionnement afin de pouvoir éventuellement découvrir des traitements pour un large éventail de maladies.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*0mfEtacqGLSrmaUlNjJa0g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt; &lt;a href=&quot;https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Entretiens sur le ML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans une &lt;a href=&quot;https://www.youtube.com/watch?v=I-EIVlHvHRM&amp;amp;feature=youtu.be&quot;&gt;vidéo&lt;/a&gt; de Wired, Refik Anadol discute du potentiel des algorithmes d’apprentissage automatique pour créer des œuvres d’art.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
L’un des secteurs où l’IA pourrait avoir un impact majeur est celui de l’éducation. Dans un nouvel &lt;a href=&quot;https://engineering.stanford.edu/magazine/article/emma-brunskill-amped-education-ai?sf115875862=1&quot;&gt;épisode&lt;/a&gt; de “The Future of Everything”, Russ Altman et Emma Brunskill ont une discussion approfondie sur l’apprentissage assisté par ordinateur.&lt;/p&gt;

&lt;h1 id=&quot;outils-et-jeux-de-données-️&quot;&gt;Outils et jeux de données ⚙️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Modèles PyTorch en production&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Cortex est un outil permettant d’automatiser l’infrastructure et de déployer les modèles PyTorch en tant qu’API en production avec AWS. Pour en savoir plus sur la façon dont cela se fait, cliquez &lt;a href=&quot;https://medium.com/pytorch/how-to-build-production-software-with-pytorch-9a8725382f2a&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Visualisation des séquences de génération de texte&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Facebook AI a lancé &lt;a href=&quot;https://ai.facebook.com/blog/vizseq-a-visual-analysis-toolkit-for-accelerating-text-generation-research/&quot;&gt;VizSeq&lt;/a&gt;, un outil qui aide à évaluer visuellement les séquences de textes générées sous des métriques comme BLUE et METEOR. L’objectif principal de cet outil est de fournir une analyse plus intuitive des ensembles de données textuelles via des visualisations. Pour lire l’article complet, cliquez &lt;a href=&quot;https://www.aclweb.org/anthology/D19-3043.pdf&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Ff7BTxmEjUXHtYu9JkfClg.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ai.facebook.com/blog/vizseq-a-visual-analysis-toolkit-for-accelerating-text-generation-research/&quot;&gt;&lt;em&gt;Source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Reconnaissance vocale en ligne&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Facebook AI a mis en open source son outil &lt;a href=&quot;https://ai.facebook.com/blog/online-speech-recognition-with-wav2letteranywhere/&quot;&gt;wav2letter@anywhere&lt;/a&gt;. Il s’agit d’un framework basé sur un Transformer acoustique afin d’établir un état de l’art en ligne de la reconnaissance vocale. Les principales améliorations portent sur la taille du modèle et la réduction de la latence entre l’audio et la transcription, deux éléments importants pour accélérer l’inférence en temps réel.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*4_2Obuu8u8l2Vtp8UMHe7Q.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt; &lt;a href=&quot;https://ai.facebook.com/blog/online-speech-recognition-with-wav2letteranywhere/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;ethique-en-ia-&quot;&gt;Ethique en IA 🚨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Implications de l’IA&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans un objectif de prévenir les abus et les actions contraires à l’éthique des systèmes d’IA sur le public, l’Union européenne envisage d’interdire la technologie de reconnaissance faciale au public pendant cinq ans. (&lt;a href=&quot;https://www.reuters.com/article/us-eu-ai/eu-mulls-five-year-ban-on-facial-recognition-tech-in-public-areas-idUSKBN1ZF2QL&quot;&gt;Article complet&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Coûts environnementaux des modèles de NLP modernes&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Cet &lt;a href=&quot;https://arxiv.org/abs/1906.02243&quot;&gt;article&lt;/a&gt; aborde les considérations énergétiques et politiques des approches modernes en NLP. Les modèles actuels reposent sur des millions/milliards de paramètres et par conséquent sur d’importantes ressources de calcul. En résulte une consommation d’énergie très importante. Les auteurs espèrent sensibiliser davantage les chercheurs aux coûts environnementaux liés à l’entraînement de ces modèles de NLP.
Zachary Lipton parle d’équité, d’interprétabilité et des dangers du solutionnisme dans cette &lt;a href=&quot;https://c4ejournal.net/2020/01/16/zack-lipton-fairness-interpretability-and-the-dangers-of-solutionism-ethics-of-ai-in-context2020-c4ej-2/&quot;&gt;conférence&lt;/a&gt; donnée à l’Université de Toronto. Les principaux sujets tournent autour des considérations et des implications des approches d’équité en matière de blanchiment d’argent.&lt;/p&gt;

&lt;h1 id=&quot;articles-et-blog-️&quot;&gt;Articles et Blog ✍️&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;ML open source&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Thomas Wolf, responsable scientifique de Hugging Face, donne des conseils à ceux qui envisagent d’utiliser du code open-source ou de faire des recherches. Trouvez le fil de discussion Twitter &lt;a href=&quot;https://twitter.com/Thom_Wolf/status/1216990543533821952?s=20&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Introduction à l’apprentissage auto-supervisé en computer vision&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Jeremy Howard a écrit &lt;a href=&quot;https://www.fast.ai/2020/01/13/self_supervised/&quot;&gt;cet article de blog&lt;/a&gt; qui présente brièvement le concept d’apprentissage auto-supervisé dans le contexte de la vision par ordinateur.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;TinyBERT&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Nous avons déjà constaté le succès de nombreuses variantes des modèles BERT (par exemple, &lt;a href=&quot;https://medium.com/huggingface/distilbert-8cf3380435b5&quot;&gt;DistilBERT&lt;/a&gt;) qui utilisent une certaine forme de &lt;a href=&quot;https://nervanasystems.github.io/distiller/knowledge_distillation.html&quot;&gt;distillation des connaissances&lt;/a&gt; pour réduire considérablement la taille du modèle et améliorer la vitesse. &lt;a href=&quot;https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT&quot;&gt;TinyBERT&lt;/a&gt; est une variante de BERT que ses auteurs ont appliqué à une solution de &lt;a href=&quot;https://towardsdatascience.com/tinybert-for-search-10x-faster-and-20x-smaller-than-bert-74cd1b6b5aec&quot;&gt;recherche par mots-clés&lt;/a&gt;. Ce projet a été inspiré par cette &lt;a href=&quot;https://www.blog.google/products/search/search-language-understanding-bert/&quot;&gt;publication&lt;/a&gt; de Google. L’intérêt de l’architecture est qu’elle fonctionne sur un CPU standard et peut être utilisée pour améliorer et comprendre les résultats de recherche.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Transfert Learning actif&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Rober Monarch a écrit un &lt;a href=&quot;https://medium.com/pytorch/https-medium-com-robert-munro-active-learning-with-pytorch-2f3ee8ebec&quot;&gt;article Medium&lt;/a&gt; sur l’apprentissage actif par transfert, extrait de son prochain livre, &lt;a href=&quot;https://www.manning.com/books/human-in-the-loop-machine-learning&quot;&gt;Human-in-the-loop Machine Learning&lt;/a&gt;. Il écrit aussi d’autres articles sur les méthodes permettant de combiner l’intelligence humaine et l’intelligence machine pour résoudre des problèmes. Ses propos sont accompagnés de code Pytorch.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Les sombres secrets de BERT&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Anna Roger a écrit cet &lt;a href=&quot;https://text-machine-lab.github.io/blog/2020/bert-secrets/&quot;&gt;article&lt;/a&gt; de blog qui parle de ce qui se passe réellement avec un BERT bien fine-tuné. Les résultats des analyses proposées suggèrent que BERT est sévèrement surparamétré et que les avantages identifiés de l’auto-attention ne sont pas nécessairement aussi affirmés, en particulier en ce qui concerne les informations linguistiques qui sont encodées et utilisées pour l’inférence.&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education 🎓&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Neural Nets for NLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Graham Neubig, professeur de NLP à la CMU, a publié des &lt;a href=&quot;https://www.youtube.com/playlist?list=PL8PYTP1V4I8CJ7nMxMC8aXv8WqKYwj-aJ&quot;&gt;vidéos&lt;/a&gt; pour le cours “Neural Nets for NLP” dispensé ce semestre.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;DeepMath&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Vous voulez vous plonger dans les mathématiques qui se cachent derrière les méthodes d’apprentissage approfondies ? Voici une série de &lt;a href=&quot;https://www.youtube.com/playlist?list=PLWQvhvMdDChzsThHFe4lYAff3pu2m0v2H&quot;&gt;conférences vidéo&lt;/a&gt; accueillant un large éventail d’intervenants.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Cours et tutoriels Python&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Google a publié le “Google IT Automation with Python Professional Certificate”. Pour en savoir plus sur le moyen d’obtention de ce certificat cliquez &lt;a href=&quot;https://blog.google/outreach-initiatives/grow-with-google/new-certificate-help-people-grow-careers&quot;&gt;ici&lt;/a&gt; et pours en savoir plus sur les cours, cliquez &lt;a href=&quot;https://www.coursera.org/professional-certificates/google-it-automation&quot;&gt;ici&lt;/a&gt;.
Bien que le cours ne soit pas directement lié à la ML ou à l’IA, cela peut consister en un cours de base pour maîtriser le langage Python. Des bourses d’études sont également disponibles.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Voici une autre &lt;a href=&quot;https://www.youtube.com/watch?v=fMqL5vckiU0&amp;amp;list=PL-wATfeyAMNrtbkCNsLcpoAyBBRJZVlnf&quot;&gt;série de vidéos&lt;/a&gt; intitulée “Deep Learning (for Audio) with Python”, qui met l’accent sur l’utilisation de Tensorflow et de Python pour construire des applications liées à l’audio/musique en tirant parti de l’apprentissage profond.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew Trask a publié une &lt;a href=&quot;https://github.com/OpenMined/PySyft/tree/master/examples/tutorials&quot;&gt;série de tutoriels&lt;/a&gt;, pour parvenir, étape par étape, à un apprentissage approfondi décentralisé et respectueux de la vie privée. Tous les notebooks contiennent des implémentations de PyTorch et sont destinés aux débutants.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Etats de l’art du deep learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Cette &lt;a href=&quot;https://www.youtube.com/watch?v=0VH1Lim8gL8&quot;&gt;conférence&lt;/a&gt; de Lex Fridman traite de la recherche et le développement récents dans le domaine de l’apprentissage approfondi. Il parle des grandes avancées sur des sujets tels que les perceptrons, les réseaux de neurones, la rétropropagation, CNN, l’apprentissage profond, ImageNet, les GAN, AlphaGo et les Transformers plus récemment. Cette conférence fait partie de la série “Deep Learning” du MIT.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Groupes d’études&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Deux groupes d’étude / lectures d’articles conseillés par Elvis : le &lt;a href=&quot;https://twitter.com/__MLT__&quot;&gt;MLT&lt;/a&gt; et le &lt;a href=&quot;https://www.nightai.co/&quot;&gt;nightai&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Le paysage de l’apprentissage par renforcement&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Découvrez avec le Dr Katja Hofmann de Microsoft, les concepts et méthodes clés de l’apprentissage par renforcement dans &lt;a href=&quot;https://note.microsoft.com/MSR-Webinar-RL-Algorithm-to-Adoption-Registration-Live.html?wt.mc_id=twitter_MSR-WBNR_post_v3&quot;&gt;sa série d’articles&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;mentions-spéciales-️&quot;&gt;Mentions spéciales ⭐️&lt;/h1&gt;

&lt;p&gt;Jettez un œil à cette &lt;a href=&quot;https://gist.github.com/y0ast/d91d09565462125a1eb75acc65da1469&quot;&gt;implémentation PyTorch&lt;/a&gt; utilisant de ResNet-18 appliquée à CIFAR-10 permettant d’atteindre une précision de 94%.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
PyTorch 1.4 est sorti ! Consultez les notes de mise à jour &lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v1.4.0&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Elona Shatri a rédigé un &lt;a href=&quot;https://medium.com/@e.shatri1/what-is-optical-music-recognition-6515d8a53e01&quot;&gt;résumé&lt;/a&gt; sur la façon dont elle entend aborder la reconnaissance optique de la musique par un apprentissage approfondi.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Le titre de cet article de blog est explicite : “&lt;a href=&quot;https://cims.nyu.edu/~andrewgw/caseforbdl/&quot;&gt;Les arguments en faveur de l’apprentissage approfondi bayésien&lt;/a&gt;”.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Chris Said partage son &lt;a href=&quot;https://chris-said.io/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I/&quot;&gt;expérience&lt;/a&gt; dans l’optimisation de la taille des échantillons pour les tests A/B, une partie importante de la science des données pratiques. Les sujets abordés comprennent les coûts et les avantages des grandes tailles d’échantillon et les meilleures pratiques pour les praticiens.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Neural Data Server (NDS) est un moteur de recherche dédié à l’obtention de données d’apprentissage par transfert. Pour en savoir plus sur la méthode cliquez &lt;a href=&quot;https://arxiv.org/abs/2001.02799&quot;&gt;ici&lt;/a&gt;,  et sur le service cliquez &lt;a href=&quot;http://aidemos.cs.toronto.edu/nds/&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Vous pouvez retrouver la précédente newsletter &lt;a href=&quot;https://dair.ai/NLP_Newsletter_-1_-FR/&quot;&gt;ici&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous avez des jeux de données, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine édition de la newletter, n’hésitez pas à me contacter à ellfae@gmail.com ou par message sur &lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;Abonnez-vous&lt;/a&gt; pour recevoir les prochains numéros dans votre boîte mail.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_-2_-FR/&quot;&gt;NLP Newsletter [FR] #2: Reformer, DeepMath, ELECTRA, TinyBERT, VizSeq, Open-Sourcing ML,…&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 09, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter [FR] #1: Tokenizers, TensorFlow 2.1, TextVectorization, TorchIO, NLP Shortfalls,…]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_-1_-FR/" />
  <id>https://dair.ai/NLP_Newsletter_#1_[FR]</id>
  <published>2020-03-09T00:00:00-05:00</published>
  <updated>2020-03-09T00:00:00-05:00</updated>
  <author>
    <name>Loïck BOURDOIS</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2400/1*gLVPodYjYd4YaF9sJbSpjg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;avant-propos&quot;&gt;Avant-propos&lt;/h1&gt;

&lt;p&gt;Bonjour et bonne année ! Suite à de nombreuses demandes, j’ai décidé de recommencer la &lt;strong&gt;Newsletter consacré au NLP&lt;/strong&gt;. Cette fois-ci, je vais la garder courte et ciblée (également maintenue dans ce &lt;a href=&quot;https://github.com/dair-ai/nlp_newsletter&quot;&gt;reportoire&lt;/a&gt;). L’objectif de ce bulletin est de vous tenir informé de certaines des avancées intéressantes et récentes liées au NLP et au ML sans prendre trop de temps sur votre journée chargée.&lt;/p&gt;

&lt;h1 id=&quot;publications-&quot;&gt;Publications 📙&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Système d’IA pour la détection de cancers du sein&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind a publié un article dans Nature intitulé “&lt;a href=&quot;https://www.nature.com/articles/s41586-019-1799-6&quot;&gt;International evaluation of an AI system for breast cancer screening&lt;/a&gt;”. Le travail porte sur l’évaluation d’un système d’IA qui surpasse les experts humains en matière de dépistage du cancer du sein. Ces systèmes font toujours l’objet d’un débat notamment la manière dont ils sont évalués. Vous trouverez &lt;a href=&quot;https://www.nature.com/articles/d41586-019-03822-8&quot;&gt;ici&lt;/a&gt; un bref résumé de l’article.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Extraction d’informations&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Pankaj Gupta a rendu publique sa thèse de doctorat intitulée “&lt;a href=&quot;https://www.researchgate.net/publication/336739252_PhD_Thesis_Neural_Information_Extraction_From_Natural_Language_Text&quot;&gt;Extraction d’informations neurales à partir d’un texte en langage naturel&lt;/a&gt;”. Le sujet principal porte sur la manière d’extraire efficacement les relations sémantiques d’un texte en langage naturel en utilisant des approches basées sur les neurones. Cette recherche vise à contribuer à la construction de bases de connaissances structurées, qui peuvent être utilisées dans une série d’applications de NLP, telles que la recherche sur le web, les questions-réponses, entre autres tâches.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Améliorer les recommandations&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Des chercheurs du MIT et d’IBM ont mis au point une &lt;a href=&quot;news.mit.edu/2019/finding-good-read-among-billions-of-choices-1220&quot;&gt;méthode&lt;/a&gt; (publiée l’année dernière au NeurIPS) de catégorisation, d’affichage et de recherche de documents pertinents, basée sur une combinaison de trois outils d’analyse de texte très utilisés : la modélisation de sujets, le word embedding et le transport optimal. La méthode donne également des résultats prometteurs pour le tri des documents. Ces méthodes sont applicables à une grande variété de scénarios nécessitant des suggestions telles que les systèmes de recherche et de recommandation.&lt;/p&gt;

&lt;h1 id=&quot;créativité-et-société-&quot;&gt;Créativité et société 🎨&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Carrières&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Le [rapport] https://hai.stanford.edu/sites/g/files/sbiybj10986/f/ai_index_2019_report.pdf) 2019 de l’AI Index suggère qu’il y a plus de demandes que d’offres de diplômés en AI. Toutefois, certains aspects des emplois liés à l’IA, tels que les transitions de carrière et les entretiens, ne sont pas encore bien définis.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans ce &lt;a href=&quot;https://towardsdatascience.com/how-i-found-my-current-job-3fb22e511a1f&quot;&gt;post&lt;/a&gt;, Vladimir Iglovivok décrit en détail sa carrière et son aventure dans le domaine de l’IA. Allant de la construction de systèmes de recommandation traditionnels à la construction de modèles de vision par ordinateur qui ont remporté des concours sur Kaggle. Il travaille maintenant sur des véhicules autonomes à Lyft, mais le chemin pour y parvenir n’a pas été si facile.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous êtes intéressé par une carrière dans l’IA, la société d’Andrew Ng, deeplearning.ai, a fondé Workera, qui vise à aider les scientifiques spécialisés dans les données et les ingénieurs en apprentissage machine dans leur carrière en IA. Obtenez leur rapport officiel &lt;a href=&quot;https://workera.ai/candidates/report/&quot;&gt;ici&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;outils-et-jeux-de-données-️&quot;&gt;Outils et jeux de données ⚙️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Un tokenizer utra rapide&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hugging Face, la start-up de NLP derrière la librairie Transformers, dispose de tokenizers open-source, une implémentation ultra-rapide de tokenisation qui peut être utilisée dans les pipelines. Consultez la &lt;a href=&quot;https://github.com/huggingface/tokenizers&quot;&gt;documentation&lt;/a&gt; sur l’utilisation des tokenizers sur le site de GitHub.
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*BGcXk6Yf9fXGZlEtxz1hcg.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;TensorFlow 2.1 intègre une nouvelle couche&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization&quot;&gt;TextVectorization&lt;/a&gt; qui vous permet de traiter facilement les chaînes de caractères brutes et d’effectuer efficacement la normalisation du texte, la tokenisation, la génération de n-grammes et l’indexation du vocabulaire. Vous pouvez aussi consulter le &lt;a href=&quot;https://colab.research.google.com/drive/1RvCnR7h0_l4Ekn5vINWToI9TNJdpUZB3&quot;&gt;Google Colab&lt;/a&gt; de François Chollet qui montre comment utiliser cette fonctionnalité pour la classification de texte.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Le NLP et le ML pour la recherche&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
L’un des domaines qui a fait d’énormes progrès l’année dernière est le NLP. La recherche est l’un des domaines qui pourrait potentiellement bénéficier de l’apprentissage par transfert.
Il existe une opportunité de construire des moteurs de recherche qui améliorent la recherche sémantique en utilisant des techniques modernes de NLP telles que les représentations contextualisées d’un modèle basé sur les Transformers comme &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;BERT&lt;/a&gt;. Google a publié il y a quelques mois un &lt;a href=&quot;https://www.blog.google/products/search/search-language-understanding-bert/&quot;&gt;article&lt;/a&gt; sur leur blog sur la façon dont ils utilisent les modèles BERT pour améliorer et comprendre les recherches.
Si vous êtes curieux de savoir comment les représentations contextualisées peuvent être appliquées à la recherche à l’aide de technologies ouvertes telles que Elasticsearch et TensorFlow, vous pouvez consulter ce &lt;a href=&quot;https://towardsdatascience.com/elasticsearch-meets-bert-building-search-engine-with-elasticsearch-and-bert-9e74bf5b4cf2&quot;&gt;billet&lt;/a&gt; ou celui-&lt;a href=&quot;https://towardsdatascience.com/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a&quot;&gt;ci&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Analyse d’images médicales&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/fepegar/torchio&quot;&gt;TorchIO&lt;/a&gt; est un package Python basé PyTorch. TorchIO offre des fonctionnalités permettant de lire et d’échantillonner facilement et efficacement des images médicales en 3D. Les fonctionnalités comprennent des transformations spatiales pour l’augmentation et le prétraitement des données.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/0*FSPuSC8TK9X-NQ2q.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/fepegar/torchio&quot;&gt;source&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;ethique-en-ia-&quot;&gt;Ethique en IA 🚨&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Comportement frauduleux dans la communauté du ML&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Les gagnants de la première place d’un concours Kaggle ont été disqualifiés pour activité frauduleuse. L’équipe a utilisé des tactiques intelligentes mais irresponsables et inacceptables pour remporter la première place du concours. L’histoire complète est disponible &lt;a href=&quot;https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/125436&quot;&gt;ici&lt;/a&gt;. Cette histoire met en évidence un des nombreux comportements graves et inacceptables que la communauté de l’apprentissage machine veut atténuer. L’utilisation correcte et éthique des technologies de ML est la seule façon de progresser.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Biais concernant le genre dans la traduction automatique&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sur la question de savoir si les systèmes de traduction automatique reflètent des préjugés sexistes, un groupe de chercheurs a publié cet &lt;a href=&quot;https://arxiv.org/abs/1809.02208&quot;&gt;article&lt;/a&gt; présentant une étude de cas utilisant Google Translate. L’un des résultats revendiqués par les auteurs est que Google Translate “présente une forte tendance aux défauts masculins, en particulier pour les domaines liés à une répartition déséquilibrée des sexes, comme les emplois dans les STEM (Science, Technology, Engineering and Mathematics)”.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Biais en ML et équité&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous voulez vous familiariser avec l’éthique et l’équité en matière d’IA, vous pouvez écouter ce &lt;a href=&quot;https://twimlai.com/twiml-talk-336-trends-in-fairness-and-ai-ethics-with-timnit-gebru/&quot;&gt;podcast&lt;/a&gt; avec Timnit Gebru et animé par TWIML.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Timnit est un chercheur dans le domaine de l’équité en ML qui, avec Eun Seo Jo, a publié un &lt;a href=&quot;https://arxiv.org/abs/1912.10389&quot;&gt;article&lt;/a&gt; dans lequel ils identifient cinq approches clés dans les pratiques de collecte de données pouvant ainsi fournir des méthodes plus fiables dans le domaine du  ML socioculturel. Cela pourrait potentiellement conduire à des méthodes de collecte de données plus systématiques, issues de la recherche collaborative interdisciplinaire.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sina Fazelpour et Zachary Lipton ont récemment publié un &lt;a href=&quot;http://zacklipton.com/media/papers/fairness-non-ideal-fazelpour-lipton-2020.pdf&quot;&gt;article&lt;/a&gt; dans lequel ils affirment qu’en raison de la nature non idéale de notre monde, il est possible qu’un ML équitable basé sur la pensée idéale puisse potentiellement conduire à des politiques et des interventions mal orientées. En fait, leur analyse démontre “que les lacunes des algorithmes à vocation équitable proposés reflètent les problèmes plus larges rencontrés par l’approche idéale”.&lt;/p&gt;

&lt;h1 id=&quot;articles-et-blog-️&quot;&gt;Articles et Blog ✍️&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Lacunes en NLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Benjamin Heinzerling a publié un &lt;a href=&quot;https://thegradient.pub/nlps-clever-hans-moment-has-arrived/&quot;&gt;article&lt;/a&gt; dans The Gradient où il aborde les domaines dans lesquels le NLP est défaillant, comme la compréhension des arguments et le raisonnement. Benjamin fait référence à un &lt;a href=&quot;https://www.aclweb.org/anthology/P19-1459/&quot;&gt;article&lt;/a&gt; récent de Nivin &amp;amp; Kao qui remet en question les capacités de l’apprentissage par transfert et les modèles linguistiques pour une compréhension poussée du langage naturel.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Temps forts du NLP et du ML en 2019&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Pour la nouvelle année, Elvis (créateur de cette newsletter et du site dair.ai )a publié un &lt;a href=&quot;https://medium.com/dair-ai/nlp-year-in-review-2019-fb8d523bcb19&quot;&gt;document&lt;/a&gt; sur certains des points intéressants du NLP et du ML qu’il a rencontré en 2019.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sebastian Ruder a également écrit récemment un &lt;a href=&quot;https://ruder.io/research-highlights-2019/&quot;&gt;article&lt;/a&gt; détaillé sur les dix principales orientations de recherche en ML et en NLP qu’il a trouvées percutantes en 2019. Parmi la liste figurent des sujets tels que l’entraînement universel non supervisée, l’augmentation des modèles pré-entrainés, les Transformers, entre autres.
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/0*8zoPc5OnYERIaaMP.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“VideoBERT (&lt;/em&gt;[&lt;em&gt;Sun et al., 2019&lt;/em&gt;](https://arxiv.org/abs/1904.01766 une récente variante multimodale de BERT qui génère des “tokens” vidéo en fonction d’une recette (ci-dessus) et prédit les futurs tokens à différentes échelles de temps en fonction d’un tokens vidéo (ci-dessous).” —* &lt;a href=&quot;https://arxiv.org/pdf/1904.01766.pdf&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Google AI Research publie un &lt;a href=&quot;https://ai.googleblog.com/2020/01/google-research-looking-back-at-2019.html&quot;&gt;résumé&lt;/a&gt; des recherches qu’ils ont menées au cours de l’année et les futures orientations de recherche auxquelles ils prêtent attention.&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education 🎓&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Démocratisation des cours d’IA&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Dans un effort pour démocratiser l’enseignement de l’IA et pour éduquer les masses sur les implications de la technologie de l’IA, l’Université d’Helsinki s’est associée à Reaktor pour publier un cours gratuit couvrant les bases de l’IA. Ce &lt;a href=&quot;https://www.elementsofai.com/&quot;&gt;cours&lt;/a&gt; s’intitule “Elements de l’IA” et aborde des sujets tels que l’éthique de l’IA, la philosophie de l’IA, les réseaux de neurones, la règle de Bayes naïve, entre autres sujets fondamentaux.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Le Stanford CS224N est de retour avec une nouvelle&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;http://web.stanford.edu/class/cs224n/&quot;&gt;session&lt;/a&gt; de leurs cours “Natural Language Processing with Deep Learning”. Le cours a officiellement débuté le 7 janvier de cette année. Si vous souhaitez le suivre, rendez-vous sur leur site web pour obtenir le programme complet, des diapositives, des vidéos, des suggestions de lecture de documents, etc.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Machine Learning avec les méthodes à noyaux&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Les méthodes à noyaux telles que l’ACP et les K-means existent depuis un certain temps et ont été appliquées avec succès pour une grande variété d’applications telles que les graphes ou les séquences biologiques. Sur le sujet, pouvez consulter cette série de &lt;a href=&quot;http://members.cbio.mines-paristech.fr/~jvert/svn/kernelcourse/slides/master2017/master2017.pdf&quot;&gt;diapositives&lt;/a&gt; de Partis Tech couvrant un large éventail de méthodes du noyau et leur fonctionnement interne. Vous pouvez aussi jeter un œil au &lt;a href=&quot;https://francisbach.com/cursed-kernels/&quot;&gt;blog&lt;/a&gt; de Francis Bach qui traite de certains aspects des méthodes du noyau et d’autres sujets liés à l’apprentissage machine.&lt;/p&gt;

&lt;h1 id=&quot;mentions-spéciales-️&quot;&gt;Mentions spéciales ⭐️&lt;/h1&gt;

&lt;p&gt;Le &lt;a href=&quot;https://hunch.net/&quot;&gt;blog&lt;/a&gt; tenu par John Langford qui aborde les aspects théoriques de l’apprentissage machine.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Si vous souhaitez apprendre à concevoir et à construire des applications de ML et les amener jusqu’à la production, vous pouvez lire le &lt;a href=&quot;https://www.amazon.com/Building-Machine-Learning-Powered-Applications/dp/149204511X/&quot;&gt;livre&lt;/a&gt; d’Emmanuel Ameisen.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Si vous avez des jeux de données, des projets, des articles de blog, des tutoriels ou des documents que vous souhaitez partager dans la prochaine édition de la newletter, n’hésitez pas à me contacter à ellfae@gmail.com ou par message sur &lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;Abonnez-vous&lt;/a&gt; pour recevoir les prochains numéros dans votre boîte mail.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_-1_-FR/&quot;&gt;NLP Newsletter [FR] #1: Tokenizers, TensorFlow 2.1, TextVectorization, TorchIO, NLP Shortfalls,…&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 09, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP 简报（Issue#6）]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP%E7%AE%80%E6%8A%A5/" />
  <id>https://dair.ai/NLP简报</id>
  <published>2020-03-02T00:00:00-06:00</published>
  <updated>2020-03-02T00:00:00-06:00</updated>
  <author>
    <name>kaiyuan</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*vWICxAehSy3xOnqGIXtpoQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;欢迎来到 NLP 时事简报第六期！全文较长，建议收藏。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;如果想让自己有趣的研究/项目出现在NLP简报中，欢迎在公众号后台留言联系我&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;来看看都有哪些内容，enjoy~
@[TOC]&lt;/p&gt;
&lt;h2 id=&quot;1publications-&quot;&gt;1、Publications 📙&lt;/h2&gt;
&lt;h4 id=&quot;11-bert综述&quot;&gt;1.1 BERT综述&lt;/h4&gt;
&lt;p&gt;基于Transformer的模型已经被证实可以有效地处理从序列标记到问题解答等不同类型的NLP任务，其中一种称为&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;BERT&lt;/a&gt;的模型得到了广泛使用，但是像其他采用深度神经网络的模型一样，我们对其内部运作知之甚少。 一篇名为《 &lt;a href=&quot;https://arxiv.org/abs/2002.12327&quot;&gt;A Primer in BERTology: What we know about how BERT works&lt;/a&gt;》的新论文旨在回答一些有关BERT为什么在这么多NLP任务中表现良好的问题。 论文的内容包括：BERT学习的知识类型及其表示的位置，BERT是如何学习知识的，以及研究人员如何使用其他方法来改进它，等等。&lt;/p&gt;
&lt;h4 id=&quot;12-t5&quot;&gt;1.2 T5&lt;/h4&gt;
&lt;p&gt;Google AI最近发布了一种方法，该方法将从NLP迁移学习模型中学到的所有知识和经验汇总到一个称为&lt;code class=&quot;highlighter-rouge&quot;&gt;Text-to-Text Transfer Transformer（T5）&lt;/code&gt;的统一框架中。 这项工作建议大多数NLP任务可以用文本到文本的格式来表示，这表明输入和输出都是文本。 作者声称，这种“框架为预训练和微调提供了一致的训练目标”。 T5本质上是一种编码器/解码器Transformer，特别是对模型的&lt;code class=&quot;highlighter-rouge&quot;&gt;attention&lt;/code&gt;组件进行了各种改进。 该模型在新发布的名为&lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/c4&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Colossal Clean Crawled Corpus（C4）&lt;/code&gt;&lt;/a&gt;的数据集上进行了预训练，并在NLP任务（例如摘要，问题回答和文本分类）上获得了SOTA结果。
&lt;img src=&quot;https://img-blog.csdnimg.cn/20200303100141454.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0thaXl1YW5fc2p0dQ==,size_16,color_FFFFFF,t_70&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;13-12合1多任务视觉和语言表示学习&quot;&gt;1.3 12合1：多任务视觉和语言表示学习&lt;/h4&gt;
&lt;p&gt;当前的研究使用独立的任务和数据集来执行视觉和语言研究，即使执行这些任务所需的“具有视觉基础的语言理解技能”也是如此。 一篇新论文（将在CVPR上发表），&lt;a href=&quot;https://arxiv.org/abs/1912.02315&quot;&gt;《12-in-1: Multi-Task Vision and Language Representation Learning》&lt;/a&gt;，提出了一种大规模多任务方法，以更好地建模并共同训练视觉和语言任务以生成更通用的视觉和语言模型。 该模型减小了参数大小，并且在基于字幕的图像检索和可视问题解答等任务上表现出色。
&lt;img src=&quot;https://img-blog.csdnimg.cn/2020030310123754.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0thaXl1YW5fc2p0dQ==,size_16,color_FFFFFF,t_70&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;14-bert文本表示的跨模式可传递性&quot;&gt;1.4 BERT文本表示的跨模式可传递性&lt;/h4&gt;
&lt;p&gt;众多研究人员和合作者发表了一篇论文，&lt;a href=&quot;https://arxiv.org/abs/2002.10832&quot;&gt;BERT Can See Out of the Box: On the Cross-modal Transferability of Text Representations&lt;/a&gt;，旨在回答BERT模型是否可以产生可以推广到诸如视觉之类的文本之外的其他方式的问题。 他们提出了一种称为BERT-gen的模型，该模型利用了单模态或多模态表示，并在视觉问题生成数据集上获得了改进的结果。
&lt;img src=&quot;https://img-blog.csdnimg.cn/20200303101728570.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0thaXl1YW5fc2p0dQ==,size_16,color_FFFFFF,t_70&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;2creativity-and-society-&quot;&gt;2、Creativity and Society 🎨&lt;/h2&gt;
&lt;h4 id=&quot;21-the-next-decade-in-ai&quot;&gt;2.1 The Next Decade in AI&lt;/h4&gt;
&lt;p&gt;Gary Marcus最近发表了一篇论文，&lt;a href=&quot;https://arxiv.org/abs/2002.06177&quot;&gt;The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence&lt;/a&gt;，他在其中解释了一系列步骤，他认为，我们应该采取这些步骤来构建更强大的AI系统。 Gary在论文中的中心思想是着重于构建由认知模型指导的混合和知识驱动系统，而不是着重于构建需要更多数据和计算能力的大型系统。&lt;/p&gt;
&lt;h4 id=&quot;22-2020年的10种突破性技术&quot;&gt;2.2 2020年的10种突破性技术&lt;/h4&gt;
&lt;p&gt;MIT Technology Review 出版了一份清单，列出了他们确定的&lt;a href=&quot;https://www.technologyreview.com/lists/technologies/2020/&quot;&gt;10项突破&lt;/a&gt;，这些突破将对解决可能改变我们的生活和工作方式的问题产生影响。 列表如下（排名不分先后）：unhackable internet,  hyper-personalized medicine, digital money, anti-aging drugs, AI-discovered molecules, satellite mega-constellations, quantum supremacy, Tiny AI, differential privacy, and climate attribution.&lt;/p&gt;
&lt;h4 id=&quot;23-重新考虑机器学习的发表过程&quot;&gt;2.3 重新考虑机器学习的发表过程&lt;/h4&gt;
&lt;p&gt;Yoshua Bengio最近写了&lt;a href=&quot;https://yoshuabengio.org/2020/02/26/time-to-rethink-the-publication-process-in-machine-learning/&quot;&gt;关于ML出版物快节奏发展的担忧&lt;/a&gt;。 主要担心的是，由于发布的速度快，很多论文都包含错误并且只是渐进式出版，而花费更多的时间并确保严谨（这是多年以前的工作方式）似乎正在消失。 最重要的是，学生是那些必须应对这种压力和压力的负面后果的人。 为了解决这种情况，Bengio谈论了他的行动，以帮助减慢研究出版物的发展，以造福科学。&lt;/p&gt;
&lt;h2 id=&quot;3tools-and-datasets-️&quot;&gt;3、Tools and Datasets ⚙️&lt;/h2&gt;
&lt;h4 id=&quot;31-allennlp中的pointergenerator网络实现&quot;&gt;3.1 AllenNLP中的PointerGenerator网络实现&lt;/h4&gt;
&lt;p&gt;Pointer-Generator网络旨在增强用于改进&lt;a href=&quot;https://arxiv.org/abs/1704.04368&quot;&gt;抽象摘要&lt;/a&gt;的序列到序列注意模型。 如果您希望使用AllenNLP进行Pointer-Generator抽象摘要，Kundan Krishna已开发了一个库，&lt;a href=&quot;https://github.com/kukrishna/pointer-generator-pytorch-allennlp&quot;&gt;PointerGenerator network implementation in AllenNLP&lt;/a&gt;，该库可让您运行预先训练的模型（提供）或训练自己的模型。
&lt;img src=&quot;https://img-blog.csdnimg.cn/20200303103010155.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0thaXl1YW5fc2p0dQ==,size_16,color_FFFFFF,t_70&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;32-不同语言的qa&quot;&gt;3.2 不同语言的QA&lt;/h4&gt;
&lt;p&gt;随着Transformer模型的发展以及它们对以其他语言执行的大规模NLP任务的有效性，人们付出了巨大的努力来发布不同语言的不同类型的数据集。 例如，Sebastian Ruder共享了可用于不同语言问答研究的数据集列表：&lt;a href=&quot;https://www.aclweb.org/anthology/W18-2605/&quot;&gt;DuReader&lt;/a&gt;，&lt;a href=&quot;https://arxiv.org/abs/1909.07005&quot;&gt;KorQuAD&lt;/a&gt;，&lt;a href=&quot;https://arxiv.org/abs/1912.09723&quot;&gt;SberQuAD&lt;/a&gt;，&lt;a href=&quot;https://arxiv.org/abs/2002.06071&quot;&gt;FQuAD&lt;/a&gt;，&lt;a href=&quot;https://arxiv.org/abs/1906.05394&quot;&gt;Arabic-SQuAD&lt;/a&gt;，&lt;a href=&quot;https://github.com/crux82/squad-it&quot;&gt;SQuAD-it&lt;/a&gt;和&lt;a href=&quot;https://arxiv.org/abs/1912.05200v2&quot;&gt;Spanish SQuAD&lt;/a&gt;。&lt;/p&gt;
&lt;h4 id=&quot;33-pytorch-lightning&quot;&gt;3.3 PyTorch Lightning&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09&quot;&gt;PyTorch Lightning&lt;/a&gt;是一种可让您抽象化可能需要设置GPU / TPU训练和使用16位精度的训练的工具。 使这些事情正常工作可能会变得很乏味，但是好消息是PyTorch Lightning简化了此过程，并允许您在多GPU和TPU上训练模型，而无需更改当前的PyTorch代码。&lt;/p&gt;
&lt;h4 id=&quot;34-tf2中的图神经网络&quot;&gt;3.4 TF2中的图神经网络&lt;/h4&gt;
&lt;p&gt;Microsoft研究团队发布了一个库，该库提供对&lt;a href=&quot;https://github.com/microsoft/tf2-gnn&quot;&gt;许多不同的图神经网络（GNN）架构的实现&lt;/a&gt;的访问。 该库基于TensorFlow 2，还提供可直接在训练/评估循环中使用的数据整理模块。&lt;/p&gt;
&lt;h4 id=&quot;35-预训练-smallberta&quot;&gt;3.5 预训练 SmallBERTa&lt;/h4&gt;
&lt;p&gt;你是否曾经想从头开始训练自己的语言模型，但是没有足够的资源来训练呢？ 如果是这样，那么Aditya Malte提供了一种优雅的方式，它教您如何&lt;a href=&quot;https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b#file-smallberta_pretraining-ipynb&quot;&gt;使用较小的数据集从头训练语言模型&lt;/a&gt;。&lt;/p&gt;
&lt;h4 id=&quot;36-cluedatasetsearch&quot;&gt;3.6 CLUEDatasetSearch&lt;/h4&gt;
&lt;p&gt;CLUE benchmark团队整理了所有中文NLP数据集，附常用英文NLP数据集，可以在&lt;a href=&quot;https://github.com/CLUEbenchmark/CLUEDatasetSearch#qa&quot;&gt;CLUEbenchmark/CLUEDatasetSearch&lt;/a&gt;找到。&lt;/p&gt;

&lt;h2 id=&quot;4ethics-in-ai-&quot;&gt;4、Ethics in AI 🚨&lt;/h2&gt;
&lt;h4 id=&quot;41-面部表情与真实情感&quot;&gt;4.1 面部表情与真实情感&lt;/h4&gt;
&lt;p&gt;一段时间以来，许多研究人员和公司已尝试建立可理解并可以识别文本或视觉环境中的情绪的AI模型。 一篇新文章重新引发了辩论，&lt;a href=&quot;https://www.nature.com/articles/d41586-020-00507-5&quot;&gt;Why faces don’t always tell the truth about feelings&lt;/a&gt;，即旨在直接从面部图像识别情绪的AI技术做得不好。 该领域的杰出心理学家提出的主要论点是，没有证据表明可以仅从面部图像进行情感检测的通用表达方式。 它将需要一个模型更好地了解人格特征，身体动作等，才能真正更接近地更准确地检测人类所表现出的情绪。&lt;/p&gt;
&lt;h4 id=&quot;42-差异隐私和联合学习&quot;&gt;4.2 差异隐私和联合学习&lt;/h4&gt;
&lt;p&gt;构建AI系统时的道德考量之一是确保隐私。 当前，这可以通过两种方式来实现，即使用&lt;a href=&quot;https://www.youtube.com/watch?v=MOcTGM_UteM&quot;&gt;差异隐私或联合学习&lt;/a&gt;。 如果你想了解更多有关这些主题的信息，Jordan Harrod在此视频中为我们做了很好的介绍，其中还包括使用Colab notebook的动手实践课程。&lt;/p&gt;
&lt;h2 id=&quot;5articles-and-blog-posts-️&quot;&gt;5、Articles and Blog posts ✍️&lt;/h2&gt;
&lt;h4 id=&quot;51-深入reformer&quot;&gt;5.1 深入Reformer&lt;/h4&gt;
&lt;p&gt;Madison May撰写了一篇新博客文章，&lt;a href=&quot;https://www.pragmatic.ml/reformer-deep-dive/&quot;&gt;A Deep Dive into the Reformer&lt;/a&gt;，深入探讨了&lt;a href=&quot;https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html&quot;&gt;Reformer&lt;/a&gt;，这是Google AI最近提出的一种新改进的基于Transformer的模型。 在上一期新闻通讯中，我们也介绍了Reformer。&lt;/p&gt;
&lt;h4 id=&quot;52-一个免费的博客平台&quot;&gt;5.2 一个免费的博客平台&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://fastpages.fast.ai/fastpages/jupyter/2020/02/21/introducing-fastpages.html&quot;&gt;fastpages&lt;/a&gt;允许你免费使用GitHub页面自动建立博客。 该解决方案简化了发布博客的过程，还支持使用导出的Word文档和Jupyter notebook。&lt;/p&gt;
&lt;h4 id=&quot;53-google面试技巧&quot;&gt;5.3 Google面试技巧&lt;/h4&gt;
&lt;p&gt;Google Brain团队的Pablo Castro发表了一篇出色的博客文章，&lt;a href=&quot;https://psc-g.github.io/interviews/google/2020/02/25/interviewing-at-google.html&quot;&gt;Tips for interviewing at Google&lt;/a&gt;，重点介绍了那些有兴趣在Google求职面试的人的技巧。 主题包括有关如何准备面试，面试过程中会发生什么以及面试后会发生什么的建议。&lt;/p&gt;
&lt;h4 id=&quot;54-transformer是图神经网络&quot;&gt;5.4 Transformer是图神经网络&lt;/h4&gt;
&lt;p&gt;图神经网络（GNN）和变压器都已证明在不同的NLP任务上有效。 为了更好地理解这些方法背后的内部工作原理以及它们之间的联系，Chaitanya Joshi撰写了一篇很棒的文章，&lt;a href=&quot;https://graphdeeplearning.github.io/post/transformers-are-gnns/&quot;&gt;Transformers are Graph Neural Networks&lt;/a&gt;，解释了GNN与Transformers之间的联系以及这些方法可以以不同的方式组合成一种混合模型。
&lt;img src=&quot;https://img-blog.csdnimg.cn/20200303123803796.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0thaXl1YW5fc2p0dQ==,size_16,color_FFFFFF,t_70&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;55-cnns-and-equivariance&quot;&gt;5.5 CNNs and Equivariance&lt;/h4&gt;
&lt;p&gt;Fabian Fuchs和Ed Wagstaff讨论了&lt;a href=&quot;https://fabianfuchsml.github.io/equivariance1of2/&quot;&gt;等方差的重要性以及CNN&lt;/a&gt;如何实施。 他们首先定义等方差的概念，然后在CNN的上下文中讨论翻译。&lt;/p&gt;
&lt;h4 id=&quot;56-图像自监督学习&quot;&gt;5.6 图像自监督学习&lt;/h4&gt;
&lt;p&gt;由于自我监督在语言建模的现代技术中发挥了作用，因此在NLP简报的前几期中已经进行了很多讨论。 Jonathan Whitaker的这篇博客文章提供了一个很好的，直观的&lt;a href=&quot;https://datasciencecastnet.home.blog/2020/02/22/self-supervised-learning-with-image%E7%BD%91/&quot;&gt;图像自我监督解释&lt;/a&gt;。 如果你真的对该主题感兴趣，Amit Chaudhary还撰写了一篇出色的博客文章，以&lt;a href=&quot;https://amitness.com/2020/02/illustrated-self-supervised-learning/&quot;&gt;可视化方式描述自监督学习&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&quot;6education-&quot;&gt;6、Education 🎓&lt;/h2&gt;
&lt;h4 id=&quot;61-stanford-cs330&quot;&gt;6.1 Stanford CS330&lt;/h4&gt;
&lt;p&gt;斯坦福大学最近以YouTube播放列表的形式发布了有关&lt;a href=&quot;https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&quot;&gt;深层多任务和元学习的新课程&lt;/a&gt;的录像。 主题包括贝叶斯元学习，终身学习，强化学习入门，基于模型的强化学习等。&lt;/p&gt;
&lt;h4 id=&quot;62-pytorch-notebooks&quot;&gt;6.2 PyTorch Notebooks&lt;/h4&gt;
&lt;p&gt;dair.ai发布了一系列教程，旨在帮助您开始&lt;a href=&quot;https://github.com/dair-ai/pytorch_notebooks&quot;&gt;使用PyTorch进行深度神经网络学习&lt;/a&gt;。 这是一项正在进行的工作，当前的一些主题包括如何从头开始实现逻辑回归模型，以及如何从头开始编程神经网络或循环神经网络。&lt;/p&gt;
&lt;h4 id=&quot;63-fastai新书草稿&quot;&gt;6.3 fastai新书草稿&lt;/h4&gt;
&lt;p&gt;Jeremy Howard和Sylvain Gugger将为即将举行的课程发布一份&lt;a href=&quot;https://github.com/fastai/fastbook&quot;&gt;完整的教程fastbook&lt;/a&gt;，其中介绍了深度学习的概念以及如何使用PyTorch和fastai库开发不同的方法。&lt;/p&gt;
&lt;h4 id=&quot;64-免费数据科学课程&quot;&gt;6.4 免费数据科学课程&lt;/h4&gt;
&lt;p&gt;Kaggle提供了一系列&lt;a href=&quot;https://www.kaggle.com/learn/overview&quot;&gt;免费的微型课程&lt;/a&gt;，可帮助您开始进行数据科学之旅。 其中一些课程包括机器学习的可解释性，机器学习和Python入门，数据可视化，特征工程和深度学习等。&lt;/p&gt;

&lt;p&gt;这是另一门不错的&lt;a href=&quot;https://lewtun.github.io/dslectures/&quot;&gt;在线数据科学课程&lt;/a&gt;，提供了课程表，幻灯片和botebook，内容涉及从探索性数据分析，模型解释到自然语言处理等各种主题。&lt;/p&gt;
&lt;h4 id=&quot;65-pytorch生态系统&quot;&gt;6.5 PyTorch生态系统&lt;/h4&gt;
&lt;p&gt;nepture.ai发表了一篇文章，&lt;a href=&quot;https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem?utm_source=twitter&amp;amp;utm_medium=tweet&amp;amp;utm_campaign=blog-model-training-libraries-pytorch-ecosystem&quot;&gt;8 Creators and Core Contributors Talk About Their Model Training Libraries From PyTorch Ecosystem&lt;/a&gt;，其中包含与核心创作者和贡献者的详细讨论，讨论了他们的旅程以及构建PyTorch及其工具的哲学。&lt;/p&gt;
&lt;h4 id=&quot;66-可视化自适应稀疏注意模型&quot;&gt;6.6 可视化自适应稀疏注意模型&lt;/h4&gt;
&lt;p&gt;Sasha Rush分享了一部令人印象深刻的Colab notebook，该笔记本解释并显示了如何产生稀疏softmax输出并将稀疏性引入Transformer模型的关注组件的技术细节，该组件有助于在给定上下文中对无关单词产生零概率，从而提高了性能和可解释性。
&lt;img src=&quot;https://img-blog.csdnimg.cn/20200303155430277.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0thaXl1YW5fc2p0dQ==,size_16,color_FFFFFF,t_70&quot; alt=&quot;在这里插入图片描述&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;7noteworthy-mentions-️&quot;&gt;7、Noteworthy Mentions ⭐️&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://gist.github.com/connorbell/9269401d127f1e507cc9aaf2803067c4&quot;&gt;Conor Bell&lt;/a&gt;写了这个非常棒的python脚本，使您可以查看和准备可用于StyleGAN模型的数据集。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Manu Romero为西班牙语提供了一种&lt;a href=&quot;https://github.com/huggingface/transformers/tree/master/model_cards/mrm8488/bert-spanish-cased-finetuned-pos&quot;&gt;经过微调的POS模型&lt;/a&gt;，该模型可在Hugging Face Transformer库中进行调用。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
该github库，&lt;a href=&quot;https://github.com/tomohideshibata/BERT-related-papers&quot;&gt;BERT-related-papers&lt;/a&gt;包含一长串精心挑选的与BERT相关的论文，这些论文涉及诸如模型压缩，特定领域，多模型，生成，下游任务等不同问题。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Connor Shorten发布了一个15分钟的简短视频，&lt;a href=&quot;https://www.youtube.com/watch?time_continue=79&amp;amp;v=-Bh_7tzyoR4&amp;amp;feature=emb_logo&quot;&gt;Automatic Shortcur Removal for Self-supervised Learning&lt;/a&gt;，解释了一个新的通用框架，该框架旨在减少自我监督表示学习中“sortcut”的影响。 这一点很重要，因为如果处理不正确，该模型可能无法学习有用的语义表示，并可能证明在转移学习环境中无效。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;http://newsletter.ruder.io/issues/accelerating-science-memorizing-vs-learning-to-look-things-up-schmidhuber-s-2010s-greek-bert-arc-illustrated-reformer-annotated-gpt-2-olmpics-223195&quot;&gt;Sebastian Ruder&lt;/a&gt;发布了新一期的NLP News newsletter，主题和资源包括对2019年NLP和ML论文的分析，到用于学习有关转移学习和深度学习要点的幻灯片。&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP%E7%AE%80%E6%8A%A5/&quot;&gt;NLP 简报（Issue#6）&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 02, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter #6: BERTology Primer, fastpages, T5, Data Science Education, PyTorch Notebooks, Slow Science in ML]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_BERTology_Primer_fastpages_T5/" />
  <id>https://dair.ai/NLP_Newsletter_BERTology_Primer_fastpages_T5</id>
  <published>2020-03-02T00:00:00-06:00</published>
  <updated>2020-03-02T00:00:00-06:00</updated>
  <author>
    <name>Elvis Saravia</name>
    <uri>https://dair.ai</uri>
    <email>ellfae@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*vWICxAehSy3xOnqGIXtpoQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Welcome to the sixth issue of the NLP Newsletter. Thanks for all the support and for taking the time to read through the latest in ML and NLP. This issue covers topics that range from extending the Transformer model to slowing publication in ML to a series of ML and NLP book and project launches.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;A few updates about the NLP Newsletter and dair.ai&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
We have been translating the newsletter to other languages such as Brazilian Portuguese, Chinese, Arabic, Spanish, among others. Thanks to those folks that have helped with the translations 🤗. You can also contribute &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues/11&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A month ago, we officially launched our new &lt;a href=&quot;https://dair.ai/&quot;&gt;website&lt;/a&gt;. You can look at our &lt;a href=&quot;https://github.com/dair-ai&quot;&gt;GitHub organization&lt;/a&gt; for more information about dair.ai and projects. If you are interested in seeing how others are already contributing to dair.ai or are interested in contributing to democratizing artificial intelligence research, education, and technologies, check our &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues&quot;&gt;issues&lt;/a&gt; section.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Subscribe&lt;/em&gt;&lt;/a&gt; &lt;em&gt;🔖 to the NLP Newsletter to receive future issues in your inbox.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;publications-&quot;&gt;Publications 📙&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;A Primer in BERTology: What we know about how BERT works&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Transformer-based models have shown to be effective at approaching different types of NLP tasks that range from &lt;em&gt;sequence labeling&lt;/em&gt; to &lt;em&gt;question answering&lt;/em&gt;. One of those models called BERT &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;(Devlin et al. 2019)&lt;/a&gt; is widely used but, like other models that employ deep neural networks, we know very little about their inner workings. A new &lt;a href=&quot;https://arxiv.org/abs/2002.12327&quot;&gt;paper&lt;/a&gt; titled “&lt;strong&gt;A Primer in BERTology: What we know about how BERT works&lt;/strong&gt;” aims to answer some of the questions about why BERT performs well on so many NLP tasks. Some of the topics addressed in the paper include the type of knowledge learned by BERT and where it is represented, and how that knowledge is learned and other methods researchers are using to improve it.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Google AI recently published a &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;method&lt;/a&gt; that brings together all the lessons learned and improvements from NLP transfer learning models into one unified framework called Text-to-Text Transfer Transformer (T5). This work proposes that most NLP tasks can be formulated in a text-to-text format, suggesting that both the inputs and outputs are texts. The authors claim that this “&lt;em&gt;framework provides a consistent training objective both for pre-training and fine-tuning&lt;/em&gt;”. T5 is essentially an encoder-decoder Transformer that applies various improvements in particular to the attention components of the model. The model was pre-trained on a newly released dataset called &lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/c4&quot;&gt;Colossal Clean Crawled Corpus&lt;/a&gt; and achieved SOTA results on NLP tasks such as summarization, question answering, and text classification.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*T9MXxcDOd2fX6xblbu7VdQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;&lt;em&gt;(Raffel et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;12-in-1: Multi-Task Vision and Language Representation Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Current research uses independent tasks and datasets to perform vision-and-language research even when the “&lt;em&gt;visually-grounded language understanding skill&lt;/em&gt;s” required to perform these tasks overlap. A new &lt;a href=&quot;https://arxiv.org/abs/1912.02315&quot;&gt;paper&lt;/a&gt; (to be presented at CVPR) proposes a large-scale multi-task approach to better model and jointly train vision-and-language tasks to generate a more generic vision-and-language model. The model reduces the parameter size and performs well on tasks like caption-based image retrieval and visual question answering.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*yyvN4bK0K2iykyJ2-QVBjw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.02315&quot;&gt;&lt;em&gt;(Lu et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;BERT Can See Out of the Box: On the Cross-modal Transferability of Text Representations&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
reciTAL researchers and collaborators published a paper that aims to answer the question of whether a BERT model can produce representations that generalize to other modalities beyond text such as vision. They propose a model called BERT-gen that leverages mono or multi-modal representations and achieve improved results on visual question generation datasets.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*2NgR7yBuVLDcEza9UT41dw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.10832&quot;&gt;&lt;em&gt;(Scialom et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;creativity-and-society-&quot;&gt;Creativity and Society 🎨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Gary Marcus recently published a &lt;a href=&quot;https://arxiv.org/abs/2002.06177&quot;&gt;paper&lt;/a&gt; where he explains a series of steps that, in his view, we should be taking to build more robust AI systems. Gary’s central idea in this paper is to focus on building hybrid and knowledge-driven systems guided by cognitive models as opposed to focusing on building larger systems that require more data and computation power.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;10 Breakthrough Technologies 2020&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
MIT Technology Review published a list of the &lt;a href=&quot;https://www.technologyreview.com/lists/technologies/2020/&quot;&gt;10 breakthroughs&lt;/a&gt; they have identified that will make a difference in solving problems that could change the way we live and work. The list — in no particular order — includes unhackable internet, hyper-personalized medicine, digital money, anti-aging drugs, AI-discovered molecules, satellite mega-constellations, quantum supremacy, Tiny AI, differential privacy, and climate attribution.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Time to rethink the publication process in machine learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Yoshua Bengio recently &lt;a href=&quot;https://yoshuabengio.org/2020/02/26/time-to-rethink-the-publication-process-in-machine-learning/&quot;&gt;wrote&lt;/a&gt; on his concerns about the fast-paced cycles of ML publications. The main concern is that due to the velocity of publishing, a lot of papers get published that contain errors and are just incremental, whereas spending more time and ensuring rigour, which is how it used to work many years ago, seems to be vanishing. On top of it all, students are the ones that have to deal with the negative consequences of this pressure and stress. To address the situation, Bengio talks about his actions to help in the process of slowing down research publications for the good of science.&lt;/p&gt;

&lt;h1 id=&quot;tools-and-datasets-️&quot;&gt;Tools and Datasets ⚙️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;PointerGenerator network implementation in AllenNLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Pointer-Generator networks aim to augment sequence-to-sequence attentional models that are used to &lt;a href=&quot;https://arxiv.org/abs/1704.04368&quot;&gt;improve&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1704.04368&quot;&gt;&lt;em&gt;abstractive summarization&lt;/em&gt;&lt;/a&gt;. If you wish to use this technique for abstractive summarization using AllenNLP, Kundan Krishna has developed a &lt;a href=&quot;https://github.com/kukrishna/pointer-generator-pytorch-allennlp&quot;&gt;library&lt;/a&gt; that allows you to run a pretrained model (provided) or train your own model.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Fa4G6BrnJm3NSDr3TDHhfw.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Question answering for different languages&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
With the proliferation of Transformer models and their effectiveness for large-scale NLP tasks performed in other languages, there has been an impressive amount of effort to release different types of datasets in different languages. For instance, Sebastian Ruder &lt;a href=&quot;https://twitter.com/seb_ruder/status/1231713840502657025?s=20&quot;&gt;shared&lt;/a&gt; a list of datasets that can be used for question answering research in different languages: &lt;a href=&quot;https://www.aclweb.org/anthology/W18-2605/&quot;&gt;DuReader&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1909.07005&quot;&gt;KorQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1912.09723&quot;&gt;SberQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2002.06071&quot;&gt;FQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1906.05394&quot;&gt;Arabic-SQuAD&lt;/a&gt;, &lt;a href=&quot;https://github.com/crux82/squad-it&quot;&gt;SQuAD-it&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/abs/1912.05200v2&quot;&gt;Spanish SQuAD&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;PyTorch Lightning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
PyTorch Lightning is a &lt;a href=&quot;https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09&quot;&gt;tool&lt;/a&gt; that allows you to abstract training that could require setting up GPU/TPU training and the use of 16-bit precision. Getting those things to work can become tedious but the great news is that PyTorch Lightning simplifies this process and allows you to train models on multi GPUs and TPUs without the need to change your current PyTorch code.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Graph Neural Networks in TF2&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A Microsoft Research team releases a &lt;a href=&quot;https://github.com/microsoft/tf2-gnn&quot;&gt;library&lt;/a&gt; that provides access to implementations of many different graph neural network (GNN) architectures. This library is based on TensorFlow 2 and also provides data-wrangling modules that can directly be used in training/evaluation loops.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Pre-training SmallBERTa — A tiny model to train on a tiny dataset&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Have you ever wanted to train your own language model from scratch but didn’t have enough resources to do so? If so, then Aditya Malte have you covered with this great &lt;a href=&quot;https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b#file-smallberta_pretraining-ipynb&quot;&gt;Colab notebook&lt;/a&gt; that teaches you how to train an LM from scratch with a smaller dataset.&lt;/p&gt;

&lt;h1 id=&quot;ethics-in-ai-&quot;&gt;Ethics in AI 🚨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Why faces don’t always tell the truth about feelings&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
For some time now, many researchers and companies have attempted to build AI models that understand and can recognize emotions either in the textual or visual context. A new &lt;a href=&quot;https://www.nature.com/articles/d41586-020-00507-5&quot;&gt;article&lt;/a&gt; reopens the debate that AI techniques that aim to recognize emotion directly from face images are not doing it right. The main argument, raised by prominent psychologists in the space, is that there is no evidence of universal expressions that can be used for emotion detection from face images alone. It would take a model better understanding of personality traits, body movement, among other things to really get closer to more accurately detecting the emotions displayed by humans.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Differential Privacy and Federated Learning Explained&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
One of the ethical considerations when building AI systems is to ensure privacy. Currently, this can be achieved in two ways, either using differential privacy or federated learning. If you want to know more about these topics, Jordan Harrod provides us a great introduction in this &lt;a href=&quot;https://www.youtube.com/watch?v=MOcTGM_UteM&quot;&gt;video&lt;/a&gt; which also includes a hands-on practice session with the use of a Colab notebook.&lt;/p&gt;

&lt;h1 id=&quot;articles-and-blog-posts-️&quot;&gt;Articles and Blog posts ✍️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;A Deep Dive into the Reformer&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Madison May wrote a &lt;a href=&quot;https://www.pragmatic.ml/reformer-deep-dive/&quot;&gt;new blog post&lt;/a&gt; that provides a deep dive into &lt;a href=&quot;https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html&quot;&gt;Reformer&lt;/a&gt;, which is a new and improved Transformer-based model recently proposed by Google AI. We also featured Reformer in a &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-reformer-deepmath-electra-tinybert-for-search-vizseq-open-sourcing-ml-68d5b6eed057&quot;&gt;previous issue&lt;/a&gt; of the newsletter.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;A free blogging platform&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://fastpages.fast.ai/fastpages/jupyter/2020/02/21/introducing-fastpages.html&quot;&gt;fastpages&lt;/a&gt; allows you to automatically set up a blog using GitHub pages for free. This solution simplifies the process of publishing a blog and it also supports the use of exported word documents and Jupyter notebooks.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Tips for interviewing at Google&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Pablo Castro, from the Google Brain team, published an &lt;a href=&quot;https://psc-g.github.io/interviews/google/2020/02/25/interviewing-at-google.html&quot;&gt;excellent blog post&lt;/a&gt; highlighting a list of tips for those interested in interviewing for a job at Google. Topics include advice on how to prepare for the interview, what to expect during the interview, and what happens after the interview.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Transformers are Graph Neural Networks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Both graph neural networks (GNNs) and Transformers have shown to be effective at different NLP tasks. To better understand the inner workings behind these approaches and how they relate, Chaitanya Joshi wrote a great &lt;a href=&quot;https://graphdeeplearning.github.io/post/transformers-are-gnns/&quot;&gt;article&lt;/a&gt; explaining the connection between GNNs and Transformers and different ways these methods can be combined in a sort of hybrid model.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*u-BkejfKSKcnWOBx.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Representing a sentence as a fully-connected word graph —&lt;/em&gt; &lt;a href=&quot;https://graphdeeplearning.github.io/post/transformers-are-gnns/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;CNNs and Equivariance&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Fabian Fuchs and Ed Wagstaff &lt;a href=&quot;https://fabianfuchsml.github.io/equivariance1of2/&quot;&gt;discuss&lt;/a&gt; the importance of equivariance and how CNNs enforce it. The concept of equivariance is first defined and then discussed in the context of CNNs with respect to translation.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Self-supervised learning with images&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Self-supervised has been discussed a lot in previous issues of the NLP Newsletter due to the role it has played in modern techniques for language modeling. This &lt;a href=&quot;https://datasciencecastnet.home.blog/2020/02/22/self-supervised-learning-with-image%e7%bd%91/&quot;&gt;blog post&lt;/a&gt; by Jonathan Whitaker provides a nice and intuitive explanation of self-supervision in the context of images. If you are really interested in the topic, Amit Chaudhary also wrote an excellent &lt;a href=&quot;https://amitness.com/2020/02/illustrated-self-supervised-learning/&quot;&gt;blog post&lt;/a&gt; describing the concept in a visual way.&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education 🎓&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Stanford CS330: Deep Multi-Task and Meta-Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Stanford recently released video recordings, in the form of a YouTube playlist, for their new &lt;a href=&quot;https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&quot;&gt;course on deep multi-task and meta-learning&lt;/a&gt;. Topics include bayesian meta-learning, lifelong learning, a reinforcement learning primer, model-based reinforcement learning, among others.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;PyTorch Notebooks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
dair.ai releases a &lt;a href=&quot;https://github.com/dair-ai/pytorch_notebooks&quot;&gt;series of notebooks&lt;/a&gt; that aim to get you started with deep neural networks using PyTorch. This is a work in progress and some current topics include how to implement a logistic regression model from scratch and how to program a neural network or recurrent neural network from scratch. Colab notebooks are also available in the GitHub repository.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;The fastai book (draft)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Jeremy Howard and Sylvain Gugger release a &lt;a href=&quot;https://github.com/fastai/fastbook&quot;&gt;comprehensive list&lt;/a&gt; of draft notebooks for an upcoming course that introduces deep learning concepts and how to develop different methods using PyTorch and the fastai library.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Free Data Science courses&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In case you missed it, Kaggle provides a series of &lt;a href=&quot;https://www.kaggle.com/learn/overview&quot;&gt;free micro-courses&lt;/a&gt; to get you started with your Data Science journey. Some of these courses include machine learning explainability, an intro to machine learning and Python, data visualization, feature engineering, and deep learning, among others.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Here is another excellent &lt;a href=&quot;https://lewtun.github.io/dslectures/&quot;&gt;online data science course&lt;/a&gt; that provides a syllabus, slides, and notebooks on topics that range from exploratory data analysis to model interpretation to natural language processing.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;8 Creators and Core Contributors Talk About Their Model Training Libraries From PyTorch Ecosystem&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
nepture.ai published an &lt;a href=&quot;https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem?utm_source=twitter&amp;amp;utm_medium=tweet&amp;amp;utm_campaign=blog-model-training-libraries-pytorch-ecosystem&quot;&gt;extensive article&lt;/a&gt; that contains detailed discussions with core creators and contributors about their journey and philosophy of building PyTorch and tools around it.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Visualizing Adaptive Sparse Attention Models&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sasha Rush shares an impressive &lt;a href=&quot;http://Visualizing%20Adaptive%20Sparse%20Attention%20Models&quot;&gt;Colab notebook&lt;/a&gt; that explains and shows the technical details of how to produce sparse softmax outputs and induce sparsity into the attention component of a Transformer model which helps to produce zero probability for irrelevant words in a given context, improving performance and interpretability all at once.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*7BB322LlVgt1zzk-cviSoA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Visualizing probability distribution of a softmax output&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;noteworthy-mentions-️&quot;&gt;Noteworthy Mentions ⭐️&lt;/h1&gt;

&lt;p&gt;You can access the previous issue of the 🗞 NLP Newsletter &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-the-annotated-gpt-2-understanding-self-distillation-haiku-ganilla-sparkwiki-b0f47f595c82&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Conor Bell wrote this nice &lt;a href=&quot;https://gist.github.com/connorbell/9269401d127f1e507cc9aaf2803067c4&quot;&gt;python script&lt;/a&gt; that allows you to view and prepare a dataset that can be used for a StyleGAN model.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Manu Romero &lt;a href=&quot;https://github.com/huggingface/transformers/tree/master/model_cards/mrm8488/bert-spanish-cased-finetuned-pos&quot;&gt;contributes&lt;/a&gt; a fine-tuned POS model for Spanish. The model is available for use in the Hugging Face Transformer library. It will be interesting to see this effort in other languages.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
This &lt;a href=&quot;https://github.com/tomohideshibata/BERT-related-papers&quot;&gt;repo&lt;/a&gt; contains a long list of carefully curated BERT-related papers that approach different problems such as model compression, domain-specific, multi-model, generation, downstream tasks, etc.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Connor Shorten published a short &lt;a href=&quot;https://www.youtube.com/watch?time_continue=79&amp;amp;v=-Bh_7tzyoR4&amp;amp;feature=emb_logo&quot;&gt;15-minute video&lt;/a&gt; explaining a new general framework that aims to reduce the effect of “shortcut” features in self-supervised representation learning. This is important because if not done right, the model can fail to learn useful semantic representations and potentially prove ineffective in a transfer learning setting.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sebastian Ruder published a new issue of the NLP News newsletter that highlights topics and resources that range from an analysis of NLP and ML papers in 2019 to slides for learning about transfer learning and deep learning essentials. Check it out &lt;a href=&quot;http://newsletter.ruder.io/issues/accelerating-science-memorizing-vs-learning-to-look-things-up-schmidhuber-s-2010s-greek-bert-arc-illustrated-reformer-annotated-gpt-2-olmpics-223195&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Subscribe&lt;/em&gt;&lt;/a&gt; &lt;em&gt;🔖 to the NLP Newsletter to receive future issues in your inbox.&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_BERTology_Primer_fastpages_T5/&quot;&gt;NLP Newsletter #6: BERTology Primer, fastpages, T5, Data Science Education, PyTorch Notebooks, Slow Science in ML&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 02, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter [PT-BR] #6: BERTology Primer, fastpages, T5, Data Science Education, PyTorch Notebooks, Slow Science in ML]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter-PT-BR-_BERTology_Primer_fastpages_T5/" />
  <id>https://dair.ai/NLP_Newsletter[PT-BR]_BERTology_Primer_fastpages_T5</id>
  <published>2020-03-02T00:00:00-06:00</published>
  <updated>2020-03-02T00:00:00-06:00</updated>
  <author>
    <name>Victor Garritano Noronha</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*vWICxAehSy3xOnqGIXtpoQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!-- Welcome to the sixth issue of the NLP Newsletter. Thanks for all the support and for taking the time to read through the latest in ML and NLP. This issue covers topics that range from extending the Transformer model to slowing publication in ML to a series of ML and NLP book and project launches. --&gt;

&lt;p&gt;&lt;br /&gt;
Seja muito bem-vindo à sexta edição da &lt;em&gt;NLP Newsletter&lt;/em&gt;. Agradecemos por todo o suporte e dedicação à leitura dos temas mais recentes em ML e NLP. Essa edição cobre tópicos como extensões ao modelo Transformer, desaceleração no processo de publicação em Aprendizado de Máquina, divulgação de livros e projetos sobre ML e NLP e muito mais.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Algumas atualizaçãoes sobre a NLP Newsletter e o dair.ai&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- We have been translating the newsletter to other languages such as Brazilian Portuguese, Chinese, Arabic, Spanish, among others. Thanks to those folks that have helped with the translations 🤗. You can also contribute [here](https://github.com/dair-ai/dair-ai.github.io/issues/11). --&gt;
&lt;p&gt;&lt;br /&gt;
Nós estamos traduzindo a Newsletter para outros idiomas, como o Português Brasileiro, Chinês, Árabe, Espanhol, dentre outros. Agradecemos aos colegas que realizaram as traduções 🤗. Você também pode contribuir &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues/11&quot;&gt;aqui&lt;/a&gt;!&lt;/p&gt;

&lt;!-- A month ago, we officially launched our new [website](https://dair.ai/). You can look at our [GitHub organization](https://github.com/dair-ai) for more information about dair.ai and projects. If you are interested in seeing how others are already contributing to dair.ai or are interested in contributing to democratizing artificial intelligence research, education, and technologies, check our [issues](https://github.com/dair-ai/dair-ai.github.io/issues) section. --&gt;

&lt;p&gt;&lt;br /&gt;
No mês passado, nós realizamos o lançamento oficial do nosso novo &lt;a href=&quot;https://dair.ai/&quot;&gt;website&lt;/a&gt;. Você pode dar uma olhada em nossa [organização no GitHub] (https://github.com/dair-ai) para mais informações sobre os projetos em andamento. Se você está interessado em saber mais sobre as contribuições já realizadas para a dar.ai, ou mesmo contribuir para a democratização das tecnologias, ensino e pesquisa sobre Inteligência Artificial, veja nossa seção de &lt;a href=&quot;https://github.com/dair-ai/dair-ai.github.io/issues&quot;&gt;issues&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Inscreva-se&lt;/em&gt;&lt;/a&gt; &lt;em&gt;🔖 para receber as próximas edições na sua caixa de entrada!&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;publicações-&quot;&gt;Publicações 📙&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;A Primer in BERTology: What we know about how BERT works&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Transformer-based models have shown to be effective at approaching different types of NLP tasks that range from *sequence labeling* to *question answering*. One of those models called BERT [(Devlin et al. 2019)](https://arxiv.org/abs/1810.04805) is widely used but, like other models that employ deep neural networks, we know very little about their inner workings. A new [paper](https://arxiv.org/abs/2002.12327) titled “**A Primer in BERTology: What we know about how BERT works**” aims to answer some of the questions about why BERT performs well on so many NLP tasks. Some of the topics addressed in the paper include the type of knowledge learned by BERT and where it is represented, and how that knowledge is learned and other methods researchers are using to improve it. --&gt;

&lt;p&gt;&lt;br /&gt;
Modelos baseados no &lt;em&gt;Transformer&lt;/em&gt; mostraram-se bastante efetivos na abordagem das mais diversas tarefas de Processamento de Linguagem Natural, como &lt;em&gt;sequence labeling&lt;/em&gt; e &lt;em&gt;question answering&lt;/em&gt;. Um desses modelos, o BERT &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;(Devlin et al. 2019)&lt;/a&gt;, vem sendo amplamente utilizado. Entretanto, assim como acontece com outros modelos que utilizam redes neurais profundas, ainda sabemos muito pouco sobre seu funcionamento interno. Um novo &lt;a href=&quot;https://arxiv.org/abs/2002.12327&quot;&gt;artigo&lt;/a&gt; entitulado “&lt;strong&gt;A Primer in BERTology: What we know about how BERT works&lt;/strong&gt;” busca começar a responder questões sobre as razões que possibilitam o BERT funcionar tão bem em tantas tarefas de NLP. Alguns dos tópicos investigados no trabalho incluem o tipo de conhecimento aprendido pelo modelo e como o mesmo é representado, além de métodos que outros pesquisadores estão utilizando para melhorar o processo de aprendizado.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Google AI recently published a [method](https://arxiv.org/abs/1910.10683) that brings together all the lessons learned and improvements from NLP transfer learning models into one unified framework called Text-to-Text Transfer Transformer (T5). This work proposes that most NLP tasks can be formulated in a text-to-text format, suggesting that both the inputs and outputs are texts. The authors claim that this “*framework provides a consistent training objective both for pre-training and fine-tuning*”. T5 is essentially an encoder-decoder Transformer that applies various improvements in particular to the attention components of the model. The model was pre-trained on a newly released dataset called [Colossal Clean Crawled Corpus](https://www.tensorflow.org/datasets/catalog/c4) and achieved SOTA results on NLP tasks such as summarization, question answering, and text classification. --&gt;

&lt;p&gt;&lt;br /&gt;
A Google AI publicou recentemente um &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;método&lt;/a&gt; que incorpora todas as lições aprendidas e melhorias do &lt;em&gt;Transfer Learning&lt;/em&gt; para NLP num &lt;em&gt;franework&lt;/em&gt; unificado, denominado Text-to-Text Transfer Transformer (T5). O trabalho propõe que a maioria das tarefas de NLP podem ser formuladas no formato &lt;em&gt;text-to-text&lt;/em&gt;, onde tanto a entrada quanto a saída do problema apresentam-se na forma de texto. Os autores alegam que “esse framework fornece uma função objetivo para treinamento que é consistente tanto na fase de pré-treinamento quanto no &lt;em&gt;fine-tuning&lt;/em&gt;”. O T5 é essencialmente um &lt;em&gt;encoder-decoder&lt;/em&gt; baseado no &lt;em&gt;Transformer&lt;/em&gt;, com várias melhorias, em especial nos componentes de atenção da arquitetura. O modelo foi pré-treinado sobre uma nova base de dados disponibilizada recentemente, conhecida como &lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/c4&quot;&gt;Colossal Clean Crawled Corpus&lt;/a&gt;, onde foi estabelecido um novo estado-da-arte para tarefas como sumarização, &lt;em&gt;question answering&lt;/em&gt; e classificação de texto.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*T9MXxcDOd2fX6xblbu7VdQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;&lt;em&gt;(Raffel et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;12-in-1: Multi-Task Vision and Language Representation Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Current research uses independent tasks and datasets to perform vision-and-language research even when the “*visually-grounded language understanding skill*s” required to perform these tasks overlap. A new [paper](https://arxiv.org/abs/1912.02315) (to be presented at CVPR) proposes a large-scale multi-task approach to better model and jointly train vision-and-language tasks to generate a more generic vision-and-language model. The model reduces the parameter size and performs well on tasks like caption-based image retrieval and visual question answering. --&gt;

&lt;p&gt;&lt;br /&gt;
Os esforços de pesquisa atuais utilizam tarefas e bases de dados independentes para realizar avanços na área de linguística e visão computacional, mesmo quando os conhecimentos necessários para abordar essas tarefas possuem interseção. Um novo &lt;a href=&quot;https://arxiv.org/abs/1912.02315&quot;&gt;artigo&lt;/a&gt; (que será apresentado na CVPR) propõe uma abordagem multi-tarefa em larga escala para uma melhor modelagem e treinamento conjunto em tarefas de linguística e visão computacional, gerando uma modelo mais genérico para as mesmas. O método reduz a quantidade de parâmetros e apresenta um bom desempenho em problemas como recuperação de imagens baseadas em legendas, e &lt;em&gt;question answering&lt;/em&gt; visual.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*yyvN4bK0K2iykyJ2-QVBjw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.02315&quot;&gt;&lt;em&gt;(Lu et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;BERT Can See Out of the Box: On the Cross-modal Transferability of Text Representations&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- reciTAL researchers and collaborators published a paper that aims to answer the question of whether a BERT model can produce representations that generalize to other modalities beyond text such as vision. They propose a model called BERT-gen that leverages mono or multi-modal representations and achieve improved results on visual question generation datasets. --&gt;

&lt;p&gt;&lt;br /&gt;
Pesquisadores e colaboradores da reciTAL publicaram um trabalho que busca responder se um modelo BERT é capaz de gerar representações que generalizam para outras áreas, além de texto e visão computacional. Os autores apresentam um modelo denominado &lt;em&gt;BERT-gen&lt;/em&gt;, que tira proveito de representações mono e multi-modais para obter desempenhos superiores em bases de dados de gerações de perguntas baseadas em imagens.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*2NgR7yBuVLDcEza9UT41dw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.10832&quot;&gt;&lt;em&gt;(Scialom et al. 2020)&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;criatividade-e-sociedade-&quot;&gt;Criatividade e Sociedade 🎨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Gary Marcus recently published a [paper](https://arxiv.org/abs/2002.06177) where he explains a series of steps that, in his view, we should be taking to build more robust AI systems. Gary’s central idea in this paper is to focus on building hybrid and knowledge-driven systems guided by cognitive models as opposed to focusing on building larger systems that require more data and computation power. --&gt;

&lt;p&gt;&lt;br /&gt;
Gary Marcus publicou recentemente um &lt;a href=&quot;https://arxiv.org/abs/2002.06177&quot;&gt;trabalho&lt;/a&gt; onde ele explica a série de passos que, na opinião dele, devem ser seguidos para o desenvolvimento de sistemas de IA mais robustos. A ideia central do artigo é priorizar a construção de sistemas híbridos e orientados à conhecimento, guiados por modelos cognitivos, ao invés da proposição de modelos com mais parâmetros que exigem mais dados e poder computacional.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;10 Breakthrough Technologies 2020&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- MIT Technology Review published a list of the [10 breakthroughs](https://www.technologyreview.com/lists/technologies/2020/) they have identified that will make a difference in solving problems that could change the way we live and work. The list — in no particular order — includes unhackable internet, hyper-personalized medicine, digital money, anti-aging drugs, AI-discovered molecules, satellite mega-constellations, quantum supremacy, Tiny AI, differential privacy, and climate attribution. --&gt;

&lt;p&gt;&lt;br /&gt;
A revista &lt;em&gt;MIT Technology Review&lt;/em&gt; publicou a lista dos &lt;a href=&quot;https://www.technologyreview.com/lists/technologies/2020/&quot;&gt;10 avanços&lt;/a&gt; tecnológicos que segundo eles farão a diferença na resolução de problemas que podem mudar a maneira como vivemos e trabalhamos. A lista — sem ordem específica — inclui a internet &lt;em&gt;não-hackável&lt;/em&gt;, medicina hiper-personalizada, moedas digitais, medicamentos anti-idade, moléculas descobertas por sistemas de IA, mega-constelações de satélites artificias, supremacia quântica, IA em aparelhos celulares, privacidade diferencial e &lt;em&gt;climate attribution&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Time to rethink the publication process in machine learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Yoshua Bengio recently [wrote](https://yoshuabengio.org/2020/02/26/time-to-rethink-the-publication-process-in-machine-learning/) on his concerns about the fast-paced cycles of ML publications. The main concern is that due to the velocity of publishing, a lot of papers get published that contain errors and are just incremental, whereas spending more time and ensuring rigour, which is how it used to work many years ago, seems to be vanishing. On top of it all, students are the ones that have to deal with the negative consequences of this pressure and stress. To address the situation, Bengio talks about his actions to help in the process of slowing down research publications for the good of science. --&gt;

&lt;p&gt;&lt;br /&gt;
Yoshua Bengio &lt;a href=&quot;https://yoshuabengio.org/2020/02/26/time-to-rethink-the-publication-process-in-machine-learning/&quot;&gt;escreveu&lt;/a&gt; recentemente sobre suas preocupações em relação aos atuais ciclos acelerados de publicações em Aprendizado de Máquina. O ponto principal é que, por causa da velocidade dessas, diversos trabalhos publicados apresentam erros e são apenas incrementais, deixando o investimento de tempo na revisão e verificação do rigor empregado na metodologia e experimentos de lado. Diante de tudo isso, os estudantes são aqueles que precisam lidar com as consequências negativas da pressão e estresse gerados por essa situação. Com o objetivo de solucionar esse problema, Bengio compartilha suas ações para ajudar no processo de desaceleração das publicações para o bem da ciência.&lt;/p&gt;

&lt;h1 id=&quot;ferramentas-e-bases-de-dados-️&quot;&gt;Ferramentas e Bases de Dados ⚙️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Implementação da PointerGenerator network com a AllenNLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Pointer-Generator networks aim to augment sequence-to-sequence attentional models that are used to [improve](https://arxiv.org/abs/1704.04368) [*abstractive summarization*](https://arxiv.org/abs/1704.04368). If you wish to use this technique for abstractive summarization using AllenNLP, Kundan Krishna has developed a [library](https://github.com/kukrishna/pointer-generator-pytorch-allennlp) that allows you to run a pretrained model (provided) or train your own model. --&gt;

&lt;p&gt;&lt;br /&gt;
Redes &lt;em&gt;Pointer-Generator&lt;/em&gt; buscam aprimorar o mecanismo de atenção de modelos &lt;em&gt;sequence-to-sequence&lt;/em&gt; e são utilizadas para &lt;a href=&quot;https://arxiv.org/abs/1704.04368&quot;&gt;melhorar o desempenho&lt;/a&gt; em tarefas como &lt;a href=&quot;https://arxiv.org/abs/1704.04368&quot;&gt;sumarização abstrata&lt;/a&gt;. Se você gostaria de utilizando essa técnica com a &lt;em&gt;framework&lt;/em&gt; AllenNLP, saiba que o Kundan Krishna desenvolveu um &lt;a href=&quot;https://github.com/kukrishna/pointer-generator-pytorch-allennlp&quot;&gt;módulo&lt;/a&gt; que permite a execução de um modelo pré-treinado dessa categoria, além do treinamento de um novo modelo do zero.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Fa4G6BrnJm3NSDr3TDHhfw.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Question answering para diferentes idiomas&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- With the proliferation of Transformer models and their effectiveness for large-scale NLP tasks performed in other languages, there has been an impressive amount of effort to release different types of datasets in different languages. For instance, Sebastian Ruder [shared](https://twitter.com/seb_ruder/status/1231713840502657025?s=20) a list of datasets that can be used for question answering research in different languages: [DuReader](https://www.aclweb.org/anthology/W18-2605/), [KorQuAD](https://arxiv.org/abs/1909.07005), [SberQuAD](https://arxiv.org/abs/1912.09723), [FQuAD](https://arxiv.org/abs/2002.06071), [Arabic-SQuAD](https://arxiv.org/abs/1906.05394), [SQuAD-it](https://github.com/crux82/squad-it), and [Spanish SQuAD](https://arxiv.org/abs/1912.05200v2). --&gt;

&lt;p&gt;&lt;br /&gt;
Com a disseminação de modelos baseados no &lt;em&gt;Transformer&lt;/em&gt; e sua efetividade em tarefas de NLP aplicadas a outros idiomas, existe um esforço significativo na construção e liberação de diferentes bases de dados em diferentes dialetos. Por exemplo, o Sebastian Ruder &lt;a href=&quot;https://twitter.com/seb_ruder/status/1231713840502657025?s=20&quot;&gt;compartilhou&lt;/a&gt; uma lista de &lt;em&gt;datasets&lt;/em&gt; que podem ser utilizados no desenvolvimento de métodos para &lt;em&gt;question answering&lt;/em&gt; em diversas línguas: DuReader](https://www.aclweb.org/anthology/W18-2605/), &lt;a href=&quot;https://arxiv.org/abs/1909.07005&quot;&gt;KorQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1912.09723&quot;&gt;SberQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2002.06071&quot;&gt;FQuAD&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1906.05394&quot;&gt;Arabic-SQuAD&lt;/a&gt;, &lt;a href=&quot;https://github.com/crux82/squad-it&quot;&gt;SQuAD-it&lt;/a&gt; e &lt;a href=&quot;https://arxiv.org/abs/1912.05200v2&quot;&gt;Spanish SQuAD&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;PyTorch Lightning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- PyTorch Lightning is a [tool](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09) that allows you to abstract training that could require setting up GPU/TPU training and the use of 16-bit precision. Getting those things to work can become tedious but the great news is that PyTorch Lightning simplifies this process and allows you to train models on multi GPUs and TPUs without the need to change your current PyTorch code. --&gt;

&lt;p&gt;&lt;br /&gt;
A PyTorch Lightning é uma &lt;a href=&quot;https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09&quot;&gt;ferramenta&lt;/a&gt; que possibilita a abstração da escolha do dispositivo utilizado durante o treinamento de redes neurais (CPU ou GPU), além do uso de precisão de 16 bits. Fazer essas configurações funcionarem pode ser um trabalho entediante, mas felizmente os colaboradores da PyTorch Lightning simplificaram esse processo, permitindo o treinamento de modelos em várias GPUs/TPUs sem a necessidade de alteração do código.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Graph Neural Networks no TF2&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- A Microsoft Research team releases a [library](https://github.com/microsoft/tf2-gnn) that provides access to implementations of many different graph neural network (GNN) architectures. This library is based on TensorFlow 2 and also provides data-wrangling modules that can directly be used in training/evaluation loops. --&gt;

&lt;p&gt;&lt;br /&gt;
O time de pesquisa da Microsoft liberou uma &lt;a href=&quot;https://github.com/microsoft/tf2-gnn&quot;&gt;biblioteca&lt;/a&gt; com a implementação de diversas arquiteturas de &lt;em&gt;Graph Neural Networks (GNNs)&lt;/em&gt;. A biblioteca, baseada na versão 2.0 do TensorFlow, fornece funcionalidades para manipulação de dados que podem ser utilizadas diretamente nas iterações de treino/avaliação.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Pre-training SmallBERTa — A tiny model to train on a tiny dataset&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Have you ever wanted to train your own language model from scratch but didn’t have enough resources to do so? If so, then Aditya Malte have you covered with this great [Colab notebook](https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b#file-smallberta_pretraining-ipynb) that teaches you how to train an LM from scratch with a smaller dataset. --&gt;

&lt;p&gt;&lt;br /&gt;
Você já pensou em treinar o seu próprio modelo de linguagem do zero, mas nunca teve o poder computacional necessário para isso? Se já, então o Aditya Malte pode lhe ajudar com esse excelente &lt;a href=&quot;https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b#file-smallberta_pretraining-ipynb&quot;&gt;notebook no Colab&lt;/a&gt; que exemplifica o processo de treinamento de um modelo de linguagem numa base de dados reduzida.&lt;/p&gt;

&lt;h1 id=&quot;ética-em-ia-&quot;&gt;Ética em IA 🚨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Why faces don’t always tell the truth about feelings&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Há algum tempo, diversos pesquisadores e empresas tentam construir modelos de IA que consigam entender e reconhecer emoções em contextos visuais ou textuais. Um novo &lt;a href=&quot;https://www.nature.com/articles/d41586-020-00507-5&quot;&gt;artigo&lt;/a&gt; reabre o debate que técnicas de IA que tentam reconhecer emoções diretamente de imagens faciais não estão fazendo seu trabalho direito. O argumento principal, formulado por psicólogos proeminentes na área, é que não existe evidência da existência de expressões universais que possam ser utilizadas na detecção de emoções de maneira independente. Seria necessária uma melhor compreensão de traços de personalidade e movimentos corporais por parte do modelo, dentre outras características, para que seja possível detectar as emoções humanas de maneira mais precisa.&lt;/p&gt;

&lt;!-- For some time now, many researchers and companies have attempted to build AI models that understand and can recognize emotions either in the textual or visual context. A new [article](https://www.nature.com/articles/d41586-020-00507-5) reopens the debate that AI techniques that aim to recognize emotion directly from face images are not doing it right. The main argument, raised by prominent psychologists in the space, is that there is no evidence of universal expressions that can be used for emotion detection from face images alone. It would take a model better understanding of personality traits, body movement, among other things to really get closer to more accurately detecting the emotions displayed by humans. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Differential Privacy and Federated Learning Explicadas&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- One of the ethical considerations when building AI systems is to ensure privacy. Currently, this can be achieved in two ways, either using differential privacy or federated learning. If you want to know more about these topics, Jordan Harrod provides us a great introduction in this [video](https://www.youtube.com/watch?v=MOcTGM_UteM) which also includes a hands-on practice session with the use of a Colab notebook. --&gt;

&lt;p&gt;&lt;br /&gt;
Uma das considerações éticas que devem ser levadas em consideração durante a construção de sistemas de IA é a garantia de privacidade. Atualmente, essa garantia pode ser obtida de duas maneiras: através da &lt;em&gt;differential privacy&lt;/em&gt; ou do &lt;em&gt;federated learning&lt;/em&gt;. Se você quiser saber mais sobre esses dois tópicos, Jordan Harrod produziu uma excelente introdução nesse &lt;a href=&quot;https://www.youtube.com/watch?v=MOcTGM_UteM&quot;&gt;vídeo&lt;/a&gt;, que inclui uma sessão &lt;em&gt;hands-on&lt;/em&gt; utilizando &lt;em&gt;notebooks&lt;/em&gt; do Colab.&lt;/p&gt;

&lt;h1 id=&quot;artigos-e-postagens-️&quot;&gt;Artigos e Postagens ✍️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;A Deep Dive into the Reformer&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Madison May wrote a [new blog post](https://www.pragmatic.ml/reformer-deep-dive/) that provides a deep dive into [Reformer](https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html), which is a new and improved Transformer-based model recently proposed by Google AI. We also featured Reformer in a [previous issue](https://medium.com/dair-ai/nlp-newsletter-reformer-deepmath-electra-tinybert-for-search-vizseq-open-sourcing-ml-68d5b6eed057) of the newsletter. --&gt;

&lt;p&gt;&lt;br /&gt;
Madison May realizou uma &lt;a href=&quot;https://www.pragmatic.ml/reformer-deep-dive/&quot;&gt;postagem&lt;/a&gt; em seu &lt;em&gt;blog&lt;/em&gt; que fornece uma análise mais profunda do &lt;a href=&quot;https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html&quot;&gt;Reformer&lt;/a&gt;, um novo modelo baseado no &lt;em&gt;Transformer&lt;/em&gt;, proposto recentemente pela Google AI. O &lt;em&gt;Reformer&lt;/em&gt; já havia aparecido numa &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-reformer-deepmath-electra-tinybert-for-search-vizseq-open-sourcing-ml-68d5b6eed057&quot;&gt;edição anterior&lt;/a&gt; da Newsletter.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Uma plataforma de blogs gratuita&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A &lt;a href=&quot;https://fastpages.fast.ai/fastpages/jupyter/2020/02/21/introducing-fastpages.html&quot;&gt;fastpages&lt;/a&gt; permite a criação e configuração automática de um &lt;em&gt;blog&lt;/em&gt; utilizando a &lt;em&gt;GitHub pages&lt;/em&gt; de maneira gratuita. Essa solução simplifica o processo de publicação e também oferece suporte à utilização de documentos exportados e &lt;em&gt;Jupyter notebooks&lt;/em&gt;.&lt;/p&gt;

&lt;!-- allows you to automatically set up a blog using GitHub pages for free. This solution simplifies the process of publishing a blog and it also supports the use of exported word documents and Jupyter notebooks. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Dicas para entrevistas na Google&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Pablo Castro, from the Google Brain team, published an [excellent blog post](https://psc-g.github.io/interviews/google/2020/02/25/interviewing-at-google.html) highlighting a list of tips for those interested in interviewing for a job at Google. Topics include advice on how to prepare for the interview, what to expect during the interview, and what happens after the interview. --&gt;

&lt;p&gt;&lt;br /&gt;
Pablo Castro, do time da Google Brain, publicou uma &lt;a href=&quot;https://psc-g.github.io/interviews/google/2020/02/25/interviewing-at-google.html&quot;&gt;excelente postagem&lt;/a&gt; destacando as principais dicas para aqueles interessados em aplicar para uma posição na Google. Os tópicos abordados incluem dicas sobre o processo de entrevistas, como preparação, o que esperar durante e o que acontece depois delas.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Transformers are Graph Neural Networks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Both graph neural networks (GNNs) and Transformers have shown to be effective at different NLP tasks. To better understand the inner workings behind these approaches and how they relate, Chaitanya Joshi wrote a great [article](https://graphdeeplearning.github.io/post/transformers-are-gnns/) explaining the connection between GNNs and Transformers and different ways these methods can be combined in a sort of hybrid model. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;em&gt;Graph Neural Networks (GNNs)&lt;/em&gt; e &lt;em&gt;Transformers&lt;/em&gt; mostraram-se bastante efetivos em diversas tarefas de NLP. Com o objetivo de compreender melhor o funcionamento interno dessas arquiteturas e como elas se relacionam, Chaitanya Joshi escreveu um excelente &lt;a href=&quot;https://graphdeeplearning.github.io/post/transformers-are-gnns/&quot;&gt;artigo&lt;/a&gt; em seu &lt;em&gt;blog&lt;/em&gt;, evidenciando a conexão entre GNNs e &lt;em&gt;Transformers&lt;/em&gt;, e as diversas maneiras pelas quais esses métodos podem ser combinados e utilizados em conjunto.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*u-BkejfKSKcnWOBx.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Representação de uma frase como um grafo completo de palavras —&lt;/em&gt; &lt;a href=&quot;https://graphdeeplearning.github.io/post/transformers-are-gnns/&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;CNNs e Equivariância&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Fabian Fuchs and Ed Wagstaff [discuss](https://fabianfuchsml.github.io/equivariance1of2/) the importance of equivariance and how CNNs enforce it. The concept of equivariance is first defined and then discussed in the context of CNNs with respect to translation. --&gt;

&lt;p&gt;&lt;br /&gt;
Fabian Fuchs e Ed Wagstaff &lt;a href=&quot;https://fabianfuchsml.github.io/equivariance1of2/&quot;&gt;discutiram&lt;/a&gt; a importância da equivariância e como as &lt;em&gt;Convolutional Neural Networks (CNNs)&lt;/em&gt; garantem essa propriedade. O conceito é apresentado e discutido posteriormente no contexto de CNNs em relação à translação.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Self-supervised learning com imagens&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A técnica de &lt;em&gt;self-supervised learning&lt;/em&gt; foi amplamente discutida nas edições anteriores da Newsletter devido ao seu papel em modelos recentes para &lt;em&gt;language modeling&lt;/em&gt;. Esse &lt;a href=&quot;https://datasciencecastnet.home.blog/2020/02/22/self-supervised-learning-with-image%e7%bd%91/&quot;&gt;&lt;em&gt;blog post&lt;/em&gt;&lt;/a&gt;, feito pelo Jonathan Whitaker, fornece uma explicação intuitiva da técnica de aprendizado no contexto de imagens. Se você deseja um conhecimento mais profundo sobre o assunto, o Amit Chaudhary também publicou um &lt;a href=&quot;https://amitness.com/2020/02/illustrated-self-supervised-learning/&quot;&gt;artigo interessante&lt;/a&gt; descrevendo o conceito de maneira visual.&lt;/p&gt;

&lt;!-- Self-supervised has been discussed a lot in previous issues of the NLP Newsletter due to the role it has played in modern techniques for language modeling. This [blog post](https://datasciencecastnet.home.blog/2020/02/22/self-supervised-learning-with-image%e7%bd%91/) by Jonathan Whitaker provides a nice and intuitive explanation of self-supervision in the context of images. If you are really interested in the topic, Amit Chaudhary also wrote an excellent [blog post](https://amitness.com/2020/02/illustrated-self-supervised-learning/) describing the concept in a visual way. --&gt;

&lt;h1 id=&quot;educação-&quot;&gt;Educação 🎓&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Stanford CS330: Deep Multi-Task and Meta-Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A universidade de Stanford liberou recentemente suas vídeo-aulas, numa &lt;em&gt;playlist&lt;/em&gt; no YouTube, para o &lt;a href=&quot;https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&quot;&gt;novo curso em &lt;em&gt;deep multi-task e meta-learning&lt;/em&gt;&lt;/a&gt;. Os assuntos apresentados incluem &lt;em&gt;bayesian meta-learning&lt;/em&gt;, &lt;em&gt;lifelong learning&lt;/em&gt;, uma visão geral sobre aprendizado por reforço, &lt;em&gt;model-based reinforcement learning&lt;/em&gt;, entre outros.&lt;/p&gt;

&lt;!-- Stanford recently released video recordings, in the form of a YouTube playlist, for their new [course on deep multi-task and meta-learning](https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5). Topics include bayesian meta-learning, lifelong learning, a reinforcement learning primer, model-based reinforcement learning, among others. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;PyTorch Notebooks&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A dar.ai liberou recentemente um &lt;a href=&quot;https://github.com/dair-ai/pytorch_notebooks&quot;&gt;compilado de &lt;em&gt;notebooks&lt;/em&gt;&lt;/a&gt; apresentando uma introdução à redes neurais profundas utilizando o PyTorch. O trabalho continua em desenvolvimento, e alguns dos tópicos já disponíveis incluem como implementar um modelo de regressão logística do zero, assim como a programação de redes neurais &lt;em&gt;feed-forward&lt;/em&gt; e recorrentes. Notebooks no Colab estão disponíveis no GitHub.&lt;/p&gt;

&lt;!-- dair.ai releases a [series of notebooks](https://github.com/dair-ai/pytorch_notebooks) that aim to get you started with deep neural networks using PyTorch. This is a work in progress and some current topics include how to implement a logistic regression model from scratch and how to program a neural network or recurrent neural network from scratch. Colab notebooks are also available in the GitHub repository. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;The fastai book (draft)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Jeremy Howard e Sylvain Gugger liberaram uma &lt;a href=&quot;https://github.com/fastai/fastbook&quot;&gt;lista&lt;/a&gt; com alguns &lt;em&gt;notebooks&lt;/em&gt; para um futuro curso que introduz conceitos de &lt;em&gt;Deep Learning&lt;/em&gt; e como implementar diferentes métodos utilizando o PyTorch e a biblioteca da fastai.&lt;/p&gt;

&lt;!-- Jeremy Howard and Sylvain Gugger release a [comprehensive list](https://github.com/fastai/fastbook) of draft notebooks for an upcoming course that introduces deep learning concepts and how to develop different methods using PyTorch and the fastai library. --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Cursos gratuitos de Ciência de Dados&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- In case you missed it, Kaggle provides a series of [free micro-courses](https://www.kaggle.com/learn/overview) to get you started with your Data Science journey. Some of these courses include machine learning explainability, an intro to machine learning and Python, data visualization, feature engineering, and deep learning, among others. --&gt;

&lt;p&gt;&lt;br /&gt;
O Kaggle disponibilizou uma série de [mini-cursos gratuitos]https://www.kaggle.com/learn/overview) para o pontapé inicial da sua carreira como Cientista de Dados. Os cursos abordam assuntos como Explicabilidade em ML, Introdução ao Aprendizado de Máquina e ao Python, Visualização de Dados, &lt;em&gt;Feature Engineering&lt;/em&gt;, &lt;em&gt;Deep Learning&lt;/em&gt;, entre outros.&lt;/p&gt;

&lt;!-- Here is another excellent [online data science course](https://lewtun.github.io/dslectures/) that provides a syllabus, slides, and notebooks on topics that range from exploratory data analysis to model interpretation to natural language processing. --&gt;

&lt;p&gt;&lt;br /&gt;
Um outro &lt;a href=&quot;https://lewtun.github.io/dslectures/&quot;&gt;excelente curso online&lt;/a&gt; de Ciência de Dados disponibiliza notas de aulas, &lt;em&gt;slides&lt;/em&gt; e &lt;em&gt;notebooks&lt;/em&gt; sobre tópicos que vão desde análise exploratória até interpretação de modelos para Processamento de Linguagem Natural.&lt;/p&gt;

&lt;!-- ***8 Creators and Core Contributors Talk About Their Model Training Libraries From PyTorch Ecosystem*** --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;8 Criadores e Colaboradores discutem suas bibliotecas de treinamento de modelos no ecossistema do PyTorch&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- nepture.ai published an [extensive article](https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem?utm_source=twitter&amp;utm_medium=tweet&amp;utm_campaign=blog-model-training-libraries-pytorch-ecosystem) that contains detailed discussions with core creators and contributors about their journey and philosophy of building PyTorch and tools around it. --&gt;

&lt;p&gt;&lt;br /&gt;
A nepture.ai publicou um &lt;a href=&quot;https://neptune.ai/blog/model-training-libraries-pytorch-ecosystem?utm_source=twitter&amp;amp;utm_medium=tweet&amp;amp;utm_campaign=blog-model-training-libraries-pytorch-ecosystem&quot;&gt;excelente artigo&lt;/a&gt; que contém discussões detalhadas com criadores e colaboradores sobre suas jornadas e a filosofia utilizada na criação do PyTorch e nas ferramentas construídas com base na biblioteca.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Visualizando Adaptive Sparse Attention Models&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;!-- Sasha Rush shares an impressive [Colab notebook](http://Visualizing%20Adaptive%20Sparse%20Attention%20Models) that explains and shows the technical details of how to produce sparse softmax outputs and induce sparsity into the attention component of a Transformer model which helps to produce zero probability for irrelevant words in a given context, improving performance and interpretability all at once. --&gt;

&lt;p&gt;&lt;br /&gt;
Sashs Rush compartilhou um &lt;a href=&quot;https://colab.research.google.com/drive/1EB7MI_3gzAR1gFwPPO27YU9uYzE_odSu&quot;&gt;notebook impressionante&lt;/a&gt; que explica e mostra os detalhes técnicos sobre como produzir saídas esparsas com a softmax e induzir esparsidade nos componentes de atenção do modelo &lt;em&gt;Transformer&lt;/em&gt;, auxiliando na atribuição de probabilidade zero para palavras irrelevantes num dado contexto, melhorando simultaneamente o desempenho e a interpretabilidade.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*7BB322LlVgt1zzk-cviSoA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Visualizando a distribuição de probabilidade da saída da softmax&lt;/em&gt;
&lt;!-- *Visualizing probability distribution of a softmax output* --&gt;&lt;/p&gt;

&lt;h1 id=&quot;menções-honrosas-️&quot;&gt;Menções Honrosas ⭐️&lt;/h1&gt;

&lt;!-- You can access the previous issue of the 🗞 NLP Newsletter [here](https://medium.com/dair-ai/nlp-newsletter-the-annotated-gpt-2-understanding-self-distillation-haiku-ganilla-sparkwiki-b0f47f595c82). --&gt;

&lt;p&gt;Você pode conferir a edição da passada da 🗞 Newsletter &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-the-annotated-gpt-2-understanding-self-distillation-haiku-ganilla-sparkwiki-b0f47f595c82&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;!-- Conor Bell wrote this nice [python script](https://gist.github.com/connorbell/9269401d127f1e507cc9aaf2803067c4) that allows you to view and prepare a dataset that can be used for a StyleGAN model. --&gt;

&lt;p&gt;&lt;br /&gt;
Conor Bell escreveu esse &lt;a href=&quot;https://gist.github.com/connorbell/9269401d127f1e507cc9aaf2803067c4&quot;&gt;script em Python&lt;/a&gt; que permite a visualização e preparação de uma base de dados que pode ser utilizada no modelo StyleGAN.&lt;/p&gt;

&lt;!-- Manu Romero [contributes](https://github.com/huggingface/transformers/tree/master/model_cards/mrm8488/bert-spanish-cased-finetuned-pos) a fine-tuned POS model for Spanish. The model is available for use in the Hugging Face Transformer library. It will be interesting to see this effort in other languages. --&gt;

&lt;p&gt;&lt;br /&gt;
Manu Romero &lt;a href=&quot;https://github.com/huggingface/transformers/tree/master/model_cards/mrm8488/bert-spanish-cased-finetuned-pos&quot;&gt;compartilhou&lt;/a&gt; um modelo de POS tagging para o espanhol. O modelo está disponível para uso utilizando a biblioteca &lt;em&gt;Transformers&lt;/em&gt; da Hugging Face. Será interessante acompanhar a divulgação de modelos para outros idiomas.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Esse &lt;a href=&quot;https://github.com/tomohideshibata/BERT-related-papers&quot;&gt;repositório&lt;/a&gt; contém uma extensa lista de artigos, cuidadosamente selecionados, que possuem relação com o BERT e abordam diversos problemas como compressão de modelos, tarefas de domínios específicos, entre outros.&lt;/p&gt;

&lt;!-- This [repo](https://github.com/tomohideshibata/BERT-related-papers) contains a long list of carefully curated BERT-related papers that approach different problems such as model compression, domain-specific, multi-model, generation, downstream tasks, etc. --&gt;

&lt;!-- Connor Shorten published a short [15-minute video](https://www.youtube.com/watch?time_continue=79&amp;v=-Bh_7tzyoR4&amp;feature=emb_logo) explaining a new general framework that aims to reduce the effect of “shortcut” features in self-supervised representation learning. This is important because if not done right, the model can fail to learn useful semantic representations and potentially prove ineffective in a transfer learning setting. --&gt;

&lt;p&gt;&lt;br /&gt;
Connor Shorten publicou um &lt;a href=&quot;https://www.youtube.com/watch?time_continue=79&amp;amp;v=-Bh_7tzyoR4&amp;amp;feature=emb_logo&quot;&gt;vídeo de 15 minutos&lt;/a&gt; explicando um novo &lt;em&gt;framework&lt;/em&gt; que busca reduzir o efeito das &lt;em&gt;“shortcut” features&lt;/em&gt; no &lt;em&gt;self-supervised representation learning&lt;/em&gt;. Essa é uma tarefa importante porquê, caso não seja realizada corretamente, o modelo pode falhar em aprender representações semânticas úteis e potencialmente se tornar ineficiente durante o &lt;em&gt;transfer learning&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sebastian Ruder publicou uma nova edição da newsletter &lt;em&gt;NLP News&lt;/em&gt;, que apresenta tópicos e recursos como análises de artigos de ML e NLP em 2019, e apresentações sobre os fundamentos do &lt;em&gt;Deep Learning&lt;/em&gt; e &lt;em&gt;Transfer Learning&lt;/em&gt;. Confira &lt;a href=&quot;http://newsletter.ruder.io/issues/accelerating-science-memorizing-vs-learning-to-look-things-up-schmidhuber-s-2010s-greek-bert-arc-illustrated-reformer-annotated-gpt-2-olmpics-223195&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;!-- Sebastian Ruder published a new issue of the NLP News newsletter that highlights topics and resources that range from an analysis of NLP and ML papers in 2019 to slides for learning about transfer learning and deep learning essentials. Check it out [here](http://newsletter.ruder.io/issues/accelerating-science-memorizing-vs-learning-to-look-things-up-schmidhuber-s-2010s-greek-bert-arc-illustrated-reformer-annotated-gpt-2-olmpics-223195). --&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Inscreva-se&lt;/em&gt;&lt;/a&gt; &lt;em&gt;🔖 para receber as próximas edições na sua caixa de entrada!&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter-PT-BR-_BERTology_Primer_fastpages_T5/&quot;&gt;NLP Newsletter [PT-BR] #6: BERTology Primer, fastpages, T5, Data Science Education, PyTorch Notebooks, Slow Science in ML&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on March 02, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP简报（Issue#5）：The Annotated GPT-2、CodeBERT、JAX、GANILLA等]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP%E7%AE%80%E6%8A%A5-Issue-5-The_Annotated_GPT-2-CodeBERT-JAX-GA/" />
  <id>https://dair.ai/NLP简报（Issue#5）：The_Annotated_GPT-2、CodeBERT、JAX、GA</id>
  <published>2020-02-29T00:00:00-06:00</published>
  <updated>2020-02-29T00:00:00-06:00</updated>
  <author>
    <name>Elvis Saravia</name>
    <uri>https://dair.ai</uri>
    <email>ellfae@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*YIhZsPaiBFkRMMWo5FAhGw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
欢迎来到 NLP 时事简报！全文较长，建议收藏。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;如果想让自己有趣的研究/项目出现在NLP简报中，随时在公众号后台留言联系我&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
来看看都有哪些内容，enjoy&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;1、Publications 📙&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;1.1 理解self-distillation&lt;/li&gt;
      &lt;li&gt;1.2 深度学习十年简史&lt;/li&gt;
      &lt;li&gt;1.3 利用神经网络求解高等数学方程&lt;/li&gt;
      &lt;li&gt;1.4 CodeBERT&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;2、Creativity and Society 🎨&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;2.1 AI for scientific discovery&lt;/li&gt;
      &lt;li&gt;2.2 改善image-to-illustration&lt;/li&gt;
      &lt;li&gt;2.3 Andrew Ng谈自监督学习&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;3、Tools and Datasets ⚙️&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;3.1 JAX libraries&lt;/li&gt;
      &lt;li&gt;3.2 处理维基百科数据的工具&lt;/li&gt;
      &lt;li&gt;3.3 Rust Tokenizers, DistilBERT base cased&lt;/li&gt;
      &lt;li&gt;3.4 夸夸语料&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;4、Ethics in AI 🚨&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;4.1 NLP和ML模型的道德考量&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;5、Articles and Blog posts ✍️&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;5.1 The Annotated GPT-2&lt;/li&gt;
      &lt;li&gt;5.2 Beyond BERT?&lt;/li&gt;
      &lt;li&gt;5.3 矩阵压缩算子&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;6、Education 🎓&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;6.1 NLP基础&lt;/li&gt;
      &lt;li&gt;6.2 数学基础课&lt;/li&gt;
      &lt;li&gt;6.3 书籍推荐&lt;/li&gt;
      &lt;li&gt;6.4 计算机科学自学指南&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;7、Noteworthy Mentions ⭐️&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1publications-&quot;&gt;&lt;strong&gt;1、Publications 📙&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1.1 理解self-distillation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
在深度学习中，self-distillation[1]是将知识从一种架构转移到另一种相同架构的过程。在训练时，原始模型的预测作为目标值提供给另一个模型。除具有所需的属性（例如减小模型大小）外，经验结果还表明该方法在held-out datasets上效果很好。&lt;/p&gt;

&lt;p&gt;一组研究人员最近发表了一篇论文，Self-Distillation Amplifies Regularization in Hilbert Space[2]，提供了理论分析，重点是更好地了解知识蒸馏过程中正在发生的事情以及为何有效。结果表明，几轮self-distillation会通过逐渐限制代表解的基函数的数量放大正则化，这往往会减少过度拟合。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*nWYO71awfooL4MrGK-tFww.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;1.2 深度学习十年简史&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
人工智能的先驱、LSTM之父JürgenSchmidhuber最近发布了一个新博客，The 2010s: Our Decade of Deep Learning / Outlook on the 2020s[3]，提供自2010年以来的深度学习历史概述，包括LSTM，前馈神经网络，GAN，深度强化学习，元学习，世界模型 ，蒸馏神经网络，注意学习等一些主题。文章最后总结了2020年代的前景，鼓励人们关注紧迫的问题，例如隐私和数据市场。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;1.3 利用神经网络求解高等数学方程&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Facebook AI研究人员发表了一篇论文，Deep Learning for Symbolic Mathematics[4]，声称提出了一个针对数学问题和匹配解决方案进行训练的模型，以学习预测诸如解决集成问题之类的任务的可能解决方案。该方法基于类似于机器翻译中使用的新颖框架，在该框架中，数学表达式表示为一种语言，而解决方案则视为翻译问题。因此，输出是解决方案本身，而不是模型输出翻译。据此，研究人员声称深度神经网络不仅擅长符号推理，而且还可以胜任各种任务。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*P_JtxoC8pYkXuXCp3e3QeQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;1.4 CodeBERT&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
在这篇名为《CodeBERT: A Pre-Trained Model for Programming and Natural Languages》[5]的论文中，来自哈工大、中山大学和微软的研究人员详细介绍了这一新预训练模型，该模型可处理双模态数据：编程语言（PL）和自然语言（NL）。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
CodeBERT 学习能够支持下游 NL-PL 应用的通用表示，比如自然语言代码搜索、代码文档生成，经实验 CodeBERT 模型在两项任务均取得 SOTA 效果，同时研究者构建了 NL-PL 探测数据集，CodeBERT 在 zero-shot 设置中的性能表现也持续优于 RoBERTa。&lt;/p&gt;

&lt;h2 id=&quot;2creativity-and-society-&quot;&gt;&lt;strong&gt;2、Creativity and Society 🎨&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;2.1 AI for scientific discovery&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Mattew Hutson报告了如何使用人工智能（AI）来生成仿真器[6]，这些仿真器在对复杂自然现象进行建模方面具有重要作用，而自然现象又可能导致不同类型的科学发现。构建这些仿真器的变化是，它们通常需要大规模数据和广泛的参数探索。最近的论文提出了DENSE方法[7]，一种基于神经结构搜索[8]来构建准确的仿真器，而仅依赖有限数量的训练数据。他们通过对包括天体物理学，气候科学和聚变能等在内的案例进行仿真来对其进行测试。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;2.2 改善image-to-illustration&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
GANILLA[9]是一种使用GAN来改进未配对的图像到图像翻译任务[10]中样式和内容传递的方法。提出了一种具有改进的生成器网络用于图像到插图的模型，并基于新的定量评估框架对模型进行了评估，该框架同时考虑了内容和样式。这项工作的新颖性在于拟议的生成器网络，该生成器网络考虑了先前模型无法实现的样式和内容之间的平衡。可以在此处阅读原文：GANILLA: Generative Adversarial Networks for Image to Illustration Translation[11]&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*l_B4vfaHVkXDwzM7SldiqQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;2.3 Andrew Ng谈自监督学习&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
deeplearning.ai的创始人Andrew Ng加入人工智能播客[12]，讨论的主题包括他早期从事ML的工作，AI的未来和AI教育，正确使用ML的建议，他的个人目标以及在2020年代应该关注ML技术。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew解释了为什么他对自监督的表示学习感到非常兴奋。自监督式学习涉及一个学习问题，该问题旨在从数据本身获得监督，以利用大量未标记数据，这比纯净标记数据更常见。这些表示很重要，可用于处理下游任务，类似于BERT等语言模型中使用的任务。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
使用自监督学习来学习广义的视觉表示也引起了很大关注，这使模型在资源匮乏的环境中更加准确。例如，最近一种名为SimCLR[13]的方法（由Geoffrey Hinton领导）提出了一种视觉表示的对比自监督学习框架，以改善在不同环境下的图像分类结果，例如转移学习和半监督学习。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*8zLzHFCyM3goc9y7KApHfg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3tools-and-datasets-️&quot;&gt;&lt;strong&gt;3、Tools and Datasets ⚙️&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;3.1 JAX libraries&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
JAX[14]是一个新库，结合了NumPy和自动微分功能，可以进行高性能ML研究。为了简化使用JAX构建神经网络的管道，DeepMind发布了Haiku[15]和RLax[16]。使用熟悉的面向对象编程模型，RLax简化了强化学习代理的实现，而Haiku简化了神经网络的构建。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;3.2 处理维基百科数据的工具&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sparkwiki[17] 是处理Wikipedia数据的工具。此版本是有趣的行为分析研究的众多努力的一部分，例如，捕获跨不同语言版本的Wikipedia的趋势和语言偏见[18]。作者发现，独立于语言，维基百科用户的浏览行为表明，他们倾向于在电影，音乐和体育等类别上拥有共同的兴趣，但是随着当地事件和文化特殊性的出现，差异变得更加明显。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*K7N9KbQlbuqowUeePjLtdw.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;3.3 Rust Tokenizers, DistilBERT base cased, Model cards&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hugging Face发行的新版Transformers[19]包括其快速分词器库的集成，该库旨在加速BERT，RoBERTa，GPT2等模型以及其他社区构建的模型。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;3.4 夸夸语料&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
夸夸语料[20]，来自豆瓣互相表扬组数据。&lt;/p&gt;

&lt;h2 id=&quot;4ethics-in-ai-&quot;&gt;&lt;strong&gt;4、Ethics in AI 🚨&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;4.1 NLP和ML模型的道德考量&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
在NLP Highlights的新内容中[21]，Emily Bender和主持人讨论了在学术界和实际使用情况下开发NLP模型和技术时的一些道德考量。讨论中的一些主题包括设计NLP任务，数据收集方法以及最终发布结果时的道德考虑。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
除了上述所有考虑因素之外，AI社区中经常讨论的一个问题过于关注优化指标，这与AI旨在实现的目标背道而驰。Rachel Thomas和David Uminsky[22]讨论了通过对不同用例进行透彻分析而可能出错的地方。他们还提出了一个缓解该问题的简单框架，其中涉及多个指标的使用和组合，然后是那些直接受到该技术影响的人的参与。&lt;/p&gt;

&lt;h2 id=&quot;5articles-and-blog-posts-️&quot;&gt;&lt;strong&gt;5、Articles and Blog posts ✍️&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;5.1 The Annotated GPT-2&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Aman Arora最近发表了一篇特别的博客文章，标题为“ The Annotated GPT-2[23]”，解释了基于Transformer的模型GPT-2的内部工作原理。他的方法受到The Annotated Transformer[24]的启发，采用了注释方法，通过代码和易于理解的解释来解释模型的重要部分。Aman付出了巨大的努力，使用PyTorch和Hugging Face的Transformers库重新实现OpenAI的GPT-2。这是出色的工作！&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*oRFMJTEojyQ-uocVES5GYA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;5.2 Beyond BERT?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sergi Castella[25]对BERT以外的内容感兴趣。主要主题包括改善指标，Hugging Face的Transformers库如何支持研究，查看有趣的数据集，解压缩模型等。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;5.3 矩阵压缩算子&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
TensorFlow博客发布了一篇博客文章，Matrix Compression Operator[26]，解释了在深度神经网络模型中压缩矩阵背后的技术和重要性。矩阵压缩可以帮助构建更有效的微型模型，这些模型可以集成到较小的设备中，例如电话和家庭助理。通过低秩逼近和量化等方法专注于模型的压缩，这意味着我们无需牺牲模型的质量。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*fpAdJvBIf4SKxF3gTIpe_g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;6education-&quot;&gt;&lt;strong&gt;6、Education 🎓&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;6.1 NLP基础&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
NLP基础[27]从基础开始讲授NLP概念，同时分享最佳实践，重要参考，应避免的常见错误以及NLP的未来。包含一个Colab笔记本[28]，该项目将在此github[29]维护。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*mS5NcoJ_c8hYTjiJsuu_8g.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;6.2 数学基础课&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Machine Learning Tokyo 将在3月8日主持一个远程在线讨论，其中回顾他们最近的在线学习课程中[30]涉及的章节。该小组以前研究过Marc Peter Deisenroth，Ado Faisal和Cheng Soon Ong所著的《机器学习数学》[31]一书章节。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;6.3 书籍推荐&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
在上一部分中，我们讨论了矩阵压缩对于构建微型ML模型的重要性。如果你有兴趣了解有关如何为嵌入式系统构建更小的深度神经网络的更多信息，请查看Pete Warden和Daniel Situnayake撰写的这本名为TinyML[32]的出色著作。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*omOa3aw2bfMzX2Qm.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
另一本值得关注的有趣书籍是即将出版的题为“Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD[33]”，作者为Jeremy Howard和Sylvain Gugger。该书旨在提供必要的数学基础，以建立和训练模型来处理计算机视觉和NLP领域中的任务。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*i2OmtWOncatOYsZv.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;6.4 计算机科学自学指南&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
使用建议的教科书或视频讲座系列，以大致顺序学习以下所有九个主题，但理想情况下两者都学习。针对每个主题进行100-200小时的学习，然后在整个职业生涯中重温最爱favorites。另外在reddit上也有类似的讨论[34]。&lt;/p&gt;

&lt;h2 id=&quot;7noteworthy-mentions-️&quot;&gt;&lt;strong&gt;7、Noteworthy Mentions ⭐️&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Torchmeta[35]是一个是由Tristan Deleu创作的可以轻松使用相关的数据加载器进行元学习研究的库。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Manuel Tonneau撰写了一篇文章，仔细研究了语言建模中涉及的一些机制[36]，包括贪婪和波束搜索以及原子核采样等主题。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
MIT发布了名为“Introduction to Deep Learning[37]”的课程的完整提纲和课程表，其中包括已授课的视频， 他们的目标是每周发布视频讲座和幻灯片。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
了解如何使用基于Transformer的方法在不到300行代码中训练用于命名实体识别（NER）的模型[38]。您可以在此处找到随附的Google Colab[39]。&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;本文参考资料&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[1] &lt;strong&gt;self-distillation:&lt;/strong&gt; https://arxiv.org/pdf/1503.02531.pdf&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[2] &lt;strong&gt;Self-Distillation Amplifies Regularization in Hilbert Space:&lt;/strong&gt; http://xxx.itp.ac.cn/abs/2002.05715&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[3] &lt;strong&gt;The 2010s: Our Decade of Deep Learning / Outlook on the 2020s:&lt;/strong&gt; http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[4] &lt;strong&gt;Deep Learning for Symbolic Mathematics:&lt;/strong&gt; https://arxiv.org/abs/1912.01412&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[5] &lt;strong&gt;《CodeBERT: A Pre-Trained Model for Programming and Natural Languages》:&lt;/strong&gt; https://arxiv.org/abs/2002.08155&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[6] &lt;strong&gt;如何使用人工智能（AI）来生成仿真器:&lt;/strong&gt; https://www.sciencemag.org/news/2020/02/models-galaxies-atoms-simple-ai-shortcuts-speed-simulations-billions-times&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[7] &lt;strong&gt;论文提出了DENSE方法:&lt;/strong&gt; https://arxiv.org/abs/2001.08055&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[8] &lt;strong&gt;神经结构搜索:&lt;/strong&gt; https://en.wikipedia.org/wiki/Neural_architecture_search&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[9] &lt;strong&gt;GANILLA:&lt;/strong&gt; https://github.com/giddyyupp/ganilla&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[10] &lt;strong&gt;图像到图像翻译任务:&lt;/strong&gt; https://paperswithcode.com/task/image-to-image-translation&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[11] &lt;strong&gt;GANILLA: Generative Adversarial Networks for Image to Illustration Translation:&lt;/strong&gt; https://arxiv.org/abs/2002.05638&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[12] &lt;strong&gt;人工智能播客:&lt;/strong&gt; https://www.youtube.com/watch?v=0jspaMLxBig&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[13] &lt;strong&gt;SimCLR:&lt;/strong&gt; https://arxiv.org/abs/2002.05709&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[14] &lt;strong&gt;JAX:&lt;/strong&gt; https://github.com/google/jax&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[15] &lt;strong&gt;Haiku:&lt;/strong&gt; https://github.com/deepmind/dm-haiku&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[16] &lt;strong&gt;RLax:&lt;/strong&gt; https://github.com/deepmind/rlax&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[17] &lt;strong&gt;Sparkwiki:&lt;/strong&gt; https://github.com/epfl-lts2/sparkwiki&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[18] &lt;strong&gt;捕获跨不同语言版本的Wikipedia的趋势和语言偏见:&lt;/strong&gt; https://arxiv.org/abs/2002.06885&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[19] &lt;strong&gt;新版Transformers:&lt;/strong&gt; https://github.com/huggingface/transformers/releases/tag/v2.5.0&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[20] &lt;strong&gt;夸夸语料:&lt;/strong&gt; https://github.com/xiaopangxia/kuakua_corpus&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[21] &lt;strong&gt;NLP Highlights的新内容中:&lt;/strong&gt; https://soundcloud.com/nlp-highlights/106-ethical-considerations-in-nlp-research-emily-bender&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[22] &lt;strong&gt;Rachel Thomas和David Uminsky:&lt;/strong&gt; https://arxiv.org/abs/2002.08512&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[23] &lt;strong&gt;The Annotated GPT-2:&lt;/strong&gt; https://amaarora.github.io/2020/02/18/annotatedGPT2.html&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[24] &lt;strong&gt;The Annotated Transformer:&lt;/strong&gt; https://nlp.seas.harvard.edu/2018/04/03/attention.html&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[25] &lt;strong&gt;Sergi Castella:&lt;/strong&gt; https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[26] &lt;strong&gt;Matrix Compression Operator:&lt;/strong&gt; https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[27] &lt;strong&gt;NLP基础:&lt;/strong&gt; https://medium.com/dair-ai/fundamentals-of-nlp-chapter-1-tokenization-lemmatization-stemming-and-sentence-segmentation-b362c5d07684&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[28] &lt;strong&gt;Colab笔记本:&lt;/strong&gt; https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[29] &lt;strong&gt;此github:&lt;/strong&gt; https://github.com/dair-ai/nlp_fundamentals&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[30] &lt;strong&gt;在线学习课程中:&lt;/strong&gt; https://www.meetup.com/Machine-Learning-Tokyo/events/268817313/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[31] &lt;strong&gt;《机器学习数学》:&lt;/strong&gt; https://mml-book.github.io/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[32] &lt;strong&gt;TinyML:&lt;/strong&gt; https://tinymlbook.com/?linkId=82595412&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[33] &lt;strong&gt;Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD:&lt;/strong&gt; https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[34] &lt;strong&gt;reddit上也有类似的讨论:&lt;/strong&gt; https://www.reddit.com/r/learnprogramming/comments/87j7fw/teach_yourself_computer_science_a_diy_curriculum/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[35] &lt;strong&gt;Torchmeta:&lt;/strong&gt; https://arxiv.org/abs/1909.06576&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[36] &lt;strong&gt;语言建模中涉及的一些机制:&lt;/strong&gt; https://creatext.ai/blog-posts/machine-text-writing-gpt2-beam-search?utm_medium=newsletter&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[37] &lt;strong&gt;Introduction to Deep Learning:&lt;/strong&gt; http://introtodeeplearning.com/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[38] &lt;strong&gt;训练用于命名实体识别（NER）的模型:&lt;/strong&gt; https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[39] &lt;strong&gt;随附的Google Colab:&lt;/strong&gt; https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP%E7%AE%80%E6%8A%A5-Issue-5-The_Annotated_GPT-2-CodeBERT-JAX-GA/&quot;&gt;NLP简报（Issue#5）：The Annotated GPT-2、CodeBERT、JAX、GANILLA等&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on February 29, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter: GPT-2 Anotado, Entendendo self-distillation, Haiku, GANILLA, Sparkwiki, Ética em NLP, Torchmeta,…]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter-PT-BR-_The_Annotated_GPT-2,_Understanding/" />
  <id>https://dair.ai/NLP_Newsletter[PT-BR]_The_Annotated_GPT-2,_Understanding</id>
  <published>2020-02-29T00:00:00-06:00</published>
  <updated>2020-02-29T00:00:00-06:00</updated>
  <author>
    <name>Flavio Clesio</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*YIhZsPaiBFkRMMWo5FAhGw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Antes de tudo, gostaria de agradecer de ❤️ a todos vocês pelo incrível apoio e incentivo para continuar com a NLP Newsletter. Esse esforço requer pesquisa, edição, e tradução tediosas, mas que considero gratificantes e úteis para fornecer o melhor conteúdo. Espero que você esteja gostando deste conteúdo. 😉&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Assine a NLP Newsletter&lt;/em&gt;&lt;/a&gt; &lt;em&gt;🔖 para receber edições futuras via e-mail.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;publicações-&quot;&gt;Publicações 📙&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Um entendimento teórico do self-distillation&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
No contexto de Deep Learning, &lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot;&gt;&lt;em&gt;self-distillation&lt;/em&gt;&lt;/a&gt; (&lt;em&gt;NT: auto-destilação&lt;/em&gt;) é o processo de transferência de conhecimento de uma arquitetura para outra. As previsões do modelo original são alimentadas como valores de destino para o outro modelo durante o treinamento. Além de ter propriedades desejáveis como a redução do tamanho dos modelos, os resultados empíricos mostram que essa abordagem funciona bem em conjuntos de dados não vistos anteriormente pelo modelo (NT: amostras &lt;em&gt;held out&lt;/em&gt;). Um grupo de pesquisadores publicou recentemente um artigo que fornece uma análise teórica com o foco em um melhor entendimento sobre o que está acontecendo neste processo de &lt;em&gt;destilação do conhecimento&lt;/em&gt; e o porque ele é eficaz. Os resultados mostram que alguns poucos ciclos de destilação amplificam a regularização (devido ao fato que a técnica &lt;a href=&quot;https://twitter.com/TheGradient/status/1228132843630387201?s=20&quot;&gt;&lt;em&gt;progressivamente ajuda a limitar o número de funções base que representam a solução&lt;/em&gt;&lt;/a&gt;) as quais tendem a reduzir o over-fitting. (Leia o paper &lt;a href=&quot;https://arxiv.org/abs/2002.05715&quot;&gt;&lt;strong&gt;aqui&lt;/strong&gt;&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*nWYO71awfooL4MrGK-tFww.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.05715&quot;&gt;&lt;em&gt;Fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Os anos 2010s: Nossa década de Deep Learning / Perspectivas para os 2020s&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;http://people.idsia.ch/~juergen/&quot;&gt;Jürgen Schmidhuber,&lt;/a&gt; um dos pioneiros em Inteligência Artificial,  &lt;a href=&quot;http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html&quot;&gt;&lt;strong&gt;postou recentemente em seu blog&lt;/strong&gt;&lt;/a&gt; uma visão histórica sobre Deep Learning desde o ano de 2010. Alguns tópicos incluem &lt;a href=&quot;https://en.wikipedia.org/wiki/Long_short-term_memory&quot;&gt;LSTMs&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Feedforward_neural_network&quot;&gt;feedforward neural networks&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_adversarial_network&quot;&gt;GANs&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_reinforcement_learning&quot;&gt;deep reinforcement learning&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Meta_learning_(computer_science)&quot;&gt;meta-learning&lt;/a&gt;, world models, &lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot;&gt;distilling NNs&lt;/a&gt;, &lt;a href=&quot;https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f&quot;&gt;attention learning&lt;/a&gt;, etc. O artigo traz algumas perspectivas futuras para os anos 2020 chamando atenção para questões como privacidade e mercado de dados.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Usando Redes Neurais para a resolução de equações matemáticas&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Pesquisadores do Facebook AI publicaram um &lt;a href=&quot;https://arxiv.org/abs/1912.01412&quot;&gt;&lt;strong&gt;paper&lt;/strong&gt;&lt;/a&gt; em que apresentam um modelo treinado em problemas de matemática para prever possíveis soluções para inúmeras tarefas como, por exemplo, problemas de integração. A abordagem é baseada em uma nova estrutura semelhante à usada na &lt;a href=&quot;https://en.wikipedia.org/wiki/Neural_machine_translation&quot;&gt;neural machine translation&lt;/a&gt; (&lt;em&gt;NT: tradução automática neural&lt;/em&gt;), em que expressões matemáticas são representadas como um tipo de linguagem e as soluções tratadas como um problema de tradução. Assim, ao invés do modelo produzir uma tradução, a saída desta tradução é a própria solução do problema. Com isso, os pesquisadores afirmam que as redes Deep Learning não são apenas boas em raciocínio simbólico, mas podem ser usadas também para tarefas mais diversas.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*P_JtxoC8pYkXuXCp3e3QeQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Equações sendo usadas como entrada, juntamente com a solução correspondente gerada pelo modelo—&lt;/em&gt; &lt;a href=&quot;https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;criatividade-e-sociedade-&quot;&gt;Criatividade e Sociedade 🎨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Inteligência Artificial para descobertas científicas&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Mattew Hutson &lt;a href=&quot;https://www.sciencemag.org/news/2020/02/models-galaxies-atoms-simple-ai-shortcuts-speed-simulations-billions-times&quot;&gt;&lt;strong&gt;informa&lt;/strong&gt;&lt;/a&gt; como a inteligência artificial (IA) pode ser utilizada para produzir emuladores que têm um uso importante na modelagem de fenômenos naturais complexos e que, por sua vez, podem levar a diferentes tipos de &lt;em&gt;descobertas científicas&lt;/em&gt;. A mudança na construção desses emuladores acontece devido ao fato de que estes modelos geralmente exigem dados em larga escala e uma vasta exploração de parâmetros. Um &lt;a href=&quot;https://arxiv.org/abs/2001.08055&quot;&gt;&lt;strong&gt;paper recente&lt;/strong&gt;&lt;/a&gt; propõe um método chamado DENSE que é uma abordagem baseada em &lt;a href=&quot;https://en.wikipedia.org/wiki/Neural_architecture_search&quot;&gt;&lt;em&gt;neural architecture search (NAS)&lt;/em&gt;&lt;/a&gt; (NT: Exploração e busca de arquitetura de Redes Neurais) para criar emuladores precisos, contando apenas com uma quantidade limitada de dados de treinamento. Eles o testaram executando simulações para casos que incluem astrofísica, ciência climática e energia de fusão, entre outros.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Melhorando a tradução de imagem para imagem&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/2002.05638&quot;&gt;GANILLA&lt;/a&gt; é uma abordagem que propõe o uso de &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_adversarial_network&quot;&gt;GANs&lt;/a&gt; para melhorar a transferência de estilo e conteúdo em pares para tarefas de tradução &lt;a href=&quot;https://paperswithcode.com/task/image-to-image-translation&quot;&gt;&lt;em&gt;image-to-image&lt;/em&gt;&lt;/a&gt; (NT: imagem para imagem). A abordagem propōe um modelo de imagem para imagem (com uma rede de geradores aprimorada) e este modelo é avaliado com base em uma nova estrutura de avaliação quantitativa que considera tanto o conteúdo quanto o estilo. A novidade do trabalho está na rede de geradores proposta, que considera um equilíbrio entre estilo e conteúdo que os modelos anteriores não conseguem. O código e os modelos pré-treinados estão &lt;a href=&quot;https://github.com/giddyyupp/ganilla&quot;&gt;disponíveis&lt;/a&gt;. Leia o artigo completo &lt;a href=&quot;https://arxiv.org/abs/2002.05638&quot;&gt;&lt;strong&gt;aqui&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*l_B4vfaHVkXDwzM7SldiqQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Andrew Ng fala sobre o interesse em aprendizagem auto-supervisionada&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew Ng, o fundador do &lt;a href=&quot;deeplearning.ai&quot;&gt;deeplearning.ai&lt;/a&gt;, falou no &lt;a href=&quot;https://www.youtube.com/watch?v=0jspaMLxBig&quot;&gt;&lt;strong&gt;podcast de Inteligência Artificial do Lex Friedman&lt;/strong&gt;&lt;/a&gt; sobre os seguintes tópicos: seus primeiros anos em ML, o futuro da IA, educação em IA, recomendações para o uso adequado da ML, seus objetivos pessoais e quais técnicas de ML que devemos prestar atenção nesta década de 2020.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew explicou o motivo da sua animação em relação ao &lt;em&gt;self-supervised representation learning.&lt;/em&gt; &lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html&quot;&gt;&lt;strong&gt;Self-supervised learning&lt;/strong&gt;&lt;/a&gt; (NT: aprendizado de representação auto-supervisionado) envolve a estruturação de um problema de aprendizagem que visa obter supervisão dos próprios dados para fazer uso de grandes quantidades de dados não rotulados, o que é mais comum que os dados rotulados limpos. As representações são importantes e podem ser usadas para lidar com tarefas posteriores, semelhantes às usadas em modelos de linguagem como o &lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#bert&quot;&gt;BERT&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Também há muito interesse em usar o aprendizado auto-supervisionado para treinamento de representações visuais generalizadas que tornam os modelos mais precisos em ambientes com poucos recursos. Por exemplo, um método recente chamado &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;&lt;strong&gt;SimCLR&lt;/strong&gt;&lt;/a&gt; (liderado por &lt;a href=&quot;https://en.wikipedia.org/wiki/Geoffrey_Hinton&quot;&gt;Geoffrey Hinton&lt;/a&gt;) propõe uma estrutura para &lt;em&gt;aprendizagem auto-supervisionada contrastante&lt;/em&gt; (&lt;em&gt;NT: contrastive self-supervised learning&lt;/em&gt;) de representações visuais para melhorar a classificação de imagens em diferentes configurações, como transferência de aprendizado (NT: &lt;a href=&quot;https://en.wikipedia.org/wiki/Transfer_learning&quot;&gt;transfer learning&lt;/a&gt;) e aprendizado semi-supervisionado.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*8zLzHFCyM3goc9y7KApHfg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;ferramentas-e-datasets-️&quot;&gt;Ferramentas e Datasets ⚙️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Bibliotecas JAX&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/google/jax&quot;&gt;JAX&lt;/a&gt; é uma nova biblioteca que combina o NumPy e &lt;a href=&quot;https://en.wikipedia.org/wiki/Automatic_differentiation&quot;&gt;diferenciação automática&lt;/a&gt; para realizar pesquisas de ML de alto desempenho. Para simplificar os pipelines para a construção de redes neurais usando JAX, a &lt;a href=&quot;https://deepmind.com/&quot;&gt;DeepMind&lt;/a&gt; lançou o &lt;a href=&quot;https://github.com/deepmind/dm-haiku&quot;&gt;&lt;strong&gt;Haiku&lt;/strong&gt;&lt;/a&gt; e &lt;a href=&quot;https://github.com/deepmind/rlax&quot;&gt;&lt;strong&gt;RLax&lt;/strong&gt;&lt;/a&gt;. O RLax simplifica a implementação de agentes de aprendizado por reforço e o Haiku simplifica a construção de redes neurais usando &lt;em&gt;modelos familiares com o paradigma de programação orientada a objetos.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Uma ferramenta para processar dados da Wikipédia&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/epfl-lts2/sparkwiki&quot;&gt;&lt;strong&gt;Sparkwiki&lt;/strong&gt;&lt;/a&gt; é uma ferramenta para processar dados da Wikipédia. Esta versão faz parte de muitos esforços para permitir pesquisas interessantes de análise comportamental, como &lt;a href=&quot;https://arxiv.org/abs/2002.06885&quot;&gt;a captura de tendências e preconceitos em diferentes idiomas na Wikipédia&lt;/a&gt;. Os autores descobriram que, independentemente do idioma, o comportamento de navegação dos usuários da Wikipédia mostra que eles tendem a compartilhar interesses comuns por categorias como filmes, música e esportes, mas as diferenças se tornam mais aparentes com eventos locais e particularidades culturais.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*K7N9KbQlbuqowUeePjLtdw.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Tokenizers em Rust, DistilBERT e outros&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Um novo release dos &lt;a href=&quot;https://github.com/huggingface/transformers/releases/tag/v2.5.0&quot;&gt;&lt;strong&gt;Transformers&lt;/strong&gt;&lt;/a&gt; da Hugging Face inclui a integração de sua biblioteca de tokenização rápida, que visa acelerar modelos como o &lt;a href=&quot;https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270&quot;&gt;BERT&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot;&gt;RoBERTa&lt;/a&gt;, &lt;a href=&quot;https://openai.com/blog/better-language-models/&quot;&gt;GPT-2&lt;/a&gt; e outros modelos criados pela comunidade.&lt;/p&gt;

&lt;h1 id=&quot;ética-em-inteligência-artificial-&quot;&gt;Ética em Inteligência Artificial 🚨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Considerações éticas para modelos de NLP (Processamento de Linguagem Natural) e Machine Learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Em um novo &lt;a href=&quot;https://soundcloud.com/nlp-highlights/106-ethical-considerations-in-nlp-research-emily-bender&quot;&gt;&lt;strong&gt;episódio&lt;/strong&gt;&lt;/a&gt; do postcast &lt;a href=&quot;https://soundcloud.com/nlp-highlights&quot;&gt;NLP Highlights,&lt;/a&gt; &lt;a href=&quot;https://twitter.com/emilymbender&quot;&gt;Emily Bender&lt;/a&gt; e os hosts conversaram sobre algumas considerações éticas no desenvolvimento de modelos e tecnologias de NLP no contexto da academia e do seu uso no mundo real. Alguns dos tópicos da discussão incluem considerações éticas nas tarefas de NLP, abordagens sobre coleta de dados e eventualmente considerações na publicação de resultados.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Além de todas as considerações acima, uma preocupação discutida é que a comunidade de IA está se concentrando demais na otimização de métricas específicas, o que contraria os objetivos que a IA pretende alcançar. Rachel Thomas e David Uminsky discutem os problemas dessa abordagem através de uma &lt;a href=&quot;https://arxiv.org/abs/2002.08512&quot;&gt;&lt;strong&gt;análise completa&lt;/strong&gt;&lt;/a&gt; de diferentes casos de uso. Eles também propõem uma estrutura simples para mitigar este problema, que envolve o uso e a combinação de várias métricas, seguidas pelo envolvimento das pessoas afetadas diretamente pela tecnologia.&lt;/p&gt;

&lt;h1 id=&quot;artigos-e-blog-posts-️&quot;&gt;Artigos e Blog posts ✍️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;“The Annotated GPT-2”&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Aman Arora publicou recentemente uma postagem no blog excepcionalmente intitulada &lt;a href=&quot;https://amaarora.github.io/2020/02/18/annotatedGPT2.html&quot;&gt;“The Annotated GPT-2“&lt;/a&gt; explicando o funcionamento interno do modelo baseado em &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)&quot;&gt;Transformer&lt;/a&gt; chamado &lt;a href=&quot;https://openai.com/blog/better-language-models/&quot;&gt;GPT-2&lt;/a&gt;. Sua abordagem foi inspirada em &lt;a href=&quot;https://nlp.seas.harvard.edu/2018/04/03/attention.html&quot;&gt;The Annotated Transformer&lt;/a&gt; que adotou uma abordagem de anotação para explicar as partes importantes do modelo. Aman fez um grande esforço para reimplementar o &lt;a href=&quot;https://openai.com/blog/better-language-models/&quot;&gt;GPT-2&lt;/a&gt; da &lt;a href=&quot;https://openai.com/&quot;&gt;OpenAI&lt;/a&gt; usando o &lt;a href=&quot;https://pytorch.org/&quot;&gt;PyTorch&lt;/a&gt; e a biblioteca &lt;a href=&quot;https://huggingface.co/transformers/&quot;&gt;Transformers da Hugging Face&lt;/a&gt;. É um trabalho brilhante!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*oRFMJTEojyQ-uocVES5GYA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amaarora.github.io/2020/02/18/annotatedGPT2.html&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Além do BERT?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1&quot;&gt;&lt;strong&gt;Um ponto interessante foi levantado&lt;/strong&gt;&lt;/a&gt; por Sergi Castella sobre o que está além do &lt;a href=&quot;https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270&quot;&gt;BERT&lt;/a&gt;. Os principais tópicos incluem o aprimoramento das métricas, uma reflexão de como a biblioteca &lt;a href=&quot;https://huggingface.co/transformers/&quot;&gt;Transformers da Hugging Face&lt;/a&gt; ajuda na pesquisa, alguns conjuntos de dados interessantes para análise, etc.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Operador de Compressão de Matrizes&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
O Blog do TensorFlow publicou um &lt;a href=&quot;https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&quot;&gt;&lt;strong&gt;post&lt;/strong&gt;&lt;/a&gt; explicando as técnicas e a importância por trás da compressão matrizes em um modelo de Deep Learning. &lt;em&gt;A compactação matricial&lt;/em&gt; (&lt;em&gt;NT: Matrix compression&lt;/em&gt;) pode ajudar a criar modelos menores e mais eficientes que podem ser incorporados a dispositivos menores, como telefones e assistentes domésticos. Concentrar-se na compressão dos modelos por meio de métodos como &lt;a href=&quot;https://en.wikipedia.org/wiki/Low-rank_approximation&quot;&gt;low-rank-approximation&lt;/a&gt; e &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantization_(signal_processing)&quot;&gt;quantização&lt;/a&gt; significa que não precisamos comprometer a qualidade do modelo.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*fpAdJvBIf4SKxF3gTIpe_g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;educação-&quot;&gt;Educação 🎓&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Fundamentos de NLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Estou animado por lançado um rascunho do Capítulo 1 da minha nova série chamado &lt;a href=&quot;https://medium.com/dair-ai/fundamentals-of-nlp-chapter-1-tokenization-lemmatization-stemming-and-sentença-segmentação-b362c5d07684&quot;&gt;&lt;strong&gt;Fundamentos de NLP&lt;/strong&gt;&lt;/a&gt;. Esta série ensina conceitos de NLP a partir do básico, compartilhando boas práticas, referências importantes, erros comuns a serem evitados e o que está por vir no que se refere a NLP. Um &lt;a href=&quot;https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ&quot;&gt;notebook no Colab&lt;/a&gt; foi incluído e o projeto será mantido &lt;a href=&quot;https://github.com/dair-ai/nlp_fundamentals&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*mS5NcoJ_c8hYTjiJsuu_8g.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Revisão/Discussão Online: Parte I sessão de leitura para fundamentos da matemática&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
O time do &lt;a href=&quot;https://www.meetup.com/Machine-Learning-Tokyo/&quot;&gt;Meetup “Machine Learning Tokyo”&lt;/a&gt; está hospedando uma discussão on-line remota, revisando capítulos que foram abordados em suas recentes sessões de estudo on-line. O grupo já havia estudado capítulos com base no livro &lt;a href=&quot;https://mml-book.github.io/&quot;&gt;Mathematics For Machine Learning&lt;/a&gt; escrito por Marc Peter Deisenroth, A Aldo Faisal e Cheng Soon Ong. O &lt;a href=&quot;https://www.meetup.com/Machine-Learning-Tokyo/events/268817313/&quot;&gt;&lt;strong&gt;evento&lt;/strong&gt;&lt;/a&gt; está programado para 8 de março de 2020.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Recomendações de livros&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Em um segmento anterior, discutimos a importância da compressão de matriz para a construção de modelos pequenos (em termos de espaço) de ML. Se você estiver interessado em aprender mais sobre como construir redes neurais profundas menores para sistemas embarcados, confira este ótimo livro chamado &lt;a href=&quot;https://tinymlbook.com/?linkId=82595412&quot;&gt;&lt;strong&gt;TinyML&lt;/strong&gt;&lt;/a&gt; de Pete Warden e Daniel Situnayake.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*omOa3aw2bfMzX2Qm.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.amazon.com/TinyML-Learning-TensorFlow-Ultra-Low-Micro-Controllers/dp/1492052043&quot;&gt;&lt;em&gt;fonte&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Outro livro interessante para ficar de olho é o próximo título  &lt;strong&gt;“&lt;/strong&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;&lt;strong&gt;Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;”&lt;/strong&gt; de Jeremy Howard e Sylvain Gugger. O livro tem como objetivo fornecer a base matemática necessária para criar e treinar modelos para abordar tarefas nas áreas de visão computacional e NLP.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*i2OmtWOncatOYsZv.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;menções-honrosas-️&quot;&gt;Menções honrosas ⭐️&lt;/h1&gt;

&lt;p&gt;Você pode acessar a NLP Newsletter anterior em PT-BR &lt;a href=&quot;https://dair.ai/NLP_Newsletter-PT-BR-_PyTorch3D,_DeepSpeed,_Turing-NLG/&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1909.06576&quot;&gt;&lt;strong&gt;Torchmeta&lt;/strong&gt;&lt;/a&gt; é uma biblioteca para pesquisa em meta-aprendizado. Esta biblioteca é de autoria de Tristan Deleu.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Manuel Tonneau escreveu um &lt;a href=&quot;https://creatext.ai/blog-posts/machine-text-writing-gpt2-beam-search?utm_medium=newsletter&quot;&gt;&lt;strong&gt;post&lt;/strong&gt;&lt;/a&gt; oferecendo uma visão mais detalhada em relação ao hardware envolvido em modelagem de linguagem. Alguns tópicos incluem &lt;em&gt;greedy&lt;/em&gt; e &lt;a href=&quot;https://en.wikipedia.org/wiki/Beam_search&quot;&gt;beam search&lt;/a&gt; e &lt;a href=&quot;https://openreview.net/forum?id=rygGQyrFvH&quot;&gt;nucleus sampling&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
O MIT &lt;a href=&quot;http://introtodeeplearning.com/&quot;&gt;&lt;strong&gt;lançou&lt;/strong&gt;&lt;/a&gt; o plano de estudos completo e a programação do curso intitulado “Introdução ao Deep Learning”, incluindo vídeos das palestras já ministradas. Eles pretendem lançar palestras em vídeo e slides uma vez por semana.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Aprenda a treinar um modelo para &lt;a href=&quot;https://en.wikipedia.org/wiki/Named-entity_recognition&quot;&gt;reconhecimento de entidade (NER)&lt;/a&gt; usando uma abordagem baseada no Transformer em &lt;a href=&quot;https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py&quot;&gt;&lt;strong&gt;300 linhas de código&lt;/strong&gt;&lt;/a&gt;. Você pode encontrar o Google Colab em anexo &lt;a href=&quot;https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn&quot;&gt;aqui&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Se você tiver datasets, projetos, postagens de blog, tutoriais ou documentos que deseja compartilhar na próxima edição da NLP Newsletter, entre em contato conosco pelo e-mail ellfae@gmail.com ou via &lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;&lt;strong&gt;DM no Twitter&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Assine a NLP Newsletter&lt;/em&gt;&lt;/a&gt; &lt;em&gt;🔖 para receber edições futuras via e-mail.&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter-PT-BR-_The_Annotated_GPT-2,_Understanding/&quot;&gt;NLP Newsletter: GPT-2 Anotado, Entendendo self-distillation, Haiku, GANILLA, Sparkwiki, Ética em NLP, Torchmeta,…&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on February 29, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[الإصدار الخامس من قائمة معالجة اللغات الطبيعية (NLP) البريدية: GPT-2 المشروح، فهم التقطير-الذاتي، Haiku، GANILLA، Sparkwiki، الأخلاق في علم معالجة اللغات الطبيعية، Torchmeta، والمزيد ...]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter.AR._The_Annotated_GPT-2_And_More/" />
  <id>https://dair.ai/NLP_Newsletter.AR._The_Annotated_GPT-2_And_More</id>
  <published>2020-02-28T00:00:00-06:00</published>
  <updated>2020-02-28T00:00:00-06:00</updated>
  <author>
    <name>Sufian Diraneyya</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*YIhZsPaiBFkRMMWo5FAhGw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
بداية، لا أستطيع شكركم ❤️ جميعاً كفاية للدعم الهائل والتشجيع لمواصلة هذه القائمة البريدية. هذا الجهد يحتاج بحث وتعديل مضجرين، الذين أراهما يستحقان، ويساعدان في تزويدكم بأفضل محتوى. أتمنى أن تستمتعوا بهذه الإصدارات، لأنني أفعل 😉.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;strong&gt;اشترك&lt;/strong&gt;&lt;/a&gt; &lt;em&gt;🔖 في قائمة NLP البريدية لتصلك الأعداد القادمة مباشرة على صندوق بريدك.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;المنشورات-&quot;&gt;المنشورات 📙&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;فهم نظري لل”التقطير الذاتي” (Self-Distillation)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
في سياق التعلم العميق (Deep Learning)، التقطير الذاتي أو ال(&lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot;&gt;&lt;strong&gt;Self-Distillation&lt;/strong&gt;&lt;/a&gt;) بالإنجليزية هي عملية تحويل المعرفة من معمارية (Architecture) ما إلى معمارية أخرى مماثلة. توقعات النموذج الأصلي تتم تغذيتها كقيم مستهدفة إلى النموذج الآخر أثناء التدريب. بجانب وجود بعض مزايا، مثل تقليل حجم النموذج، تُظهر النتائج التجريبية أن هذا الأسلوب يعمل جيداً على قواعد البيانات المفصولة (Held-Out Datasets). نشر مجموعة من الباحثين ورقة بحثية مؤخراً تقدم تحليلاً نظرياً مع التركيز على فهم أعمق لما يحدث في عملية استخراج (“تقطير”) المعرفة هذه ولماذا هي فعالة. النتائج تظهر أن بضع جولات من “التقطير” تزيد التنظيم (Regularization) (عن طريق &lt;a href=&quot;https://twitter.com/TheGradient/status/1228132843630387201?s=20&quot;&gt;&lt;strong&gt;الحد تدريجياً من عدد الوظائف الأساسية التي تمثل الحل&lt;/strong&gt;&lt;/a&gt;) الذي يميل إلى تقليل الملائمة الزائدة (Over-Fitting). (اقرأ الورقة كاملة (باللغة الإنجليزية) &lt;a href=&quot;https://arxiv.org/abs/2002.05715&quot;&gt;&lt;strong&gt;هنا&lt;/strong&gt;&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*nWYO71awfooL4MrGK-tFww.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;الشكل 1: مخطط توضيحي لعملية التقطير الذاتي لدورتين&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/2002.05715&quot;&gt;&lt;em&gt;المصدر&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;عقد 2010 إلى 2019: عقدنا من التعلم العميق / توقعات العقد القادم 2020 - 2029&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;http://people.idsia.ch/~juergen/&quot;&gt;&lt;strong&gt;Jürgen Schmidhuber&lt;/strong&gt;&lt;/a&gt;، رائد في الذكاء الاصطناعي، نشر مؤخراً &lt;a href=&quot;http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html&quot;&gt;&lt;strong&gt;تدوينة جديدة&lt;/strong&gt;&lt;/a&gt; مركزاً على تقديم نظرة تاريخية على التعلم العميق من عام 2010 وحتى الآن. ذكر فيها بعض المواضيع مثل: وحدات الذاكرة الطويلة قصيرة-المدى (LSTMs)، شبكات التغذية الأمامية العصبية (Feedforward Neural Network)، شبكات الخصومة المولِدة (Generative Adversarial Networks - GANs)، التعليم المعزز العميق (Deep Reinforcement Learning)، التعليم الفوقي (Meta-Learning)، نماذج العالم (World Models)، تقطير الشبكات العصبية (Distilling NNs)، التعلم بالانتباه (Attention Learning)، وغيرها من المواضيع. المقالة ختمت بنظرة إلى القرن القادم بدئاً من عام 2020 مشجعة الاهتمام بمشاكل مثل الخصوصية وأسواق البيانات.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;استخدام الشبكات العصبية لحل معادلات رياضية متقدمة&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
باحثو الذكاء الاصطناعي في فيسبوك نشروا &lt;a href=&quot;https://arxiv.org/abs/1912.01412&quot;&gt;&lt;strong&gt;ورقة بحثية&lt;/strong&gt;&lt;/a&gt; تدعي أن تقدم نموذجاً مدرباً على المسائل الرياضية ومطابقة الحلول لتعلم توقع الحلول الممكنة لمهام مثل حل المعادلات التكاملية. هذا الأسلوب مبني على إطار عمل (Framework) جديد مشابه للمستخدم في الترجمة الآلية العصبية (Neural Machine Translation) حيث تكون التعبيرات الرياضية مقدمة كلغة والحلول تتم معاملتها كعملية ترجمة. وهكذا، بدل أن يخرج النموذج ترجمة، المخرج يكون حل المسألة. بهذه الطريقة، يدعي الباحثون أن الشبكات العصبية العميقة ليست جيدة فقط في المنطق الرمزي (Symbolic Reasoning) بل أيضاً في مهام أكثر تنوعاً.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*P_JtxoC8pYkXuXCp3e3QeQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
المعادلات المغذاة كمدخلات والحلول المقابلة الذي أخرجها النموذج&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/&quot;&gt;&lt;em&gt;المصدر&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;الإبداع-والمجتمع-&quot;&gt;الإبداع والمجتمع 🎨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;الذكاء الاصطناعي للاكتشافات العلمية&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Mattew Hutson قدم &lt;a href=&quot;https://www.sciencemag.org/news/2020/02/models-galaxies-atoms-simple-ai-shortcuts-speed-simulations-billions-times&quot;&gt;&lt;strong&gt;تقريراً&lt;/strong&gt;&lt;/a&gt; عن كيف أن الذكاء الاصطناعي (A.I.) يمكن استخدامه لإنتاج محاكيات ذات استخدام مهم في نمذجة ظواهر طبيعية معقدة، التي بدورها أدت إلى أنواع أخرى من الاكتشافات العلمية. التغيير في بناء هذه المحاكيات هو أنهم غالباً ما يحتاجون لبيانات على نطاق واسع (Large-Scale Data) واستكشاف معاملات (Parameter Exploration) مكثف. &lt;a href=&quot;https://arxiv.org/abs/2001.08055&quot;&gt;&lt;strong&gt;ورقة بحثية حديثة&lt;/strong&gt;&lt;/a&gt; اقترحت DENSE، أسلوب مبني على بحث الهندسة العصبية (Neural Architecture Search) لبناء محاكيات دقيقة مع الاعتماد على قدر محدود من بيانات التدريب. جربوها بتشغيل محاكاة لحالات مثل: الفيزياء الفلكية، علم المناخ، طاقة الاندماج (Fusion)، وغيرها.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;تحسين ترجمة صورة-إلى-رسمة (Image-to-illustration Translation)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
GANILLA هو أسلوب يقترح استخدام شبكات الخصومة المولِدة (Generative Adversarial Networks - GANs) لتطوير تحويل كل من النمط الفني (Style) والمحتوى في عملتي &lt;a href=&quot;https://paperswithcode.com/task/image-to-image-translation&quot;&gt;&lt;strong&gt;ترجمة صورة-إلى-صورة&lt;/strong&gt;&lt;/a&gt; غير مترابطتين. بالتحديد اقترح نموذج لل”صورة-إلى-رسمة” (مع شبكة توليد محسنة) وقيمها بناءً على إطار عمل (Framework) جديد للتقييم الكمي (Quantitative Evaluation) الذي يأخذ بعين الاعتبار كلاً من النمط الفني والمحتوى. ميزة عمله هي شبكة التوليد المقترحة التي تملك موازنة بين النمط الفني والمحتوى فشلت النماذج السابقة في تحقيقها. الشفرة بالإضافة إلى نماذج مدربة مسبقاً تم نشرها. اقرأ كامل الورقة (باللغة الإنجليزية) &lt;a href=&quot;https://arxiv.org/abs/2002.05638&quot;&gt;&lt;strong&gt;هنا&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*l_B4vfaHVkXDwzM7SldiqQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;الشكل 1: مثال على مخرجات كل من CycleGAN (3) وDualGAN (4) وطريقتنا (GANILLA) باستخدام أنماط فنية مختلفة. CycleGAN و GANILLAيولدان صور بأسلوب الرسم اليدوي، لكن DualGAN يفشل في تحويل النمط الفني. وعلى الصعيد الأخر فشل CycleGAN في الحفاظ على محتوى الصور المدخلة، على سبيل المثال وضع وجوه حيوانات في الهواء بشكل عشوائي. طريقتنا تحافظ على المحتوى وتغيير النمط في نفس الوقت.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Andrew Ng يتحدث عن الاهتمام بالتعليم الذاتي الإشراف (Self-Supervised Learning)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew Ng، مؤسس deeplearning.ai، شارك في بودكاست الذكاء الاصطناعي &lt;a href=&quot;https://www.youtube.com/watch?v=0jspaMLxBig&quot;&gt;&lt;strong&gt;ليتحدث&lt;/strong&gt;&lt;/a&gt; عن مواضيع من ضمنها بداياته مع تعلم الآلة، مستقبل الذكاء الاصطناعي وتدريس الذكاء الاصطناعي، اقتراحات لاستخدام صحيح لتعلم الآلة، أهدافه الشخصية وتكنيكيات تعلم الآلة المهمة في القرن الحالي 2020.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew يشرح لماذا هو متحمس جداً بشأن تعلم الإشراف الذاتي الممثل (Self-Supervised Representation Learning). &lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html&quot;&gt;&lt;strong&gt;التعليم الذاتي الإشراف&lt;/strong&gt;&lt;/a&gt; يتضمن صياغة مشكلة تعلم تهدف إلى الحصول على رقابة من البيانات نفسها لاستخدام قدر كبير من البيانات الغير مصنفة (Unlabeled) وهي شائعة أكثر من البيانات الواضحة المصنفة (Labeled). التمثيلات (Representations) ذات أهمية في مقابل أداء المهمة ويمكن استخدامها لحل مهام باتجاه التيار (Downstream) مشابهة لما يستخدم في نماذج اللغات مثل &lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#bert&quot;&gt;&lt;strong&gt;BERT&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
هناك أيضاً الكثير من الاهتمام لاستخدام التعليم الذاتي الإشراف لتعلم تمثيلات مرئية معممة التي ستجعل النماذج أكثر دقة في الظروف المنخفضة الدقة. مثلاً، طريقة جديدة تسمى &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;&lt;strong&gt;SimCLR&lt;/strong&gt;&lt;/a&gt; (كتبت بإشرافGeoffrey Hinton ) تقدم إطار عمل (Framework) للتعلم الذاتي الإشراف المختلف (Contrastive Self-Supervised Learning) لتمثيلات مرئية لتطوير نتائج تصنيف الصور في ظروف مختلفة مثل تحويل التعلم والتعلم شبه المراقب (Semi-Supervised Learning).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*8zLzHFCyM3goc9y7KApHfg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;الشكل 1: مقياس Top-1 من مكتبة ImageNet للمصنِفات الخطية المدربة على تمثيلات تم تعلمها بطرائق مختلفة خاضعة للإشراف الذاتي (مدربة مسبقاً على ImageNet). يشير الصليب الرمادي إلى ResNet-50 الخاضع للإشراف. طريقتنا SimCLR، موضحة بالخط العريض.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
على محور X مقياس Top-1 السابق ذكره، وعلى محور Y عدد المعاملات بالملايين.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;&lt;em&gt;المصدر&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;أدوات-ومجموعات-بيانات-️&quot;&gt;أدوات ومجموعات بيانات ⚙️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;مكتبات JAX&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/google/jax&quot;&gt;&lt;strong&gt;JAX&lt;/strong&gt;&lt;/a&gt; هي مكتبة (Library) جديدة تدمج NumPy والاشتقاق الآلي (Automatic Differentiation) لإجراء أبحاث تعلم آلة عالية الأداء. لتبسيط خطوط الأنابيب (Pipelines) لبناء شبكات عصبية باستخدام JAX، DeepMind أصدرت &lt;a href=&quot;https://github.com/deepmind/dm-haiku&quot;&gt;&lt;strong&gt;Haiku&lt;/strong&gt;&lt;/a&gt; و&lt;a href=&quot;https://github.com/deepmind/rlax&quot;&gt;&lt;strong&gt;RLax&lt;/strong&gt;&lt;/a&gt;.. RLax يبسط تنفيذ وكلاء التعليم المعزز (Reinforcement Learning Agents) وHaiku يبسط بناء الشبكات العصبية باستخدام نماذج برمجة كائنية التوجه (Object-Oriented) مألوفة.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;أداة لمعالجة بيانات ويكيبيديا&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/epfl-lts2/sparkwiki&quot;&gt;&lt;strong&gt;Sparkwiki&lt;/strong&gt;&lt;/a&gt; هي أداة لمعالجة بيانات ويكيبيديا. هذا الإصدار هو جزء من جهود كثيرة لتفعيل بحوث تحليلية سلوكية مثيرة للاهتمام مثل &lt;a href=&quot;https://arxiv.org/abs/2002.06885&quot;&gt;&lt;strong&gt;التقاط الاتجاهات الرائجة وانحيازيات اللغات من إصدارات ويكيبديا بلغاتها المختلفة&lt;/strong&gt;&lt;/a&gt;. المؤلف اكتشف استقلال اللغات، سلوك مستخدمي ويكيبيديا في التصفح يظهر أنهم غالباً ما يشاركون اهتمامات مشتركة للتصنيفات مثل الأفلام، الأغاني، والرياضات، لكن الفروق تتضح في الأحداث المحلية والخصوصيات الثقافية.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*K7N9KbQlbuqowUeePjLtdw.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;مرمزات Rust (Rust Tokenizers)، قاعدة DistilBERT المغلفة (DistilBERT Base Cased)، بطاقات النماذج (Model Cards)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/huggingface/transformers/releases/tag/v2.5.0&quot;&gt;&lt;strong&gt;إطلاق محولات&lt;/strong&gt;&lt;/a&gt; (Transformers) جديد من قبل Hugging Face يتضمن دمج مكتبة الترميز السريعة خاصتهم مما يهدف إلى تسريع نماذج مثل BERT، RoBERTa، GPT2، وغيرها من النماذج المجتمعية الصنع.&lt;/p&gt;

&lt;h1 id=&quot;الأخلاق-في-الذكاء-الاصطناعي-&quot;&gt;الأخلاق في الذكاء الاصطناعي 🚨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;الاعتبارات الأخلاقية لنماذج معالجة اللغات الطبيعية (NLP) ونماذج تعلم الآلة (ML)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
في &lt;a href=&quot;https://soundcloud.com/nlp-highlights/106-ethical-considerations-in-nlp-research-emily-bender&quot;&gt;&lt;strong&gt;حلقة&lt;/strong&gt;&lt;/a&gt; جديدة من بودكاست &lt;a href=&quot;https://soundcloud.com/nlp-highlights&quot;&gt;&lt;strong&gt;NLP Highlights&lt;/strong&gt;&lt;/a&gt;، تحدثت Emily Bende والمقدمون حول بعض الاعتبارات الأخلاقية عند تطوير نماذج معالجة اللغات الطبيعية والتقنيات في سياق كل من الأكاديميا والاستخدام الفعلي على أرض الواقع. كان من المواضيع في النقاش: الاعتبارات الأخلاقية عند تصميم مهام معالجة اللغات الطبيعية، أساليب جمع البيانات، ونشر النتائج في النهاية.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
بالإضافة لكل الاعتبارات في الأعلى، قلق دائم النقاش في مجتمع الذكاء الاصطناعي هو التركيز المفرط على تحسين مقياس، وهذا يتعارض مع أساس ما يحاول الذكاء الاصطناعي تحقيقه. Rachel Thomas و David Uminsky ناقشا متى يمكن أن يصبح هذا خاطئاً عبر &lt;a href=&quot;https://arxiv.org/abs/2002.08512&quot;&gt;&lt;strong&gt;تحليل دقيق&lt;/strong&gt;&lt;/a&gt; لحالات استخدام مختلفة. اقترحا أيضاً إطار عمل (Framework) بسيطاً لتخفيف المشكلة ويتضمن استخدام ودمج عدة مقاييس، متبوعاً بإشراك المتضررين من التقنية بشكل مباشر.&lt;/p&gt;

&lt;h1 id=&quot;المقالات-والتدوينات-️&quot;&gt;المقالات والتدوينات ✍️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;GPT-2 المشروح&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Aman Arora نشر مؤخراً في تدوينة استثنائية بعنوان “&lt;a href=&quot;https://amaarora.github.io/2020/02/18/annotatedGPT2.html&quot;&gt;&lt;strong&gt;The Annotated GPT-2&lt;/strong&gt;&lt;/a&gt;” (بمعنى GPT-2 المشروح) يشرح فيها تفاصيل عمل النموذج المسمى GPT-2 المبني على محول (Transformer). أسلوبه مستوحى من &lt;a href=&quot;https://nlp.seas.harvard.edu/2018/04/03/attention.html&quot;&gt;The Annotated Transformer&lt;/a&gt; (بمعنى المحول المشروح) الذي أخذ مسعى الحاشية والملاحظات لشرح أجزاء مهمة من النموذج عبر الشفرة (“الكود”) وشروحات سهلة المتابعة. بذل Aman جهداً عظيماً ليعيد تنفيذ GPT-2 الخاص ب OpenAIباستخدام PyTorch ومكتبة المحولات (Transformers Library) الخاصة ب Hugging Face. إنه عمل عبقري!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*oRFMJTEojyQ-uocVES5GYA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
محول فك التشفير داخل GPT-2&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
لإعادة استخدام المصطلحات المستخدمة لوصف المحول، الانتباه (Attention) هو دالة من الاستعلام (Q) ومجموعة المفتاح (K) وأزواج القيمة (V). للتعامل مع المتتابعات الأطول، نقوم بتعديل الاهتمام الذاتي متعدد الرؤوس للمحول ليقلل من استخدام الذاكرة عن طريق الحد من المنتج النقطي بين Q وK في معادلة “الانتباه بمعلومية الاستعلام والمفتاح والقيمة” الظاهرة في الرسم.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://amaarora.github.io/2020/02/18/annotatedGPT2.html&quot;&gt;&lt;em&gt;المصدر&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;ما وراء BERT؟&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1&quot;&gt;&lt;strong&gt;مقال&lt;/strong&gt;&lt;/a&gt; مثير للاهتمام من كتابة Sergi Castella يتساءل فيه عن أسرار BERT. كان من المواضيع الأساسية: تطوير المقاييس، كيف مكنت مكتبة محولات Hugging Face البحث العلمي، مجموعات بيانات مثيرة تستحق النظر، تشريح نماذج، والمزيد.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;عملية ضغط المصفوفات (Matrix Compression Operator)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
مدونة TensorFlow نشرت &lt;a href=&quot;https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&quot;&gt;&lt;strong&gt;تدوينة&lt;/strong&gt;&lt;/a&gt; تشرح تكنيكات والأهمية وراء ضغط المصفوفات في نموذج شبكة عصبية عميقة. ضغط المصفوفات يمكنه أن يساعد لبناء نماذج صغيرة أكثر كفاءة التي يمكن أن تدمج في أجهزة صغيرة مثل الهواتف والمساعدات المنزلية (Home Assistants). التركيز على ضغط النماذج عبر طرق مثل التقريب المنخفض الرتبة (Low-Rank Approximation) والتكميم (Quantization) يعنى أننا لا نحتاج إلى أن نضر بجودة النموذج.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*fpAdJvBIf4SKxF3gTIpe_g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
الضغط كتحول: ربط طبقة إلى طبقتين.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
على اليسار الشبكة الأصلية، وعلى اليمين الشبكة المتحولة.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&quot;&gt;&lt;em&gt;المصدر&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;التعليم-&quot;&gt;التعليم 🎓&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;أساسيات معالجة اللغات الطبيعية (NLP)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
أنا متحمس لنشر مسودة من الفصل الأول من سلسلتي الجديدة المسماة &lt;a href=&quot;https://medium.com/dair-ai/fundamentals-of-nlp-chapter-1-tokenization-lemmatization-stemming-and-sentence-segmentation-b362c5d07684&quot;&gt;&lt;strong&gt;Fundamentals of NLP&lt;/strong&gt;&lt;/a&gt;(أساسيات معالجة اللغات الطبيعية). إنها تعلم مفاهيم معالجة اللغات الطبيعية بدئاً من الأساسيات في أثناء ما تشارك أفضل الممارسات، مراجع مهمة، أخطاء شائعة لتجنبها، وما ينتظرنا في معالجة اللغات العصبية. &lt;a href=&quot;https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ&quot;&gt;&lt;strong&gt;دفتر&lt;/strong&gt;&lt;/a&gt; كولاب (Google Colab) مرفق والمشروع سيتم تحديثه &lt;a href=&quot;https://github.com/dair-ai/nlp_fundamentals&quot;&gt;&lt;strong&gt;هنا&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*mS5NcoJ_c8hYTjiJsuu_8g.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;[عبر الانترنت] مراجعة/مناقشة: الجزء 1 جلسة قراءة الأسس الرياضية&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://mml-book.github.io/&quot;&gt;&lt;strong&gt;Mathematics For Machine Learning&lt;/strong&gt;&lt;/a&gt; تستضيف نقاش على الإنترنت لمراجعة الفصول التي تمت تغطيتها مؤخراً في جلساتهم للدراسة عبر الإنترنت. المجموعة درست سابقاً فصولاً بناءً على كتاب يدعى Mathematics For Machine Learning كتبه Marc Peter Deisenroth وA Aldo Faisal وCheng Soon Ong. &lt;a href=&quot;https://www.meetup.com/Machine-Learning-Tokyo/events/268817313/&quot;&gt;&lt;em&gt;الحدث&lt;/em&gt;&lt;/a&gt; مخطط ليقام في الثامن من مارس 2020.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;اقتراحات الكتب&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
في قطعة سابقة ناقشنا أهمية ضغط المصفوفات لبناء نماذج تعلم آلة صغيرة. إذا كنت مهتماً لتعلم المزيد عن كيفية بناء شبكات عصبية عميقة أصغر للأنظمة المتضمنة (Embedded Systems) خذ نظرة على هذا الكتاب الرائع باسم &lt;a href=&quot;https://tinymlbook.com/?linkId=82595412&quot;&gt;&lt;strong&gt;TinyML&lt;/strong&gt;&lt;/a&gt; (بمعنى: تعلم آلة صغير) من تأليف Pete Warden و Daniel Situnayake.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*omOa3aw2bfMzX2Qm.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.amazon.com/TinyML-Learning-TensorFlow-Ultra-Low-Micro-Controllers/dp/1492052043&quot;&gt;&lt;em&gt;المصدر&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
كتاب آخر مثير للاهتمام للمتابعة هو العنوان القادم “&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;&lt;strong&gt;Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD&lt;/strong&gt;&lt;/a&gt;” (بمعنى: التعلم العميق للمبرمجين ب fastai و PyTorch: تطبيقات الذكاء الاصطناعي بدون شهادة دكتوراة) من تأليف Jeremy Howard و Sylvain Gugger. الكتاب يهدف إلى تقديم البناء الرياضي الضروري لبناء وتدريب النماذج لتتعامل مع مهام في مجالات الرؤية الحاسوبية (Computer Vision) ومعالجة اللغات الطبيعية (NLP).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*i2OmtWOncatOYsZv.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;&lt;em&gt;المصدر&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;إضافات-جديرة-بالذكر-️&quot;&gt;إضافات جديرة بالذكر ⭐️&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;
يمكنك الوصول إلى الإصدار السابق من قائمة معالجة اللغات الطبيعية (NLP) البريدية من &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-pytorch3d-deepspeed-turing-nlg-question-answering-benchmarks-hydra-sparse-322f018ee096&quot;&gt;&lt;strong&gt;هنا&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1909.06576&quot;&gt;&lt;strong&gt;Torchmeta&lt;/strong&gt;&lt;/a&gt; هي مكتبة (Library) تتيح سهولة الاستخدام لمحملات البيانات (Data Loaders) ذات الصلة لأبحاث التعلم الفوقي (Meta-Learning). كتبها Tristan Deleu.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Manuel Tonneau كتبت &lt;a href=&quot;https://creatext.ai/blog-posts/machine-text-writing-gpt2-beam-search?utm_medium=newsletter&quot;&gt;&lt;strong&gt;مقالاً&lt;/strong&gt;&lt;/a&gt; يقدم نظرة أقرب لبعض الآلات المستخدمة في نمذجة اللغة (Language Modeling). بعض المواضيع تضمنت البحث الجشع (Greedy Search) والبحث الشعاعي (Beam Search) وأخذ العينات النووي (Nucleus Sampling).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
MIT &lt;a href=&quot;http://introtodeeplearning.com/&quot;&gt;&lt;strong&gt;نشرت&lt;/strong&gt;&lt;/a&gt; المنهج والجدول الدراسي الكاملين لكورس “Introduction to Deep Learning” (مقدمة إلى التعلم العميق)، بما في ذلك مقاطع الفيديو للمحاضرات التي ألقيت بالفعل. يهدفون إلى نشر مقاطع فيديو وشرائح العرض للمحاضرات أسبوعياً.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
تعلم كيف تدرب نموذجاً للتعرف على الكيانات المسماة (Named Entity Recognition (NER)) باستخدام أسلوب مبنى على المحول (Transformer) في أقل من &lt;a href=&quot;https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py&quot;&gt;&lt;strong&gt;300 سطر من الشفرة “الكود”&lt;/strong&gt;&lt;/a&gt;. يمكنك العثور على جوجل كولاب المصاحب &lt;a href=&quot;https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn&quot;&gt;&lt;strong&gt;هنا&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;
إذا كان لديك أي مجموعات بيانات، مشاريع، تدوينات، شروحات، أو أوراق بحثية تتمنى أن يتم نشرهم في العدد القادم من قائمة معالجة اللغات الطبيعية (NLP) البريدية، لا تترد في التواصل معي عبر البريد الالكتروني ellfae@gmail.com أو&lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;&lt;strong&gt;الرسائل الخاصة على تويتر&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;strong&gt;اشترك&lt;/strong&gt;&lt;/a&gt; &lt;em&gt;🔖 في قائمة NLP البريدية لتصلك الأعداد القادمة مباشرة على صندوق بريدك.&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter.AR._The_Annotated_GPT-2_And_More/&quot;&gt;الإصدار الخامس من قائمة معالجة اللغات الطبيعية (NLP) البريدية: GPT-2 المشروح، فهم التقطير-الذاتي، Haiku، GANILLA، Sparkwiki، الأخلاق في علم معالجة اللغات الطبيعية، Torchmeta، والمزيد ...&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on February 28, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP Newsletter #5: The Annotated GPT-2, Understanding self-distillation, Haiku, GANILLA, Sparkwiki, Ethics in NLP, Torchmeta,…]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP_Newsletter_The_Annotated_GPT-2,_Understanding/" />
  <id>https://dair.ai/NLP_Newsletter_The_Annotated_GPT-2,_Understanding</id>
  <published>2020-02-23T00:00:00-06:00</published>
  <updated>2020-02-23T00:00:00-06:00</updated>
  <author>
    <name>Elvis Saravia</name>
    <uri>https://dair.ai</uri>
    <email>ellfae@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*YIhZsPaiBFkRMMWo5FAhGw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
First of all, I can’t thank ❤️ all of you enough for the incredible support and encouragement to continue with the NLP Newsletter. This effort requires tedious research and editing which I find to be both rewarding and also useful to provide you the best content. Hope you are enjoying them because I do. 😉&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Subscribe&lt;/em&gt;&lt;/a&gt; &lt;em&gt;🔖 to the NLP Newsletter to receive future issues in your inbox.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;publications-&quot;&gt;Publications 📙&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;A theoretical understanding of self-distillation&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In the context of deep learning, &lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot;&gt;&lt;em&gt;self-distillation&lt;/em&gt;&lt;/a&gt; is the process of transferring knowledge from one architecture to another identical architecture. Predictions of the original model are fed as target values to the other model while training. Besides having desirable properties, such as reducing model size, empirical results demonstrate that this approach works well on held-out datasets. A group of researchers recently published a paper that provides a theoretical analysis with the focus to better understand what is happening in this knowledge distillation process and why it is effective. Results show that a few rounds of self-distillation amplify regularization (by &lt;a href=&quot;https://twitter.com/TheGradient/status/1228132843630387201?s=20&quot;&gt;&lt;em&gt;progressively limiting the number of basis functions that represent the solution&lt;/em&gt;&lt;/a&gt;) which tends to reduce over-fitting. (Read the full paper &lt;a href=&quot;https://arxiv.org/abs/2002.05715&quot;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*nWYO71awfooL4MrGK-tFww.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.05715&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;The 2010s: Our Decade of Deep Learning / Outlook on the 2020s&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;http://people.idsia.ch/~juergen/&quot;&gt;Jürgen Schmidhuber,&lt;/a&gt; a pioneer in Artificial Intelligence, recently posted a &lt;a href=&quot;http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html&quot;&gt;&lt;strong&gt;new blog post&lt;/strong&gt;&lt;/a&gt; focusing on providing a historical overview of deep learning since 2010. Some topics include LSTMs, feedforward neural networks, GANs, deep reinforcement learning, meta-learning, world models, distilling NNs, attention learning, etc. The article concludes with an outlook on the 2020s encouraging attention to pressing issues such as privacy and data markets.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Using neural networks to solve advanced mathematics equations&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Facebook AI researchers published a &lt;a href=&quot;https://arxiv.org/abs/1912.01412&quot;&gt;&lt;strong&gt;paper&lt;/strong&gt;&lt;/a&gt; that claims to propose a model trained on math problems and matching solutions to learn to predict possible solutions for tasks such as solving integration problems. The approach is based on a novel framework similar to what’s used in &lt;em&gt;neural machine translation&lt;/em&gt; where mathematical expressions are represented as a kind of language and the solutions being treated as the translation problem. Thus, instead of the model outputting a translation, the output is the solution itself. With this, the researchers claim that deep neural networks are not only good at symbolic reasoning but also for more diverse tasks.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*P_JtxoC8pYkXuXCp3e3QeQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Equations fed as input together with the corresponding solution outputted by the model —&lt;/em&gt; &lt;a href=&quot;https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;creativity-and-society-&quot;&gt;Creativity and Society 🎨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;AI for scientific discovery&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Mattew Hutson &lt;a href=&quot;https://www.sciencemag.org/news/2020/02/models-galaxies-atoms-simple-ai-shortcuts-speed-simulations-billions-times&quot;&gt;&lt;strong&gt;reports&lt;/strong&gt;&lt;/a&gt; how artificial intelligence (AI) can be used to produce emulators that have an important use in modeling complex natural phenomena that could, in turn, lead to different types of &lt;em&gt;scientific discovery&lt;/em&gt;. The change with building these emulators is that they often require large-scale data and extensive parameter exploration. A recent &lt;a href=&quot;https://arxiv.org/abs/2001.08055&quot;&gt;&lt;strong&gt;paper&lt;/strong&gt;&lt;/a&gt; proposes DENSE an approach based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Neural_architecture_search&quot;&gt;&lt;em&gt;neural architecture search&lt;/em&gt;&lt;/a&gt; to build accurate emulators while only relying on a limited amount of training data. They tested it by running simulations for cases including astrophysics, climate science, and fusion energy, among others.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Improving image-to-illustration translation&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
GANILLA is an approach that proposes the use of GANs to improve the transfer of both style and content in the unpaired &lt;a href=&quot;https://paperswithcode.com/task/image-to-image-translation&quot;&gt;&lt;em&gt;image-to-image translation&lt;/em&gt;&lt;/a&gt; task. In particular, a model for image-to-illustration is proposed (with an improved generator network) and evaluated based on a new framework for quantitative evaluation that considers both content and style. The novelty of the work is in the proposed generator network that considers a balance between style and content which previous models fail to achieve. Code and pretrained models are made &lt;a href=&quot;https://github.com/giddyyupp/ganilla&quot;&gt;available&lt;/a&gt;. Read the full paper &lt;a href=&quot;https://arxiv.org/abs/2002.05638&quot;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*l_B4vfaHVkXDwzM7SldiqQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Andrew Ng talks about interest in self-supervised learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew Ng, the founder of deeplearning.ai, joins the Artificial Intelligence podcast to &lt;a href=&quot;https://www.youtube.com/watch?v=0jspaMLxBig&quot;&gt;&lt;strong&gt;talk&lt;/strong&gt;&lt;/a&gt; about topics including his early days doing ML, the future of AI and AI education, recommendations for proper use of ML, his personal goals and what ML techniques to pay attention to in the 2020s.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew explained why he is very excited about &lt;em&gt;self-supervised representation learning.&lt;/em&gt; &lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html&quot;&gt;&lt;strong&gt;Self-supervised learning&lt;/strong&gt;&lt;/a&gt; involves framing a learning problem that aims to obtain supervision from the data itself to make use of large amounts of unlabeled data which is more common than clean labeled data. The representations, as opposed to the performance of the task, are important and can be used to address downstream tasks similar to what’s being used in language models such as &lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#bert&quot;&gt;BERT&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
There is also a lot of interest to use self-supervised learning for learning generalized visual representations that make models more accurate in low-resource settings. For instance, a recent method called &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;&lt;strong&gt;SimCLR&lt;/strong&gt;&lt;/a&gt; (led by Geoffrey Hinton) proposes a framework for &lt;em&gt;contrastive self-supervised learning&lt;/em&gt; of visual representations for improving image classification results in different settings such as transfer learning and semi-supervised learning.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*8zLzHFCyM3goc9y7KApHfg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;tools-and-datasets-️&quot;&gt;Tools and Datasets ⚙️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;JAX libraries&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/google/jax&quot;&gt;JAX&lt;/a&gt; is a new library that combines NumPy and automatic differentiation for conducting high-performance ML research. To simplify pipelines for building neural networks using JAX, DeepMind released &lt;a href=&quot;https://github.com/deepmind/dm-haiku&quot;&gt;&lt;strong&gt;Haiku&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&quot;https://github.com/deepmind/rlax&quot;&gt;&lt;strong&gt;RLax&lt;/strong&gt;&lt;/a&gt;. RLax simplifies the implementation of reinforcement learning agents and Haiku simplifies the building of neural networks using &lt;em&gt;familiar object-oriented programming models.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;A tool for processing Wikipedia data&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/epfl-lts2/sparkwiki&quot;&gt;&lt;strong&gt;Sparkwiki&lt;/strong&gt;&lt;/a&gt; is a tool to process Wikipedia data. This release is part of many efforts to enable interesting behavioral analytics research such as &lt;a href=&quot;https://arxiv.org/abs/2002.06885&quot;&gt;capturing trends and language biases across different language editions of Wikipedia&lt;/a&gt;. The authors discovered that independent of language, the browsing behavior of Wikipedia users shows that they tend to share common interests for categories such as movies, music, and sports but differences become more apparent with local events and cultural particularities.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*K7N9KbQlbuqowUeePjLtdw.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Rust Tokenizers, DistilBERT base cased, Model cards&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
A new &lt;a href=&quot;https://github.com/huggingface/transformers/releases/tag/v2.5.0&quot;&gt;&lt;strong&gt;Transformers release&lt;/strong&gt;&lt;/a&gt; by Hugging Face includes the integration of their fast tokenizer library which aims to speed up models like BERT, RoBERTa, GPT2, and other community-built models.&lt;/p&gt;

&lt;h1 id=&quot;ethics-in-ai-&quot;&gt;Ethics in AI 🚨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Ethical considerations for NLP and ML models&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In a new &lt;a href=&quot;https://soundcloud.com/nlp-highlights/106-ethical-considerations-in-nlp-research-emily-bender&quot;&gt;&lt;strong&gt;episode&lt;/strong&gt;&lt;/a&gt; of the &lt;a href=&quot;https://soundcloud.com/nlp-highlights&quot;&gt;NLP Highlights,&lt;/a&gt; Emily Bender and hosts talk about some ethical considerations when developing NLP models and technologies in the context of both academia and real-world usage. Some of the topics in the discussion include ethical considerations when designing NLP tasks, data collection approaches, and eventually publishing results.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In addition to all the considerations above, a concern that is always discussed in the AI community is focusing too much on optimizing a metric, which goes against the foundations of what AI aims to achieve. Rachel Thomas and David Uminsky discuss where this can go wrong through a &lt;a href=&quot;https://arxiv.org/abs/2002.08512&quot;&gt;&lt;strong&gt;thorough analysis&lt;/strong&gt;&lt;/a&gt; of different use cases. They also propose a simple framework for mitigating the problem which involves the use and combination of multiple metrics, followed by the involvement of those affected directly by the technology.&lt;/p&gt;

&lt;h1 id=&quot;articles-and-blog-posts-️&quot;&gt;Articles and Blog posts ✍️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;The Annotated GPT-2&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Aman Arora recently published an exceptional blog post appropriately titled “&lt;a href=&quot;https://amaarora.github.io/2020/02/18/annotatedGPT2.html&quot;&gt;&lt;strong&gt;The Annotated GPT-2&lt;/strong&gt;&lt;/a&gt;” explaining the inner workings of the Transformer based model called GPT-2. His approach was inspired by &lt;a href=&quot;https://nlp.seas.harvard.edu/2018/04/03/attention.html&quot;&gt;The Annotated Transformer&lt;/a&gt; that took an annotation approach to explain the important parts of the model through code and easy-to-follow explanations. Aman went through a great effort to re-implement OpenAI’s GPT-2 using PyTorch and the Transformers library by Hugging Face. It’s brilliant work!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*oRFMJTEojyQ-uocVES5GYA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amaarora.github.io/2020/02/18/annotatedGPT2.html&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Beyond BERT?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Interesting &lt;a href=&quot;https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1&quot;&gt;&lt;strong&gt;take&lt;/strong&gt;&lt;/a&gt; by Sergi Castella on what lies beyond BERT. The main topics include improving metrics, how Hugging Face’s Transformers library empowers research, interesting datasets to look at, unpacking models, etc.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Matrix Compression Operator&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
The TensorFlow Blog published a &lt;a href=&quot;https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&quot;&gt;&lt;strong&gt;blog post&lt;/strong&gt;&lt;/a&gt; explaining the techniques and importance behind compressing matrices in a deep neural network model. &lt;em&gt;Matrix compression&lt;/em&gt; can help to build more efficient tiny models that can be incorporated into smaller devices such as phones and home assistants. Focusing on the compression of the models via methods like &lt;em&gt;low-rank-approximation&lt;/em&gt; and &lt;em&gt;quantization&lt;/em&gt; means that we don’t need to compromise the quality of the model.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*fpAdJvBIf4SKxF3gTIpe_g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;education-&quot;&gt;Education 🎓&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Fundamentals of NLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
I am excited to release a draft of Chapter 1 of my new series called &lt;a href=&quot;https://medium.com/dair-ai/fundamentals-of-nlp-chapter-1-tokenization-lemmatization-stemming-and-sentence-segmentation-b362c5d07684&quot;&gt;&lt;strong&gt;Fundamentals of NLP&lt;/strong&gt;&lt;/a&gt;. It teaches NLP concepts starting from the basics while sharing best practices, important references, common mistakes to avoid, and what lies ahead in NLP. A Colab &lt;a href=&quot;https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ&quot;&gt;notebook&lt;/a&gt; is included and the project will be maintained &lt;a href=&quot;https://github.com/dair-ai/nlp_fundamentals&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*mS5NcoJ_c8hYTjiJsuu_8g.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;[Online] Review/Discussion: Part I Mathematical Foundations Reading Session&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Machine Learning Tokyo is hosting a remote online discussion reviewing chapters that were covered in their recent online study sessions. The group had previously studied chapters based on the book called &lt;a href=&quot;https://mml-book.github.io/&quot;&gt;Mathematics For Machine Learning&lt;/a&gt; written by Marc Peter Deisenroth, A Aldo Faisal, and Cheng Soon Ong. The &lt;a href=&quot;https://www.meetup.com/Machine-Learning-Tokyo/events/268817313/&quot;&gt;&lt;strong&gt;event&lt;/strong&gt;&lt;/a&gt; is scheduled for March 8, 2020.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Book recommendations&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In a previous segment, we discussed the importance of matrix compression for building tiny ML models. If you are interested to learn more about how to build smaller deep neural networks for embedded systems check out this great book called &lt;a href=&quot;https://tinymlbook.com/?linkId=82595412&quot;&gt;&lt;strong&gt;TinyML&lt;/strong&gt;&lt;/a&gt; by Pete Warden and Daniel Situnayake.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*omOa3aw2bfMzX2Qm.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.amazon.com/TinyML-Learning-TensorFlow-Ultra-Low-Micro-Controllers/dp/1492052043&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Another interesting book to keep an eye on is the upcoming title &lt;strong&gt;“&lt;/strong&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;&lt;strong&gt;Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;”&lt;/strong&gt; by Jeremy Howard and Sylvain Gugger. The book aims to provide the necessary mathematical foundation to build and train models to approach tasks in the areas of computer vision and NLP.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*i2OmtWOncatOYsZv.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;&lt;em&gt;source&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;noteworthy-mentions-️&quot;&gt;Noteworthy Mentions ⭐️&lt;/h1&gt;

&lt;p&gt;You can access the previous issue of the NLP Newsletter &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-pytorch3d-deepspeed-turing-nlg-question-answering-benchmarks-hydra-sparse-322f018ee096&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1909.06576&quot;&gt;&lt;strong&gt;Torchmeta&lt;/strong&gt;&lt;/a&gt; is a library that allows ease of use of related data loaders for meta-learning research. It was authored by Tristan Deleu.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Manuel Tonneau wrote a &lt;a href=&quot;https://creatext.ai/blog-posts/machine-text-writing-gpt2-beam-search?utm_medium=newsletter&quot;&gt;&lt;strong&gt;piece&lt;/strong&gt;&lt;/a&gt; offering a closer look at some of the machinery involved in language modeling. Some topics include &lt;em&gt;greedy&lt;/em&gt; and &lt;em&gt;beam search&lt;/em&gt; and &lt;em&gt;nucleus sampling&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
MIT &lt;a href=&quot;http://introtodeeplearning.com/&quot;&gt;&lt;strong&gt;releases&lt;/strong&gt;&lt;/a&gt; the full syllabus and course schedule for the course titled “Introduction to Deep Learning”, including videos for lectures already delivered. They aim to release video lectures and slides every week.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Learn how to train a model for named entity recognition (NER) using a Transformer based approach in under &lt;a href=&quot;https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py&quot;&gt;&lt;strong&gt;300 lines of code&lt;/strong&gt;&lt;/a&gt;. You can find the accompanying Google Colab &lt;a href=&quot;https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;If you have any datasets, projects, blog posts, tutorials, or papers that you wish to share in the next iteration of the NLP Newsletter, please free to reach out to me at ellfae@gmail.com or &lt;a href=&quot;https://twitter.com/omarsar0&quot;&gt;&lt;strong&gt;DM on Twitter&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;&lt;em&gt;Subscribe&lt;/em&gt;&lt;/a&gt; &lt;em&gt;🔖 to the NLP Newsletter to receive future issues in your inbox.&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP_Newsletter_The_Annotated_GPT-2,_Understanding/&quot;&gt;NLP Newsletter #5: The Annotated GPT-2, Understanding self-distillation, Haiku, GANILLA, Sparkwiki, Ethics in NLP, Torchmeta,…&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on February 23, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[Boletín informativo NLP #5: GPT-2 Explicado, Entendiendo ‘Self-Distillation’, Haiku, GANILLA, Sparkwiki, Ética en el NLP, Torchmeta,...]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/Bolet%C3%ADn_informativo_NLP_GPT-2_Explicado,_Entendie/" />
  <id>https://dair.ai/Boletín_informativo_NLP_GPT-2_Explicado,_Entendie</id>
  <published>2020-02-23T00:00:00-06:00</published>
  <updated>2020-02-23T00:00:00-06:00</updated>
  <author>
    <name>Nicolas Araque Volk</name>
    <uri>https://dair.ai</uri>
    
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*YIhZsPaiBFkRMMWo5FAhGw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Post Original en Ingles: &lt;a href=&quot;https://dair.ai/NLP_Newsletter_The_Annotated_GPT-2,_Understanding/&quot;&gt;https://dair.ai/NLP_Newsletter_The_Annotated_GPT-2,_Understanding/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Primero que todo, no puedo agradecerles lo suficiente a todos por el increible animo y soporte para continuar con el boletín informativo de NLP. Este esfuerzo requiere una investigación y edición tediosa, la cual encuentro gratificante y útil para proveer el mejor contenido. Espero que lo estén disfrutando tanto como yo.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://dair.ai/newsletter/&quot;&gt;Suscríbete&lt;/a&gt; al Boletín Informativo de NLP para recibir futuras ediciones en tu email. Este boletin lo desarrolla Elvis Saravia, editor de dair.ai&lt;/p&gt;

&lt;h1 id=&quot;publicaciones&quot;&gt;Publicaciones📙&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Un entendimiento teórico del self-distillation&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
En el contexto del Deep Learning, self-distillation es el proceso de transferir conocimiento de una arquitectura a otra arquitectura idéntica. Las predicciones del modelo original son alimentadas como valores objetivo al otro modelo mientras se entrena. Además de tener propiedades deseables (como reducir el tamaño del modelo) resultados empíricos demuestran que esta aproximación trabaja bastante bien en sets de datos del tipo held-out. Un grupo de investigadores recientemente publicó una investigación que provee un análisis teórico con foco en entender mejor qué está pasando en el proceso de destilación del conocimiento y por que es efectivo. Los resultados muestran que unas pocas rondas de self-distillation amplifica la regularización (&lt;a href=&quot;https://twitter.com/TheGradient/status/1228132843630387201?s=20&quot;&gt;limitando progresivamente el número de funciones básicas que representan la solución&lt;/a&gt;) lo que tiende a reducir el over-fitting. (Lee la investigación completa &lt;a href=&quot;https://arxiv.org/abs/2002.05715&quot;&gt;aquí&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*nWYO71awfooL4MrGK-tFww.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;em&gt;Traducción: Figura 1. Ilustración esquemática del proceso de self-distillation por dos iteraciones. Fuente: &lt;a href=&quot;https://arxiv.org/abs/2002.05715&quot;&gt;https://arxiv.org/abs/2002.05715&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Los 2010: La década del Deep Learning / Mirada al 2020&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;http://people.idsia.ch/~juergen/&quot;&gt;Jürgen Schmidhuber,&lt;/a&gt; un pionero en la inteligencia artificial, publicó recientemente un &lt;a href=&quot;http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html&quot;&gt;artículo&lt;/a&gt; titulado “Foco en proveer un resumen histórico del Deep Learning desde el 2010”. Algunos de los temas incluyen LSTMs, redes neuronales feedforward, GANs, DeepRL, Meta-Learning, World Models, distillings NNs, Attention Learning, etc. El artículo concluye con una mirada al 2020, donde llama la atención a temas como la privacidad y los mercados de datos.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Utilizando Redes Neuronales para resolver ecuaciones matemáticas avanzadas.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Investigadores de Facebook AI publicaron una &lt;a href=&quot;https://arxiv.org/abs/1912.01412&quot;&gt;investigación&lt;/a&gt; que dice proponer un modelo entrenado en problemas matemáticos y sus soluciones para aprender a predecir posibles soluciones en problemas cómo resolver integrales matematicas. El acercamiento está basado en un novedoso framework similar al usado en neural machine translation donde una expresión matemática es representada como una especie de lenguaje y la solución es tratada como su traducción. Por esto, en vez de que el resultado sea una traducción, es la solución al problema matematico. Con esto, los investigadores concluyen que las redes neuronales profundas no solo son buenas para el razonamiento simbólico sino también para tareas más diversas.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*P_JtxoC8pYkXuXCp3e3QeQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ecuaciones incluidas como entrada con su correspondiente solución. &lt;a href=&quot;https://ai.facebook.com/blog/using-neural-networks-to-solve-advanced-mathematics-equations/&quot;&gt;Fuente&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;creatividad-y-sociedad-&quot;&gt;Creatividad y Sociedad 🎨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;IA para el descubrimiento científico&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Mattew Hutson &lt;a href=&quot;https://www.sciencemag.org/news/2020/02/models-galaxies-atoms-simple-ai-shortcuts-speed-simulations-billions-times&quot;&gt;reporta&lt;/a&gt; cómo la inteligencia artificial (IA) puede ser usada para producir emuladores con la capacidad de modelar fenómenos naturales complejos. Estos modelos pueden a su vez apuntar a diferentes tipos de descubrimientos científicos. El desafío al construir estos emuladores es que necesitan una gran cantidad de datos y una búsqueda extensiva de parámetros. Una &lt;a href=&quot;https://arxiv.org/abs/2001.08055&quot;&gt;investigación&lt;/a&gt; reciente propone una técnica llamada DENSE, basada en &lt;a href=&quot;https://en.wikipedia.org/wiki/Neural_architecture_search&quot;&gt;neural architecture search&lt;/a&gt; para construir emuladores precisos utilizando una cantidad de datos de entrenamiento limitada. Los investigadores hicieron distintas pruebas construyendo simuladores para caso como astrofísica, ciencia climática, energía de fusión, etc.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Mejorando la traducción imagen-a-ilustración&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
GANILLA es una aproximación que propone el uso de las GANs para mejorar la transferencia de estilo y contenido en la tarea de convertir una imagen en una ilustración. En particular, un modelo para &lt;a href=&quot;https://paperswithcode.com/task/image-to-image-translation&quot;&gt;imagen-a-ilustración&lt;/a&gt; es propuesto (con una red generadora mejorada) y evaluado basado en un nuevo framework para evaluación cuantitativa que considera tanto el contenido como el estilo de la imagen generada. La novedad de esta investigación es en la red generadora propuesta que considera un balance entre estilo y contenido que anteriores intentos fallaron en alcanzar. El código y modelo pre entrenado están disponibles en &lt;a href=&quot;https://github.com/giddyyupp/ganilla&quot;&gt;línea&lt;/a&gt;. &lt;a href=&quot;https://arxiv.org/abs/2002.05638&quot;&gt;Aquí&lt;/a&gt; se puede encontrar la investigación completa.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*l_B4vfaHVkXDwzM7SldiqQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Traducción: Ejemplos de salida de CycleGAN, DualGAN, y nuestro método (GANILLA) utilizando diferentes estilos. CycleGAN y GANILLA generan imágenes en un estilo de ilustración, pero DualGAN falla en transferir el estilo. Sin embargo, CycleGAN falle en preservar el contenido de la imagen original, por ejemplo, aleatoriamente coloca caras de animales en el aire. Nuestro método preserva contenido así como también transfiere el estilo de la imagen original.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Andrew Ng habla sobre su interés en self-supervised learning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew Ng, fundador de &lt;a href=&quot;http://deeplearning.ai/&quot;&gt;deeplearning.ai&lt;/a&gt;, participa en el podcast Artificial Inteligence de Lex Friedman para &lt;a href=&quot;https://www.youtube.com/watch?v=0jspaMLxBig&quot;&gt;hablar&lt;/a&gt; de diferentes temas como sus comienzos en el ML, el futuro del AI y la educación, recomendaciones de uso para ML, sus metas personales y a cuáles técnicas de ML prestar atención en el 2020.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Andrew tambien explico por que está muy emocionado sobre el self-supervised representation learning. &lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html&quot;&gt;Self-supervised learning&lt;/a&gt; se trata sobre enfocar un problema de aprendizaje que intenta obtener la supervisión de los datos en sí mismos para hacer uso de grandes cantidades de datos no clasificados, escenario más común que datos limpios y clasificados. Las representaciones aprendidas, opuesto al rendimiento de la tarea, son importantes y pueden ser usadas en tareas más adelante, similar a lo que está siendo usado en modelos de lenguaje natural como &lt;a href=&quot;https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html#bert&quot;&gt;BERT&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hay mucho interés en usar el self-supervised learning para aprender representaciones visuales generales que hagan al modelo más preciso en situaciones de bajos recursos. Por ejemplo, un nuevo método llamado &lt;a href=&quot;https://arxiv.org/abs/2002.05709&quot;&gt;SimCLR&lt;/a&gt; (Dirigido por Geoffrey Hinton) propone un framework para el &lt;em&gt;constrastive self-supervised learning&lt;/em&gt; de representaciones visuales para mejorar el resultado de la clasificación de imágenes en diferentes contextos como el &lt;em&gt;transfer learning&lt;/em&gt; y el &lt;em&gt;semi-supervised learning&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*8zLzHFCyM3goc9y7KApHfg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Traducción: ImageNet top-1 accuracy de clasificadores lineales entrenados en representaciones aprendidas con diferentes métodos de auto-aprendizaje (pre entrenados en ImageNet). La cruz gris indica un modelo ResNet-50 supervisado. Nuestro método, SimCLR, es mostrado en negrita.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;herramientas-y-set-de-datos-️&quot;&gt;Herramientas y Set de Datos ⚙️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Librerías JAX&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/google/jax&quot;&gt;JAX&lt;/a&gt; es una nueva librería que combina NumPy y diferenciación automática para realizar investigación de alto rendimiento en ML. Para simplificar procesos para construir redes neuronales usando JAX, DeepMind lanzó &lt;a href=&quot;https://github.com/deepmind/dm-haiku&quot;&gt;Haiku&lt;/a&gt; y &lt;a href=&quot;https://github.com/deepmind/rlax&quot;&gt;RLax&lt;/a&gt;. RLAx simplifica la implementación de agentes de reinforcement learning y Haiku simplifica la construcción de redes neuronales utilizando paradigmas familiares de programación orientada a objetos.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Una herramienta para procesar datos de Wikipedia&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/epfl-lts2/sparkwiki&quot;&gt;Spakwiki&lt;/a&gt; es una herramienta para procesar datos de Wikipedia. Este lanzamiento es parte de muchos esfuerzos para permitir análisis interesantes de comportamiento como &lt;a href=&quot;https://arxiv.org/abs/2002.06885&quot;&gt;capturar tendencias y sesgos de lenguaje en ediciones de distinto idioma en Wikipedia&lt;/a&gt;. Los autores descubrieron que independientemente del lenguaje, el comportamiento de búsqueda de los usuarios de Wikipedia es muy parecido en categorías como películas, música y deportes pero que las diferencias se vuelven más aparentes con eventos locales y particularidades de cultura.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*K7N9KbQlbuqowUeePjLtdw.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Tokenizadores, caso DistilBERT y modelos&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Una &lt;a href=&quot;https://github.com/huggingface/transformers/releases/tag/v2.5.0&quot;&gt;nueva entrega de Transformers&lt;/a&gt; de Hugging Face ahora incluye la integración de su rápida librería de tokenización que intenta mejorar el tiempo de ejecución de modelos como BERT, RoBERTa, GPT2, y otro modelos construidos por la comunidad.&lt;/p&gt;

&lt;h1 id=&quot;ética-en-la-inteligencia-artificial-&quot;&gt;Ética en la Inteligencia Artificial 🚨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Consideraciones éticas para modelos de NLP y ML.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
En un nuevo &lt;a href=&quot;https://soundcloud.com/nlp-highlights/106-ethical-considerations-in-nlp-research-emily-bender&quot;&gt;episodio&lt;/a&gt; de &lt;a href=&quot;https://soundcloud.com/nlp-highlights&quot;&gt;NLP Highlights&lt;/a&gt;, Emily Bender habla sobre algunas consideraciones éticas cuando se desarrollan modelos de Procesamiento de Lenguaje Natural y tecnologías tanto en el contexto de la academia como de industria. Algunos de los tópicos discutidos incluyen consideraciones éticas cuando se diseñan tareas, recolección de datos, y eventualmente publicación de resultados.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Adicionalmente a todas las consideraciones mencionadas anteriormente, una preocupación que siempre se discute en la comunidad de AI es enfocarse demasiado en optimizar una métrica, lo que va en contra de los cimientos de lo que el campo trata de alcanzar. Rachel Thomas y David Uminsky discuten cómo esto puede salir mal a través de un &lt;a href=&quot;https://arxiv.org/abs/2002.08512&quot;&gt;análisis&lt;/a&gt; con diferentes casos de uso. Además, proponen un framework simple para mitigar el problema que involucra el uso y combinación de múltiples métricas, seguido del involucramiento directo de los usuarios afectados por la tecnología que se está desarrollando.&lt;/p&gt;

&lt;h1 id=&quot;artículos-y-blogs-️&quot;&gt;Artículos y Blogs ✍️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;GPT-2 Explicado&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Aman Arora recientemente publicó un artículo excepcional titulado “&lt;a href=&quot;https://amaarora.github.io/2020/02/18/annotatedGPT2.html&quot;&gt;&lt;strong&gt;El GPT-2 Explicado&lt;/strong&gt;&lt;/a&gt;” explicando el funcionamiento interno del modelo basado en la técnica del Transformer llamado GPT-2. Su aproximación fue inspirada por el artículo &lt;a href=&quot;https://nlp.seas.harvard.edu/2018/04/03/attention.html&quot;&gt;El Transformer Explicado&lt;/a&gt; que tomó una aproximación a explicar las partes más importantes del modelo siguiendo ejemplos fáciles de seguir y código comentado. Amant realizó un gran esfuerzo para re implementar GPT-2 de OpenAI utilizando PyTorch y la librería de Transformers de Hugging Face. Es un trabajo brillante.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*oRFMJTEojyQ-uocVES5GYA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://amaarora.github.io/2020/02/18/annotatedGPT2.html&quot;&gt;Fuente&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Más allá de BERT?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1&quot;&gt;Opinión&lt;/a&gt; interesante de Sergi Castella en que hay más allá de BERT. El tema principal incluye mejorar métricas, como la librería Transformer de Hugging Face empodera la investigación, set de datos interesantes para revisar, utilización de modelos, etc.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Operador para comprimir Matrices&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
El blog the TensorFlow publico un &lt;a href=&quot;https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&quot;&gt;articulo&lt;/a&gt; donde explican las técnicas para compresión de matrices y su importancia en un modelo de red neuronal. La compresión de matrices puede ayudar a construir modelos más eficientes y reducidos que pueden ser incorporados en dispositivos más pequeños como teléfonos y asistentes del hogar. Enfocarse en la comprensión por métodos como low-rank-approximation y quantization significa que no debemos comprometer la calidad del modelo&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*fpAdJvBIf4SKxF3gTIpe_g.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016&quot;&gt;Fuente&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;educación&quot;&gt;Educación🎓&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Fundamentos del NLP&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Estoy emocionado de liberar un borrador del capítulo 1 de mi nueva serie llamado &lt;a href=&quot;https://medium.com/dair-ai/fundamentals-of-nlp-chapter-1-tokenization-lemmatization-stemming-and-sentence-segmentation-b362c5d07684&quot;&gt;Fundamentos del NLP&lt;/a&gt;. Enseña conceptos comenzando desde lo más básico, compartiendo buenas prácticas, referencias importantes, errores comunes a evitar, y cuál es el futuro del NLP. Un &lt;a href=&quot;https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ&quot;&gt;Colab Notebook&lt;/a&gt; está incluido y el proyecto será mantenido &lt;a href=&quot;https://github.com/dair-ai/nlp_fundamentals&quot;&gt;aquí&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*mS5NcoJ_c8hYTjiJsuu_8g.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;[En línea] Revisión/Discusión: Parte 1 Sesión de lectura de Fundamentos Matemáticos&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Machine Learning Tokio está organizando una discusión en línea para revisar los capítulos que fueron cubiertos en su más reciente sesión de estudio. El grupo ha estudiado previamente capítulos del libro &lt;a href=&quot;https://mml-book.github.io/&quot;&gt;Matemáticas para el Machine Learning&lt;/a&gt; escrito por Marc Peter Deisenroth, A Aldo Faisal, y Cheng Soon Ong. El &lt;a href=&quot;https://www.meetup.com/Machine-Learning-Tokyo/events/268817313/&quot;&gt;evento&lt;/a&gt; está pautado para el 8 de Marzo de 2020.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Libros Recomendados&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
En un segmento previo discutimos la importancia de la compresión de matrices para construir modelos de ML compactos. Si estás interesado en aprender más sobre como construir redes neuronales más pequeñas para sistemas embebidos te recomiendo este gran libro llamado &lt;a href=&quot;https://tinymlbook.com/?linkId=82595412&quot;&gt;TinyML&lt;/a&gt; por Pete Warden y Daniel Situnayake.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*omOa3aw2bfMzX2Qm.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://www.amazon.com/TinyML-Learning-TensorFlow-Ultra-Low-Micro-Controllers/dp/1492052043&quot;&gt;Fuente&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Otro libro interesante a tener en cuenta es “&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;Deep Learning para programadores con fastai y pyTorch: Aplicaciones AI sin un PhD&lt;/a&gt;” por Jeremy Howard y Sylvain Gugger. El libro trata de proveer la matemática necesaria para construir y entrenar modelos que resuelvan tareas en el área de visión por computador y Entendimiento natural de texto.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*i2OmtWOncatOYsZv.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527&quot;&gt;Fuente&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;menciones-a-considerar-️&quot;&gt;Menciones a considerar ⭐️&lt;/h1&gt;

&lt;p&gt;Puedes acceder a las versiones anteriores de este boletín &lt;a href=&quot;https://medium.com/dair-ai/nlp-newsletter-pytorch3d-deepspeed-turing-nlg-question-answering-benchmarks-hydra-sparse-322f018ee096&quot;&gt;aquí&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1909.06576&quot;&gt;Torchmeta&lt;/a&gt; es una librería que permite el uso de data loaders para la investigación en meta-learning. El creador es Tristan Deleu.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Manuel Tonneau escribió una &lt;a href=&quot;https://creatext.ai/blog-posts/machine-text-writing-gpt2-beam-search?utm_medium=newsletter&quot;&gt;pieza&lt;/a&gt; ofreciendo una mirada más detallada a alguna de la maquinaria involucrada en el modelamiento del lenguaje. Algunos tópicos incluyen búsquedas greedy y beam, así como también nucleus sampling.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
El MIT &lt;a href=&quot;http://introtodeeplearning.com/&quot;&gt;liberó&lt;/a&gt; el programa de estudio completo del curso titulado “Introducción al Deep Learning”, incluye videos de las clases que ya se dictaron. Están apuntando a liberar videos y presentaciones todas las semanas.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Aprende a entrenar un modelo para reconocimiento de entidades nombradas (NER) utilizando una aproximación vía Transformers en menos de &lt;a href=&quot;https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py&quot;&gt;300 líneas de código&lt;/a&gt;. Puedes encontrar el Google Colab que lo acompaña &lt;a href=&quot;https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn&quot;&gt;aquí&lt;/a&gt;.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/Bolet%C3%ADn_informativo_NLP_GPT-2_Explicado,_Entendie/&quot;&gt;Boletín informativo NLP #5: GPT-2 Explicado, Entendiendo ‘Self-Distillation’, Haiku, GANILLA, Sparkwiki, Ética en el NLP, Torchmeta,...&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on February 23, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[Fundamentals of NLP (Chapter 1): Tokenization, Lemmatization, Stemming, and Sentence Segmentation]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/Fundamentals-of-NLP-Chapter-1/" />
  <id>https://dair.ai/Fundamentals-of-NLP-Chapter-1</id>
  <published>2020-02-21T00:00:00-06:00</published>
  <updated>2020-02-21T00:00:00-06:00</updated>
  <author>
    <name>Elvis Saravia</name>
    <uri>https://dair.ai</uri>
    <email>ellfae@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/1193/1*Qtz7CEKPnMyGx-62YqHcCg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Natural language processing (NLP) has made substantial advances in the past few years due to the success of &lt;a href=&quot;https://nlpoverview.com/&quot;&gt;modern techniques&lt;/a&gt; that are based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Deep_learning&quot;&gt;deep learning&lt;/a&gt;. With the rise of the popularity of NLP and the availability of different forms of large-scale data, it is now even more imperative to understand the inner workings of NLP techniques and concepts, from first principles, as they find their way into real-world usage and applications that affect society at large. Building intuitions and having a solid grasp of concepts are both important for coming up with innovative techniques, improving research, and building safe, human-centered AI and NLP technologies.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In this first chapter, which is part of a series called &lt;a href=&quot;https://github.com/dair-ai/nlp_fundamentals&quot;&gt;Fundamentals of NLP&lt;/a&gt;, we will learn about some of the most important basic concepts that power NLP techniques used for research and building real-world applications. Some of these techniques include lemmatization, stemming, tokenization, and sentence segmentation. These are all important techniques to train efficient and effective NLP models. Along the way, we will also cover best practices and common mistakes to avoid when training and building NLP models. We also provide some exercises for you to keep practicing and exploring some ideas.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
In every chapter, we will introduce the theoretical aspect and motivation of each concept covered. Then we will obtain hands-on experience by using bootstrap methods, industry-standard tools, and other open-source libraries to implement the different techniques. Along the way, we will also cover best practices, share important references, point out common mistakes to avoid when training and building NLP models, and discuss what lies ahead.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
The project will be maintained on &lt;a href=&quot;https://github.com/dair-ai/nlp_fundamentals&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Get access to the first chapter at this &lt;a href=&quot;https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ&quot;&gt;Colab notebook&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/640/1*mS5NcoJ_c8hYTjiJsuu_8g.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/Fundamentals-of-NLP-Chapter-1/&quot;&gt;Fundamentals of NLP (Chapter 1): Tokenization, Lemmatization, Stemming, and Sentence Segmentation&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on February 21, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP简报 [CH]: Tokenizers, TensorFlow 2.1, TextVectorization, TorchIO, NLP Shortfalls,…]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP%E7%AE%80%E6%8A%A5_Tokenizers,_TensorFlow_2_1,_TextVectorization/" />
  <id>https://dair.ai/NLP简报_Tokenizers,_TensorFlow_2_1,_TextVectorization</id>
  <published>2020-02-20T00:00:00-06:00</published>
  <updated>2020-02-20T00:00:00-06:00</updated>
  <author>
    <name>Elvis Saravia</name>
    <uri>https://dair.ai</uri>
    <email>ellfae@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/2400/1*gLVPodYjYd4YaF9sJbSpjg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;em&gt;Thanks to&lt;/em&gt; &lt;a href=&quot;https://blog.csdn.net/Kaiyuan_sjtu&quot;&gt;&lt;em&gt;Kaiyuan&lt;/em&gt;&lt;/a&gt; &lt;em&gt;(WeChat: NewBeeNLP) for this great translation.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
欢迎来到船新栏目&lt;strong&gt;「NLP 简报」&lt;/strong&gt;，本新闻简报的目的是让你不必花费太多时间就可以了解与 NLP 和 ML 有关的一些有趣和最新的故事。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;如果想让自己有趣的研究/项目出现在 NLP 简报中，随时在公众号后台留言联系我&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
下面来看看第一期的内容&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;1、Publications 📙&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;1.1 用于乳腺癌筛查的AI系统&lt;/li&gt;
      &lt;li&gt;1.2 信息抽取&lt;/li&gt;
      &lt;li&gt;1.3 Improved recommendation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;2、Creativity and Society 🎨&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;2.1 AI就业&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;3、Tools and Datasets ⚙️&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;3.1 一个极速分词器&lt;/li&gt;
      &lt;li&gt;3.2 用于搜索的ML&amp;amp;NLP&lt;/li&gt;
      &lt;li&gt;3.3 医学图像分析&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;4、Ethics in AI 🚨&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;4.1 ML社区的欺诈行为&lt;/li&gt;
      &lt;li&gt;4.2 机器翻译中的性别偏见&lt;/li&gt;
      &lt;li&gt;4.3 ML偏差与公正性&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;5、Articles and Blog posts ✍️&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;5.1 NLP shortfalls&lt;/li&gt;
      &lt;li&gt;5.2 NLP和ML2019年亮点&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;6、Education 🎓&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;6.1 Democratizing AI education&lt;/li&gt;
      &lt;li&gt;6.2 Top NLP and ML Books&lt;/li&gt;
      &lt;li&gt;6.3 使用核方法的机器学习&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;7、Noteworthy Mentions ⭐️&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;1publications-&quot;&gt;1、Publications 📙&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;1.1 用于乳腺癌筛查的 AI 系统&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
DeepMind 在 Nature 杂志上发表了一篇名为“International evaluation of an AI system for breast cancer screening[1]”的新论文。作者说，这项工作是关于在乳腺癌筛查方面超越人类专家的 AI 系统的评估。当前的 AI 系统是否真的可以实现这一点尚有争议，并且对于这种类型的系统及其评估方式一直存在批评。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;1.2 信息抽取&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Pankaj Gupta 公开发布了他的博士学位论文，题目为“Neural Information Extraction From Natural Language Text[2]”。主要讨论如何使用基于神经的方法有效地从自然语言文本中提取语义关系， 此类研究工作旨在促进构建结构化的知识库，该知识库可用于一系列下游 NLP 应用程序，例如 Web 搜索，问题解答以及其他任务。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;1.3 Improved recommendations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
MIT 和 IBM 的研究人员基于三种广泛使用的文本分析工具（主题建模，单词嵌入和最佳传输）的组合，开发了一种用于分类，显示和搜索相关文档的方法（于去年在 NeurIPS 上发布）。该方法还为文档排序提供了有希望的结果。此类方法适用于需要对大规模数据（例如搜索和推荐系统）进行改进和更快建议的各种场景和应用。&lt;/p&gt;

&lt;h1 id=&quot;2creativity-and-society-&quot;&gt;2、Creativity and Society 🎨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;2.1 AI 就业&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Stanford 2019 年 AI 指数报告[3]表明，对 AI 从业者的需求更多。但是，与 AI 相关的工作有很多方面，例如职业转变和面试仍然没有适当定义。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
在博客”Shifting Careers to Autonomous Vehicles[4]”中，Vladimir Iglovivok 详细介绍了他的职业生涯和 ML 冒险，从构建传统的推荐系统到构建壮观的计算机视觉模型（赢得了 Kaggle 竞赛）。他现在在 Lyft 从事自动驾驶汽车的工作，但是到达那里的旅程并不容易。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
如果 n 真的对 AI 事业感兴趣并且很认真，Andrew Ng 的公司 deeplearning.ai 成立了 Workera，该公司专门致力于帮助数据科学家和机器学习工程师从事 AI 事业。可以在此处获取关于Workra 的官方报告[5]。&lt;/p&gt;

&lt;h1 id=&quot;3tools-and-datasets-️&quot;&gt;3、Tools and Datasets ⚙️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;3.1 一个极速分词器&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hugging Face 是一家的 NLP 初创公司，拥有开源的 Tokenizers，这是一种可在现代 NLP pipeline 中使用的超快速的分词器，可以查看Tokenizers GitHub 库[6]以获取有关如何使用 Tokenizer 的文档。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*BGcXk6Yf9fXGZlEtxz1hcg.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
TensorFlow 2.1 合并了一个新的TextVectorization 层[7]，你可以轻松处理原始字符串并有效地执行文本 normalization，tokenization，n-gram 生成和词汇索引。点击查看Chollet 的 Colab 笔记[8]本，演示如何使用该功能进行端到端文本分类。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;3.2 用于搜索的 ML&amp;amp;NLP&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
去年，NLP 取得了巨大进步，其中一个领域是一系列改进和新的研究方向。搜索可能是可能从迁移学习 NLP 中受益的那些领域之一。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
尽管搜索属于信息检索领域，但仍有机会构建使用现代 NLP 技术（例如来自基于BERT[9]的基于变压器的模型的上下文表示）来改进语义搜索的搜索引擎。Google 在几个月前发布了一篇博客文章，讨论了他们如何利用 BERT 模型来改善和理解搜索[10]。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
如果您对如何将上下文化表示形式应用于使用 Elasticsearch 和 TensorFlow 等开放式搜索技术的搜索感到好奇，则可以查看”Elasticsearch meets BERT”[11]或”Building a Search Engine with BERT and TensorFlow”[12]。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;3.3 医学图像分析&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
TorchIO[13]是基于流行的深度学习库 PyTorch 的 Python 软件包。TorchIO 提供的功能可轻松高效地读取和采样 3D 医学图像。功能包括用于数据扩充和预处理的空间变换。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/0*FSPuSC8TK9X-NQ2q.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;4ethics-in-ai-&quot;&gt;4、Ethics in AI 🚨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;4.1 ML 社区的欺诈行为&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Kaggle 比赛的第一名优胜者因欺诈活动而被取消参赛资格，其队伍使用了聪明但不负责任和不可接受的策略来赢得比赛的第一名。原文”PetFinder.my Contest: 1st Place Winner Disqualified[14]”重点介绍了机器学习社区想要缓解的许多严重且无法接受的行为，正确和道德地使用机器学习技术是前进的唯一方法。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;4.2 机器翻译中的性别偏见&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
关于机器翻译系统是否反映性别偏见的主题，一组研究人员发表了这篇出色的论文，”Assessing Gender Bias in Machine Translation – A Case Study with Google Translate[15]”，提出了使用 Google 翻译的案例研究。作者声称的一项发现是，Google 翻译“表现出强烈的男性违约倾向，特别是在与性别分布失衡有关的领域，例如 STEM 工作。”&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;4.3 ML 偏差与公正性&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
如果您想让所有人都了解 AI 伦理和公平，那么这是一个由Timnit Gebru[16]主持，由 TWIML 主持的不错的播客。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Timnit 是 ML 公平性方面的杰出研究者，他与 Eun Seo Jo 一起发表了一篇论文，”Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning”[17]他们确定了档案中文档收集实践的五种关键方法，这些方法可以为社会文化 ML 中的数据收集提供更可靠的方法。这可能会导致跨学科合作研究获得更系统的数据收集方法。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sina Fazelpour 和 Zachary Lipton 最近发表了一篇论文，”fairness-non-ideal-fazelpour-lipton-2020[18]”，他们认为，由于我们非理想世界的产生方式的性质，基于理想思维的公平 ML 可能会导致误导政策和干预措施。实际上，他们的分析表明“提出的公平 ML 算法的缺点反映了理想方法所面临的广泛问题。”&lt;/p&gt;

&lt;h1 id=&quot;5articles-and-blog-posts-️&quot;&gt;5、Articles and Blog posts ✍️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;5.1 NLP shortfalls&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Benjamin Heinzerling 在 The Gradient 中发表了一篇有趣的文章，讨论了NLP 不足的领域[19]，例如论点理解和常识推理。本杰明参考了 Nivin＆Kao 的最新论文，”Probing Neural Network Comprehension of Natural Language Arguments[20]”，该论文挑战和质疑了转移学习和语言模型对高级自然语言理解的能力。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;5.2 NLP 和 ML2019 年亮点&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
对于新的一年，报告”NLP Year in Review — 2019[21]”，记录了 2019 年的一些最有趣的 NLP 和 ML 亮点。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
塞巴斯蒂安·鲁德（Sebastian Ruder）最近还写了一篇精彩而详尽的博客文章[22]，介绍了十大关于 ML 和 NLP 的研究方向，他认为这很有影响力 在 2019 年。列表中包括诸如通用无监督预训练，应用于科学的 ML 和 NLP，增强预训练模型，高效和远程 Transformers 等主题。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Google AI 研究会发布他们一年来进行的研究的摘要以及他们正在关注的未来研究方向，”Google Research: Looking Back at 2019, and Forward to 2020 and Beyond[23]”。&lt;/p&gt;

&lt;h1 id=&quot;6education-&quot;&gt;6、Education 🎓&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;6.1 Democratizing AI education&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
为了使 AI 教育民主化并向大众普及 AI 技术的影响，赫尔辛基大学与 Reaktor 合作发布了涵盖 AI 基础知识的精彩免费课程。受欢迎的课程称为“Elements of AI[24]”，包括诸如 AI 伦理学，AI 哲学，神经网络，朴素贝叶斯规则等主题，以及其他基础主题。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Stanford CS224N 再次推出了流行的“Natural Language Processing with Deep Learning[25]”课程。该课程于今年 1 月 7 日正式开始，因此，如果您想学习该课程，请访问其网站以获取完整的课程提纲，幻灯片，视频，阅读建议等。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;6.2 Top NLP and ML Books&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
关于一些NLP 和 ML 领域书籍推荐[26]。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;6.3 使用核方法的机器学习&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
诸如 PCA 和 K-means 之类的核方法已经存在了很长一段时间，这是因为它们已成功应用于各种应用，例如图形和生物序列。查看这套涵盖了各种Kernel Methods[27]及其内部工作原理的综合幻灯片。这也是一个由 Francis Bach 维护很棒的博客，”Are all kernels cursed?[28]”，讨论了内核方法和其他机器学习主题。&lt;/p&gt;

&lt;h1 id=&quot;7notable-mentions-️&quot;&gt;7、Notable Mentions ⭐️&lt;/h1&gt;

&lt;p&gt;另外还有一些有趣的研究/项目：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;John Langford 的博客[29]，讨论机器学习理论；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;许多行业的面向机器学习的技术已经使用梯度增强机器多年了。看看这篇文章[30]，其中介绍了一个用于应用梯度增强的库 XGBoost。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;如果你对学习如何设计和构建基于机器学习的应用程序并将其投入生产感兴趣，那么Emmanuel Ameisen[31]的书籍可以帮助你。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;本文参考资料&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[1] &lt;strong&gt;International evaluation of an AI system for breast cancer screening:&lt;/strong&gt; https://www.nature.com/articles/s41586-019-1799-6&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[2] &lt;strong&gt;Neural Information Extraction From Natural Language Text:&lt;/strong&gt; https://www.researchgate.net/publication/336739252_PhD_Thesis_Neural_Information_Extraction_From_Natural_Language_Text&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[3] &lt;strong&gt;Stanford 2019 年 AI 指数报告:&lt;/strong&gt; https://hai.stanford.edu/sites/g/files/sbiybj10986/f/ai_index_2019_report.pdf&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[4] &lt;strong&gt;Shifting Careers to Autonomous Vehicles:&lt;/strong&gt; https://towardsdatascience.com/how-i-found-my-current-job-3fb22e511a1f&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[5] &lt;strong&gt;Workra 的官方报告:&lt;/strong&gt; https://workera.ai/candidates/report&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[6] &lt;strong&gt;Tokenizers GitHub 库:&lt;/strong&gt; https://github.com/huggingface/tokenizers&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[7] &lt;strong&gt;TextVectorization 层:&lt;/strong&gt; https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[8] &lt;strong&gt;Chollet 的 Colab 笔记:&lt;/strong&gt; https://colab.research.google.com/drive/1RvCnR7h0_l4Ekn5vINWToI9TNJdpUZB3&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[9] &lt;strong&gt;BERT:&lt;/strong&gt; https://arxiv.org/abs/1810.04805&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[10] &lt;strong&gt;利用 BERT 模型来改善和理解搜索:&lt;/strong&gt; https://www.blog.google/products/search/search-language-understanding-bert/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[11] &lt;strong&gt;Elasticsearch meets BERT:&lt;/strong&gt; https://towardsdatascience.com/elasticsearch-meets-bert-building-search-engine-with-elasticsearch-and-bert-9e74bf5b4cf2&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[12] &lt;strong&gt;Building a Search Engine with BERT and TensorFlow:&lt;/strong&gt; https://towardsdatascience.com/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[13] &lt;strong&gt;TorchIO:&lt;/strong&gt; https://github.com/fepegar/torchio&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[14] &lt;strong&gt;PetFinder.my Contest: 1st Place Winner Disqualified:&lt;/strong&gt; https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/125436&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[15] &lt;strong&gt;Assessing Gender Bias in Machine Translation – A Case Study with Google Translate:&lt;/strong&gt; https://arxiv.org/abs/1809.02208&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[16] &lt;strong&gt;Timnit Gebru:&lt;/strong&gt; https://twimlai.com/twiml-talk-336-trends-in-fairness-and-ai-ethics-with-timnit-gebru/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[17] &lt;strong&gt;Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning:&lt;/strong&gt; https://arxiv.org/abs/1912.10389&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[18] &lt;strong&gt;fairness-non-ideal-fazelpour-lipton-2020:&lt;/strong&gt; http://zacklipton.com/media/papers/fairness-non-ideal-fazelpour-lipton-2020.pdf&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[19] &lt;strong&gt;NLP 不足的领域:&lt;/strong&gt; https://thegradient.pub/nlps-clever-hans-moment-has-arrived/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[20] &lt;strong&gt;Probing Neural Network Comprehension of Natural Language Arguments:&lt;/strong&gt; https://www.aclweb.org/anthology/P19-1459/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[21] &lt;strong&gt;NLP Year in Review — 2019:&lt;/strong&gt; https://medium.com/dair-ai/nlp-year-in-review-2019-fb8d523bcb19&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[22] &lt;strong&gt;精彩而详尽的博客文章:&lt;/strong&gt; https://ruder.io/research-highlights-2019/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[23] &lt;strong&gt;Google Research: Looking Back at 2019, and Forward to 2020 and Beyond:&lt;/strong&gt; https://ai.googleblog.com/2020/01/google-research-looking-back-at-2019.html&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[24] &lt;strong&gt;Elements of AI:&lt;/strong&gt; https://www.elementsofai.com/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[25] &lt;strong&gt;Natural Language Processing with Deep Learning:&lt;/strong&gt; http://web.stanford.edu/class/cs224n/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[26] &lt;strong&gt;NLP 和 ML 领域书籍推荐:&lt;/strong&gt; https://twitter.com/omarsar0/status/1214547402838986754?s=20&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[27] &lt;strong&gt;Kernel Methods:&lt;/strong&gt; http://members.cbio.mines-paristech.fr/~jvert/svn/kernelcourse/slides/master2017/master2017.pdf&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[28] &lt;strong&gt;Are all kernels cursed?:&lt;/strong&gt; https://francisbach.com/cursed-kernels/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[29] &lt;strong&gt;John Langford 的博客:&lt;/strong&gt; https://hunch.net/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[30] &lt;strong&gt;这篇文章:&lt;/strong&gt; https://opendatascience.com/xgboost-enhancement-over-gradient-boosting-machines/?utm_campaign=Learning%20Posts&amp;amp;utm_content=111061559&amp;amp;utm_medium=social&amp;amp;utm_source=twitter&amp;amp;hss_channel=tw-3018841323&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[31] &lt;strong&gt;Emmanuel Ameisen:&lt;/strong&gt; https://www.amazon.com/Building-Machine-Learning-Powered-Applications/dp/149204511X/&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP%E7%AE%80%E6%8A%A5_Tokenizers,_TensorFlow_2_1,_TextVectorization/&quot;&gt;NLP简报 [CH]: Tokenizers, TensorFlow 2.1, TextVectorization, TorchIO, NLP Shortfalls,…&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on February 20, 2020.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[NLP简报 [CH]: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,…]]></title>
  <link rel="alternate" type="text/html" href="https://dair.ai/NLP%E7%AE%80%E6%8A%A5_ISSUE_4_PyTorch3D,_DeepSpeed,_Turing-NLG/" />
  <id>https://dair.ai/NLP简报_ISSUE_4_PyTorch3D,_DeepSpeed,_Turing-NLG</id>
  <published>2020-02-17T00:00:00-06:00</published>
  <updated>2020-02-17T00:00:00-06:00</updated>
  <author>
    <name>Elvis Saravia</name>
    <uri>https://dair.ai</uri>
    <email>ellfae@gmail.com</email>
  </author>
  <content type="html">
    &lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1200/1*3vNKhz6K-oGQ8aLi3mo84Q.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;em&gt;Thanks to&lt;/em&gt; &lt;a href=&quot;https://blog.csdn.net/Kaiyuan_sjtu&quot;&gt;&lt;em&gt;Kaiyuan&lt;/em&gt;&lt;/a&gt; &lt;em&gt;(WeChat: NewBeeNLP) for this great translation.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
欢迎来到 NLP 时事简报！全文较长，建议收藏。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
另外加了目录方便直接索引到自己感兴趣的部分。enjoy&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;1、Publications 📙&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;1.1 Turing-NLG:&lt;/li&gt;
      &lt;li&gt;1.2 Neural based Dependency Parsing&lt;/li&gt;
      &lt;li&gt;1.3 End-to-end Cloud-based Information Extraction with BERT&lt;/li&gt;
      &lt;li&gt;1.4 Question Answering Benchmark&lt;/li&gt;
      &lt;li&gt;1.5 Radioactive data: tracing through training&lt;/li&gt;
      &lt;li&gt;1.6 REALM: Retrieval-Augmented Language Model Pre-Training&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;2、Creativity and Society 🎨&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;2.1 允许在科学会议上进行远程论文和海报展示&lt;/li&gt;
      &lt;li&gt;2.2 Abstraction and Reasoning Challenge&lt;/li&gt;
      &lt;li&gt;2.3 ML and NLP Publications in 2019&lt;/li&gt;
      &lt;li&gt;2.4 Growing Neural Cellular Automata&lt;/li&gt;
      &lt;li&gt;2.5 Transformer attention可视化&lt;/li&gt;
      &lt;li&gt;2.6 SketchTransfer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;3、Tools and Datasets ⚙️&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;3.1 DeepSpeed + ZeRO&lt;/li&gt;
      &lt;li&gt;3.2 一个用于进行快速有效的3D深度学习研究的库&lt;/li&gt;
      &lt;li&gt;3.3 管理你的ML项目配置&lt;/li&gt;
      &lt;li&gt;3.4 贝叶斯网络因果推理工具包&lt;/li&gt;
      &lt;li&gt;3.5 TyDi问答：多语言问答基准&lt;/li&gt;
      &lt;li&gt;3.6 Question Answering for Node.js&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;4、Ethics in AI 🚨&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;4.1 识别文本中的subjective bias&lt;/li&gt;
      &lt;li&gt;4.2 Artificial Intelligence, Values and Alignment&lt;/li&gt;
      &lt;li&gt;4.3 关于审核AI系统&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;5、Articles and Blog posts ✍️&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;5.1 用于NLP系统的模型蒸馏&lt;/li&gt;
      &lt;li&gt;5.2 单词的上下文表示&lt;/li&gt;
      &lt;li&gt;5.3 神经网络中的稀疏性&lt;/li&gt;
      &lt;li&gt;5.4 训练你自己的语言模型&lt;/li&gt;
      &lt;li&gt;5.5 分词器Tokenizer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;6、Education 🎓&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;6.1 阿姆斯特丹自由大学机器学习课程&lt;/li&gt;
      &lt;li&gt;6.2 机器学习数学资源&lt;/li&gt;
      &lt;li&gt;6.3 深度学习入门&lt;/li&gt;
      &lt;li&gt;6.4 Pytorch深度学习&lt;/li&gt;
      &lt;li&gt;6.5 Missing Semester of Your CS&lt;/li&gt;
      &lt;li&gt;6.6 深度学习进阶&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;7、Noteworthy Mentions ⭐️&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;1publications-&quot;&gt;1、Publications 📙&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;1.1 Turing-NLG: A 17-billion-parameter language model by Microsoft&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
图灵自然语言生成（T-NLG）[1]是由 Microsoft AI 研究人员提出的 170 亿参数语言模型。除了是迄今为止最大的已知语言模型（如下图所示）之外，T-NLG 是基于 78 层 Transformer 的语言模型，其在 WikiText-103 上的困惑度性能优于之前的最新技术成果（由NVIDIA Megatron-LM[2]持有） 。T-NLG 在各种任务（例如问题回答和抽象摘要）上进行了测试，同时分别显示了模型的好处，例如零简短问题功能和最小化监督。此外，该模型得益于 DeepSpeed 库（与 PyTorch 兼容）和 ZeRO 优化器，这两者也会在本期简报中具体介绍。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*CAZm7uj8EaupnvnJ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;语言模型参数大小&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;1.2 Neural based Dependency Parsing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Miryam de Lhoneux 公布了其博士学位论文“Linguistically Informed Neural Dependency Parsing for Typologically Diverse Languages[3]”。这项工作是关于使用神经学方法以类型多样的语言（即以结构上不同的方式构造和表达含义的语言）进行依赖关系解析[4]。论文指出 RNN 和递归层可能有助于合并到解析器中，因为它们有助于告知具有解析所需的重要语言知识的模型。更多 ideas 包括使用多语言解析和参数共享策略来解析相关和不相关语言。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;1.3 End-to-end Cloud-based Information Extraction with BERT&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
研究人员发表了一篇论文”Rapid Adaptation of BERT for Information Extraction on Domain-Specific Business Documents”[5]，描述了 BERT 等 Transformer 模型如何帮助特定领域的业务文档（例如监管文件和物业租赁协议）中的端到端信息提取。这种类型的工作不仅可以帮助优化业务运营，而且还显示了基于 BERT 的模型在带注释数据很少的情况下的适用性和有效性。同时提出并讨论了在云上运行的应用程序及其实现细节（请参见下图）。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*KqViSLhP0otleDY-XFy3Bg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;1.4 Question Answering Benchmark&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Wolfson 等发布了一个question understanding benchmark[6]，以及一种用于分解计算适当答案所必需的问题的方法。他们利用众包来注释分解问题所需的必要步骤， 为了展示该方法的可行性和适用性，他们改进了使用 HotPotQA 数据集的开放域问答。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*AztG-Inqt6LGQ87lSufRcw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;不同来源的问题具有相似的组成结构。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;1.5 Radioactive data: tracing through training&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Facebook AI 研究人员最近发表了一项有趣的工作[7]，旨在标记图像（称为&lt;strong&gt;「radioactive data」&lt;/strong&gt;），以验证该特定数据集是否用于训练 ML 模型。他们发现，可以使用巧妙的标记将特征移向某个方向，即使只有 1％的训练数据是 radioactiv，模型也可以使用该标记帮助检测‘radioactive data’的使用情况。这极具挑战性，因为数据中的任何更改都可能会降低模型的准确性。作者说，这项工作可以“帮助研究人员和工程师跟踪用于训练模型的数据集，以便他们可以更好地了解各种数据集如何影响不同神经网络的性能”，在关键任务 ML 应用程序中，这似乎是一种重要的方法。如果感兴趣可以查看完整论文[8]了解具体信息。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;1.6 REALM: Retrieval-Augmented Language Model Pre-Training&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
REALM [9] 是基于神经的大规模检索方法，该方法利用文本知识的语料库以无监督的方式预训练语言模型。该方法的主要目的是以一种更可解释的方式捕获知识，具体而言是利用世界知识对模型进行训练和预测。使用 REALM 处理和评估的任务包括开放域问答基准， 除了提高模型的准确性外，其他好处还包括模块化和可解释性组件。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*MJO-yzCwsB5ydKGz7hKHVA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;2creativity-and-society-&quot;&gt;2、Creativity and Society 🎨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;2.1 允许在科学会议上进行远程论文和海报展示&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
过去一周有请愿书散发，以便在与 ML 相关的科学会议上进行远程论文和海报展示，可以在change.org[10]上阅读有关它的更多信息。深度学习的先驱 Yoshua Bengio 似乎在倡导人们去签署请愿书， 并在他的新博客[11]中阐明了这一点。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;2.2 Abstraction and Reasoning Challenge&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
FrançoisChollet 最近发布了一个 Kaggle 竞赛，他发布了抽象推理语料库（ARC）[12]，旨在鼓励用户创建可以解决从未接触过的推理任务的 AI 系统。希望能够开始构建更强大的 AI 系统，从而能够更好，快速地自行解决新问题，这可能有助于解决更具挑战性的现实应用，例如改善在极端和多样化环境中运行的自动驾驶汽车 。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;2.3 ML and NLP Publications in 2019&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Marek Rei 发布机器学习和 NLP 领域 2019 年出版数据统计[13]，分析中包括的会议是 ACL，EMNLP，NAACL，EACL，COLING，TACL，CL，CoNLL，NeurIPS，ICML，ICLR 和 AAAI。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;2.4 Growing Neural Cellular Automata&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;Morphogenesis&lt;/code&gt;是一个自组织过程，通过该过程，蝾螈等某些生物可以再生或修复身体损伤。该过程对于扰动是鲁棒的，并且本质上是自适应的。受这种生物学现象的启发，需要更好地了解这一过程，研究人员发表了一篇题为”Growing Neural Cellular Automata”[14]的论文，该论文采用了可区分的形态发生模型，旨在复制自我修复系统的行为和特性，并希望能够制造出具有与生物生命相同的坚固性和可塑性的自我修复 machines。此外，这将有助于更好地了解再生过程本身， 许多应用注入再生医学以及社会和生物系统的建模等都将受益。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*2p62h1RaHD6d11LX8olnTA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;2.5 Transformer attention 可视化&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hendrik Strobelt 开源项目显示了如何使用 Hugging Face 库和 d3.js 通过 Web 应用程序快速构建简单的交互式 Transformer 注意可视化[15]。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*lMaZGDRJUI1Qcv7T5AdhlQ.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;2.6 SketchTransfer&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
SketchTransfer[16]提出了一项新任务，以测试深度神经网络在存在/不存在细节时支持不变性的能力。长期以来一直争论的是，深度网络无法推广到训练期间尚未发现的变化，人类可以相对轻松地完成某些事情，例如在观看动画片时处理丢失的视觉细节。本文讨论并发布了一个数据集，以通过提供未标记的草图图像和标记的真实图像示例来帮助研究人员仔细研究“细节不变性”问题。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*jdYuMoHiu2yya5rHzZyjwQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3tools-and-datasets-️&quot;&gt;3、Tools and Datasets ⚙️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;3.1 DeepSpeed + ZeRO&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Microsoft 开源了一个称为 DeepSpeed 的训练优化库，该库与 PyTorch 兼容，并且可以训练 1000 亿个参数的模型。该库侧重于训练模型的四个重要方面：规模，速度，成本和可用性。DeepSpeed 与 ZeRO[18]一起发布，ZeRO 是一种内存优化技术，可在当前 GPU 技术中实现大规模分布式深度学习，同时将吞吐量提高到当前最佳系统的三到五倍。ZeRO 允许训练任意大小的模型，这些模型可以根据共享模型状态适合聚合的可用内存。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*MXDI1f3cSBrY5w2g.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;3.2 一个用于进行快速有效的 3D 深度学习研究的库&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
PyTorch3D[19]是用于基于 3D 的深度学习研究的开源工具包。这个 PyTorch 库旨在帮助深度学习系统中的 3D 数据支持和理解，该库由常用 3D 运算符和损失函数的快速优化实现组成， 它还带有模块化的可区分渲染器，可帮助进行复杂 3D 输入的研究并支持高质量 3D 预测。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*VbspKMmPBUsgpdnIkd5jYA.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;3.3 管理你的 ML 项目配置&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hydra[20]是基于 Python 的配置工具，用于更有效地管理复杂的 ML 项目。它旨在通过为 ML 项目提供功能的配置重用来帮助 PyTorch 研究人员。它提供的主要好处是它允许程序员像编写代码一样编写配置，这意味着可以轻松地覆盖配置文件。Hydra 还可以帮助自动管理 ML 项目输出的工作目录，这在需要保存和访问多个作业的多个实验结果时非常有用。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;3.4 贝叶斯网络因果推理工具包&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
CausalNex[21]是用于“与贝叶斯网络进行因果推理”的工具包。该工具旨在将机器学习和因果推理相结合，以发现数据中的结构关系。作者还准备了关于使用 Python 库推断贝叶斯网络原因和原因的入门指南。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*EYwKhdnscR7ZLuNkTqCS2Q.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;3.5 TyDi 问答：多语言问答基准&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Google AI 发布了TyDi QA[22]，它是一种多语言的数据集，可以鼓励研究人员对更多类型多样的语言进行问答，这些语言以不同的方式构建和表达含义。这样做的目的是激励研究人员在类型上距离遥远的语言（例如阿拉伯语，孟加拉语，韩语，俄语，泰卢固语和泰语）上建立更强大的模型，以便将其推广到更多种语言。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*1dZv5you3jigdrQ2uAKzUw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;3.6 Question Answering for Node.js&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Hugging Face 发布了基于 DistilBERT 的问答库[23]，并继续使 NLP 更加易于访问。该模型可以使用 Node.js 在生产中运行，只需 3 行代码。该模型利用了由 Hugging Face 和 TensorFlow.js（用于将机器学习模型与 Javascript 结合使用的流行库）构建的 Tokenizer 的快速实现。&lt;/p&gt;

&lt;h1 id=&quot;4ethics-in-ai-&quot;&gt;4、Ethics in AI 🚨&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;4.1 识别文本中的 subjective bias&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
计算社会科学的研究员 Diyi Yang 讨论了 AI 系统如何帮助识别文本信息中的主观偏见[25]。这是涉及 AI 系统和 NLP 的重要研究领域，尤其是当我们讨论诸如新闻标题之类的文本媒体的消费时，很容易将其构架成偏向消费者，而实际上他们应该追求更加客观。从应用程序的角度来看，自动识别文本媒体中存在的主观偏见以帮助消费者更加了解他们正在消费的内容变得至关重要。此外还讨论了 AI 如何也可以保持偏见。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;4.2 Artificial Intelligence, Values and Alignment&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
人工智能系统的兴起以及它们如何与人类价值观保持一致是涉及人工智能系统伦理学的活跃研究领域。DeepMind[26]最近发布了一篇论文，深入探讨了围绕 AI 对齐的哲学问题。该报告重点讨论了两个部分，即技术部分（即如何对从 AI 代理获得可靠结果的值进行编码）和规范性（在 AI 中进行编码的原则是正确的）以及它们之间的联系以及可以确保的部分。本文主张采用一种基于原则的 AI 对齐方法，并在信念和观点存在差异的情况下保持公平待遇。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;4.3 关于审核 AI 系统&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
VentureBeat 报告称，Google 研究人员与其他小组合作创建了一个名为 SMACTR 的框架，该框架使工程师可以审核 AI 系统。进行这项工作的原因是为了解决目前被消费者广泛使用以供使用的 AI 系统存在的问责制差距。在这里阅读完整的报告[27]，在这里阅读完整的论文[28]。&lt;/p&gt;

&lt;h1 id=&quot;5articles-and-blog-posts-️&quot;&gt;5、Articles and Blog posts ✍️&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;5.1 用于 NLP 系统的模型蒸馏&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
在NLP Highlights[29]播客的新剧集中，Thomas Wolf 和 Victor Sanh 讨论了模型蒸馏，以及如何将其用作压缩大型模型（如 BERT）以用于可扩展的实际 NLP 应用程序的可行方法。他们在他们提出的称为DistilBERT[30]的方法中对此概念进行了进一步的讨论，在该方法中，他们构建较小的模型（基于较大模型的相同体系结构），以根据该模型的输出来模仿较大模型的行为。本质上，较小的模型（学生）会尝试根据其输出分布来拟合教师的概率分布。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;5.2 单词的上下文表示&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
最近，关于诸如 BERT 的上下文化方法成功用于处理各种复杂的 NLP 任务的讨论很多。在这篇文章中，Kawin Ethayarajh 试图回答以下问题：诸如 BERT，ELMo 和 GPT-2 之类的上下文模型及其上下文化的词表示形式是什么[31]？主题包括语境性，语境特定性的度量以及静态嵌入与语境化表示之间的比较。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/0*70aIv1Fkkz4rnHgQ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;5.3 神经网络中的稀疏性&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
ML 研究人员 FrançoisLagunas 写了这篇很棒的文章，Is the future of Neural Networks Sparse?[32] 讨论了他对在神经网络模型中采用稀疏张量的乐观态度。希望采用某种形式的稀疏性来减小当前模型的大小，这些模型由于其大小和速度而在某些时候变得不切实际。由于当前模型（例如 Transformer）的庞大规模（通常依赖数十亿个参数），因此在 ML 中可能值得探讨这一概念。但是，从开发人员工具的角度来看，在 GPU 上的神经网络中支持有效稀疏性的实现细节尚不清楚，这是机器学习社区正在努力的事情。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;5.4 训练你自己的语言模型&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
如果你想学习如何从零开始训练语言模型[33]，请查看 Hugging Face 的这份令人印象深刻且全面的教程。他们显然利用了自己的库 Transformers 和 Tokenizers 来训练模型。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;5.5 分词器 Tokenizer&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Cathal Horan 发表了一篇令人印象深刻且非常详细的博客文章，Tokenizers: How machines read[34]，内容涉及最新的 NLP 模型如何以及使用哪种类型的 tokenizer 来帮助机器学习算法从文本信息中学习。他还讨论并激发了为什么 tokenizer 是激动人心且重要的研究活跃领域的原因。文章甚至还展示了如何使用 SentencePiece 和 WordPiece 等标记化方法来训练自己的标记化工具。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*Vkjw5n9Sz0Was43haVNJMg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;6education-&quot;&gt;6、Education 🎓&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;6.1 阿姆斯特丹自由大学机器学习课程&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
现在，你可以在线学习 2020 MLVU 机器学习课程[35]，其中包括全套幻灯片，视频和教学大纲。它旨在作为 ML 的入门，但它也包含其他与深度学习相关的主题，例如 VAE 和 GAN。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;6.2 机器学习数学资源&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
SuzanaIlić 和东京机器学习（MLT）在使 ML 教育民主化方面一直做着惊人的工作。例如，该库Machine-Learning-Tokyo/Math_resources[36]展示了免费的在线资源集合，用于学习 ML 中使用的数学概念的基础。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;6.3 深度学习入门&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
MIT 的“深度学习入门”课程[37]， 每周都会发布新的讲座，所有方面和视频（包括编码实验室）都将发布。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;6.4 Pytorch 深度学习&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Alfredo Canziani 发布了PyTorch 深度学习[38]微型课程的幻灯片和笔记本，该资源库还包含一个配套网站，其中包含对本课程中所教授概念的文字描述。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;6.5 Missing Semester of Your CS&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
由麻省理工学院的教员发布的“Missing Semester”[39]是一门很棒的在线课程，其中包含对非开发背景的数据科学家而言可能有用的材料。它包括诸如 Shell 工具和脚本以及版本控制之类的主题。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*weUnTXxmHxYf-B2DDaslvw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;6.6 深度学习进阶&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
CMU 发布了“Advanced Deep Learning”[40]课程的幻灯片和教学大纲，其中包括诸如自动回归模型，生成模型以及自我监督/预测学习等主题。该课程适用于 MS 或 Ph.D， 具有 ML 高级背景的学生。&lt;/p&gt;

&lt;h1 id=&quot;7noteworthy-mentions-️&quot;&gt;7、Noteworthy Mentions ⭐️&lt;/h1&gt;

&lt;p&gt;BERT-of-Theseus[41]提出了一种通过将 BERT 模型划分为原始组件来逐步替换和压缩 BERT 模型的方法。通过逐步替换和训练，还具有将模型的原始组件和压缩版本组合在一起的优势。所提出的模型优于 GLUE 基准上的其他知识提炼方法。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
这儿找到一份有趣的课程，称为“机器学习入门”[42]，涵盖了 ML 基础知识，监督回归，随机森林，参数调整以及许多其他基本的 ML 主题。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
希腊语 BERT（GreekBERT）[43]模型现在可通过 Hugging Face Transformers 库使用。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Jeremy Howard[44]发表了一篇论文，描述了 fastai 深度学习库，该库被广泛用于研究并教授其深度学习开放课程， 推荐给致力于构建和改进深度学习和 ML 库的软件开发人员。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Deeplearning.ai 完成了 TensorFlow 所有四个课程的发布：TensorFlow: Data and Deployment Specialization[45]。该专业的主要目的是教育开发人员如何在不同的场景中有效地部署模型，以及在训练模型时以有趣且有效的方式使用数据。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Sebastian Raschka 最近发表了一篇题为《Python 中的机器学习：数据科学，机器学习和人工智能的主要发展和技术趋势[46]》的论文。本文是对机器学习工具前景的全面回顾。对于理解 ML 工程中使用的某些库和概念的各种优点而言，这是一份极好的报告。此外，还提供了有关基于 Python 的机器学习库的未来的信息。&lt;/p&gt;

&lt;h1 id=&quot;本文参考资料&quot;&gt;本文参考资料&lt;/h1&gt;

&lt;p&gt;[1] &lt;strong&gt;图灵自然语言生成（T-NLG）:&lt;/strong&gt; https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[2] &lt;strong&gt;NVIDIA Megatron-LM:&lt;/strong&gt; https://github.com/NVIDIA/Megatron-LM&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[3] &lt;strong&gt;Linguistically Informed Neural Dependency Parsing for Typologically Diverse Languages:&lt;/strong&gt; http://uu.diva-portal.org/smash/record.jsf?pid=diva2:1357373&amp;amp;dswid=7905&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[4] &lt;strong&gt;依赖关系解析:&lt;/strong&gt; http://nlpprogress.com/english/dependency_parsing.html&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[5] &lt;strong&gt;Rapid Adaptation of BERT for Information Extraction on Domain-Specific Business Documents:&lt;/strong&gt; https://arxiv.org/abs/2002.01861&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[6] &lt;strong&gt;question understanding benchmark:&lt;/strong&gt; https://arxiv.org/abs/2001.11770v1&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[7] &lt;strong&gt;有趣的工作:&lt;/strong&gt; https://ai.facebook.com/blog/using-radioactive-data-to-detect-if-a-data-set-was-used-for-training/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[8] &lt;strong&gt;完整论文:&lt;/strong&gt; https://arxiv.org/pdf/2002.00937.pdf&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[9] &lt;strong&gt;REALM: Retrieval-Augmented Language Model Pre-Training:&lt;/strong&gt; https://kentonl.com/pub/gltpc.2020.pdf&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[10] &lt;strong&gt;change.org:&lt;/strong&gt; https://www.change.org/p/organizers-of-data-science-and-machine-learning-conferences-neurips-icml-aistats-iclr-uai-allow-remote-paper-poster-presentations-at-conferences[11]&lt;strong&gt;新博客:&lt;/strong&gt; https://yoshuabengio.org/2020/02/10/fusce-risus/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[12] &lt;strong&gt;抽象推理语料库（ARC）:&lt;/strong&gt; https://www.kaggle.com/c/abstraction-and-reasoning-challenge/overview&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[13] &lt;strong&gt;机器学习和 NLP 领域 2019 年出版数据统计:&lt;/strong&gt; https://www.marekrei.com/blog/ml-and-nlp-publications-in-2019/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[14] &lt;strong&gt;Growing Neural Cellular Automata:&lt;/strong&gt; https://distill.pub/2020/growing-ca/[15]&lt;strong&gt;交互式 Transformer 注意可视化:&lt;/strong&gt; https://github.com/SIDN-IAP/attnvis&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[16] &lt;strong&gt;SketchTransfer:&lt;/strong&gt; https://arxiv.org/pdf/1912.11570.pdf&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[18] &lt;strong&gt;DeepSpeed 与 ZeRO:&lt;/strong&gt; https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[19] &lt;strong&gt;PyTorch3D:&lt;/strong&gt; https://ai.facebook.com/blog/-introducing-pytorch3d-an-open-source-library-for-3d-deep-learning/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[20] &lt;strong&gt;Hydra:&lt;/strong&gt; https://hydra.cc/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[21] &lt;strong&gt;CausalNex:&lt;/strong&gt; https://causalnex.readthedocs.io/en/latest/01_introduction/01_introduction.html&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[22] &lt;strong&gt;TyDi QA:&lt;/strong&gt; https://ai.googleblog.com/2020/02/tydi-qa-multilingual-question-answering.html&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[23] &lt;strong&gt;问答库:&lt;/strong&gt; https://github.com/huggingface/node-question-answering&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[25] &lt;strong&gt;识别文本信息中的主观偏见:&lt;/strong&gt; https://podcasts.apple.com/us/podcast/will-ai-help-identify-bias-or-perpetuate-it-with-diyi-yang/id1435564422?i=1000464141922&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[26] &lt;strong&gt;DeepMind:&lt;/strong&gt; https://deepmind.com/research/publications/Artificial-Intelligence-Values-and-Alignment[27]&lt;strong&gt;完整的报告:&lt;/strong&gt; https://venturebeat.com/2020/01/30/google-researchers-release-audit-framework-to-close-ai-accountability-gap/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[28] &lt;strong&gt;完整的论文:&lt;/strong&gt; https://dl.acm.org/doi/abs/10.1145/3351095.3372873&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[29] &lt;strong&gt;NLP Highlights:&lt;/strong&gt; https://soundcloud.com/nlp-highlights/104-model-distillation-with-victor-sanh-and-thomas-wolf&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[30] &lt;strong&gt;DistilBERT:&lt;/strong&gt; https://arxiv.org/abs/1910.01108&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[31] &lt;strong&gt;诸如 BERT，ELMo 和 GPT-2 之类的上下文模型及其上下文化的词表示形式是什么:&lt;/strong&gt; https://kawine.github.io/blog/nlp/2020/02/03/contextual.html&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[32] &lt;strong&gt;Is the future of Neural Networks Sparse?:&lt;/strong&gt; https://medium.com/huggingface/is-the-future-of-neural-networks-sparse-an-introduction-1-n-d03923ecbd70&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[33] &lt;strong&gt;从零开始训练语言模型:&lt;/strong&gt; https://huggingface.co/blog/how-to-train&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[34] &lt;strong&gt;Tokenizers: How machines read:&lt;/strong&gt; https://blog.floydhub.com/tokenization-nlp/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[35] &lt;strong&gt;MLVU 机器学习课程:&lt;/strong&gt; https://mlvu.github.io/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[36] &lt;strong&gt;Machine-Learning-Tokyo/Math_resources:&lt;/strong&gt; https://github.com/Machine-Learning-Tokyo/Math_resources&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[37] &lt;strong&gt;MIT 的“深度学习入门”课程:&lt;/strong&gt; http://introtodeeplearning.com/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[38] &lt;strong&gt;PyTorch 深度学习:&lt;/strong&gt; https://atcold.github.io/pytorch-Deep-Learning-Minicourse/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[39] &lt;strong&gt;“Missing Semester”:&lt;/strong&gt; https://missing.csail.mit.edu/&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[40] &lt;strong&gt;“Advanced Deep Learning”:&lt;/strong&gt; https://andrejristeski.github.io/10707-S20/syllabus.html&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[41] &lt;strong&gt;BERT-of-Theseus:&lt;/strong&gt; http://xxx.itp.ac.cn/abs/2002.02925&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[42] &lt;strong&gt;“机器学习入门”:&lt;/strong&gt; https://compstat-lmu.github.io/lecture_i2ml/index.html&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[43] &lt;strong&gt;希腊语 BERT（GreekBERT）:&lt;/strong&gt; https://huggingface.co/nlpaueb/bert-base-greek-uncased-v1&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[44] &lt;strong&gt;Jeremy Howard:&lt;/strong&gt; https://arxiv.org/abs/2002.04688&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[45] &lt;strong&gt;TensorFlow: Data and Deployment Specialization:&lt;/strong&gt; https://www.coursera.org/specializations/tensorflow-data-and-deployment&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
[46] &lt;strong&gt;Python 中的机器学习：数据科学，机器学习和人工智能的主要发展和技术趋势:&lt;/strong&gt; https://arxiv.org/abs/2002.04803&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;https://dair.ai/NLP%E7%AE%80%E6%8A%A5_ISSUE_4_PyTorch3D,_DeepSpeed,_Turing-NLG/&quot;&gt;NLP简报 [CH]: PyTorch3D, DeepSpeed, Turing-NLG, Question Answering Benchmarks, Hydra, Sparse Neural Networks,…&lt;/a&gt; was originally published by dair.ai at &lt;a href=&quot;https://dair.ai&quot;&gt;dair.ai&lt;/a&gt; on February 17, 2020.&lt;/p&gt;
  </content>
</entry>

</feed>
