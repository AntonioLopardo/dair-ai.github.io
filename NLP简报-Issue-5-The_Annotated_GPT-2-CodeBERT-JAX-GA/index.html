<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->

<head>
<meta charset="utf-8">
<title>NLP简报（Issue#5）：The Annotated GPT-2、CodeBERT、JAX、GANILLA等 &#8211; dair.ai</title>
<meta name="description" content="">
<meta name="keywords" content="nlp_newsletter">


<!-- Twitter Cards -->
<meta name="twitter:title" content="NLP简报（Issue#5）：The Annotated GPT-2、CodeBERT、JAX、GANILLA等">
<meta name="twitter:description" content="">
<meta name="twitter:site" content="@dair_ai">
<meta name="twitter:creator" content="@omarsar0">

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://dair.ai/images/nlp_newsletter_5.png">

<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP简报（Issue#5）：The Annotated GPT-2、CodeBERT、JAX、GANILLA等">
<meta property="og:description" content="">
<meta property="og:url" content="https://dair.ai/NLP%E7%AE%80%E6%8A%A5-Issue-5-The_Annotated_GPT-2-CodeBERT-JAX-GA/">
<meta property="og:site_name" content="dair.ai">

<meta property="og:image" content="https://dair.ai/images/nlp_newsletter_5.png">







<link rel="canonical" href="https://dair.ai/NLP%E7%AE%80%E6%8A%A5-Issue-5-The_Annotated_GPT-2-CodeBERT-JAX-GA/">
<link href="https://dair.ai/feed.xml" type="application/atom+xml" rel="alternate" title="dair.ai Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="https://dair.ai/assets/css/main.css">

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
	<script src="https://dair.ai/assets/js/vendor/html5shiv.min.js"></script>
	<script src="https://dair.ai/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="https://dair.ai/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<link href='//fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700%7CPT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="https://dair.ai/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="https://dair.ai/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="https://dair.ai/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://dair.ai/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://dair.ai/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://dair.ai/images/apple-touch-icon-144x144-precomposed.png">

</head>

<body class="post">

<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.8&appId=1537934899816329";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

<!-- Go to www.addthis.com/dashboard to customize your tools -->
<script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-4e43ef4f23bf37b0"></script>

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="https://dair.ai/">dair.ai</a>
	</div><!-- /.site-name -->
	<div class="top-navigation">
		<nav role="navigation" id="site-nav" class="nav">
		    <ul>
		        
				    
				    <li><a href="https://dair.ai/posts/" >Blog ✍️</a></li>
				
				    
				    <li><a href="https://dair.ai/about/" >About ℹ️</a></li>
				
				    
				    <li><a href="https://dair.ai/newsletter/" >NLP Newsletter 🗞️</a></li>
				
				    
				    <li><a href="https://dair.ai/projects/" >Projects 💡</a></li>
				
				    
				    <li><a href="https://github.com/dair-ai" target="_blank">GitHub 📁</a></li>
				
				    
				    <li><a href="https://github.com/dair-ai/dair-ai.github.io/contribute" target="_blank">Contribute ✨</a></li>
				
				    
				    <li><a href="https://medium.com/dair-ai" target="_blank">Medium 📰</a></li>
				
				    
				    <li><a href="https://nlpoverview.com/" target="_blank">NLP Overview 📘</a></li>
				
				    
				    <li><a href="https://github.com/dair-ai/nlp_highlights" target="_blank">2019 NLP Highlights (PDF) 🔥</a></li>
				
		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->



<div id="main" role="main">
  <div class="article-author-side">
    

<div itemscope itemtype="http://schema.org/Person">


	<img src="https://dair.ai/images/white_background_elvis.png" class="bio-photo" alt="Elvis Saravia bio photo">


  <h3 itemprop="name">Elvis Saravia</h3>
  <p>Editor at dair.ai</p>

  <a href="http://twitter.com/omarsar0" class="author-social" target="_blank"><i class="fa fa-fw fa-twitter-square"></i> Twitter</a>
  
  
  
  
  
  
  <a href="http://github.com/omarsar" class="author-social" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a>
  
  
  
  
  
  
  
  
  
  
</div>

  </div>
  <article class="post">
    <div class="headline-wrap">
      
        
          <h1><a href="https://dair.ai/NLP%E7%AE%80%E6%8A%A5-Issue-5-The_Annotated_GPT-2-CodeBERT-JAX-GA/" rel="bookmark" title="NLP简报（Issue#5）：The Annotated GPT-2、CodeBERT、JAX、GANILLA等">NLP简报（Issue#5）：The Annotated GPT-2、CodeBERT、JAX、GANILLA等</a></h1>
        
      
    </div><!--/ .headline-wrap -->

    
    <div class="article-wrap">
      <p><img src="https://cdn-images-1.medium.com/max/1200/1*YIhZsPaiBFkRMMWo5FAhGw.png" alt="" /></p>

<p><br />
欢迎来到 NLP 时事简报！全文较长，建议收藏。</p>

<p><br />
<strong>如果想让自己有趣的研究/项目出现在NLP简报中，随时在公众号后台留言联系我</strong></p>

<p><br />
来看看都有哪些内容，enjoy</p>

<ul>
  <li><strong>1、Publications 📙</strong>
    <ul>
      <li>1.1 理解self-distillation</li>
      <li>1.2 深度学习十年简史</li>
      <li>1.3 利用神经网络求解高等数学方程</li>
      <li>1.4 CodeBERT</li>
    </ul>
  </li>
  <li><strong>2、Creativity and Society 🎨</strong>
    <ul>
      <li>2.1 AI for scientific discovery</li>
      <li>2.2 改善image-to-illustration</li>
      <li>2.3 Andrew Ng谈自监督学习</li>
    </ul>
  </li>
  <li><strong>3、Tools and Datasets ⚙️</strong>
    <ul>
      <li>3.1 JAX libraries</li>
      <li>3.2 处理维基百科数据的工具</li>
      <li>3.3 Rust Tokenizers, DistilBERT base cased</li>
      <li>3.4 夸夸语料</li>
    </ul>
  </li>
  <li><strong>4、Ethics in AI 🚨</strong>
    <ul>
      <li>4.1 NLP和ML模型的道德考量</li>
    </ul>
  </li>
  <li><strong>5、Articles and Blog posts ✍️</strong>
    <ul>
      <li>5.1 The Annotated GPT-2</li>
      <li>5.2 Beyond BERT?</li>
      <li>5.3 矩阵压缩算子</li>
    </ul>
  </li>
  <li><strong>6、Education 🎓</strong>
    <ul>
      <li>6.1 NLP基础</li>
      <li>6.2 数学基础课</li>
      <li>6.3 书籍推荐</li>
      <li>6.4 计算机科学自学指南</li>
    </ul>
  </li>
  <li><strong>7、Noteworthy Mentions ⭐️</strong></li>
</ul>

<h2 id="1publications-"><strong>1、Publications 📙</strong></h2>

<p><strong>1.1 理解self-distillation</strong></p>

<p><br />
在深度学习中，self-distillation[1]是将知识从一种架构转移到另一种相同架构的过程。在训练时，原始模型的预测作为目标值提供给另一个模型。除具有所需的属性（例如减小模型大小）外，经验结果还表明该方法在held-out datasets上效果很好。</p>

<p>一组研究人员最近发表了一篇论文，Self-Distillation Amplifies Regularization in Hilbert Space[2]，提供了理论分析，重点是更好地了解知识蒸馏过程中正在发生的事情以及为何有效。结果表明，几轮self-distillation会通过逐渐限制代表解的基函数的数量放大正则化，这往往会减少过度拟合。</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*nWYO71awfooL4MrGK-tFww.png" alt="" /></p>

<p><br />
<strong>1.2 深度学习十年简史</strong></p>

<p><br />
人工智能的先驱、LSTM之父JürgenSchmidhuber最近发布了一个新博客，The 2010s: Our Decade of Deep Learning / Outlook on the 2020s[3]，提供自2010年以来的深度学习历史概述，包括LSTM，前馈神经网络，GAN，深度强化学习，元学习，世界模型 ，蒸馏神经网络，注意学习等一些主题。文章最后总结了2020年代的前景，鼓励人们关注紧迫的问题，例如隐私和数据市场。</p>

<p><br />
<strong>1.3 利用神经网络求解高等数学方程</strong></p>

<p><br />
Facebook AI研究人员发表了一篇论文，Deep Learning for Symbolic Mathematics[4]，声称提出了一个针对数学问题和匹配解决方案进行训练的模型，以学习预测诸如解决集成问题之类的任务的可能解决方案。该方法基于类似于机器翻译中使用的新颖框架，在该框架中，数学表达式表示为一种语言，而解决方案则视为翻译问题。因此，输出是解决方案本身，而不是模型输出翻译。据此，研究人员声称深度神经网络不仅擅长符号推理，而且还可以胜任各种任务。</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*P_JtxoC8pYkXuXCp3e3QeQ.png" alt="" /></p>

<p><br />
<strong>1.4 CodeBERT</strong></p>

<p><br />
在这篇名为《CodeBERT: A Pre-Trained Model for Programming and Natural Languages》[5]的论文中，来自哈工大、中山大学和微软的研究人员详细介绍了这一新预训练模型，该模型可处理双模态数据：编程语言（PL）和自然语言（NL）。</p>

<p><br />
CodeBERT 学习能够支持下游 NL-PL 应用的通用表示，比如自然语言代码搜索、代码文档生成，经实验 CodeBERT 模型在两项任务均取得 SOTA 效果，同时研究者构建了 NL-PL 探测数据集，CodeBERT 在 zero-shot 设置中的性能表现也持续优于 RoBERTa。</p>

<h2 id="2creativity-and-society-"><strong>2、Creativity and Society 🎨</strong></h2>

<p><strong>2.1 AI for scientific discovery</strong></p>

<p><br />
Mattew Hutson报告了如何使用人工智能（AI）来生成仿真器[6]，这些仿真器在对复杂自然现象进行建模方面具有重要作用，而自然现象又可能导致不同类型的科学发现。构建这些仿真器的变化是，它们通常需要大规模数据和广泛的参数探索。最近的论文提出了DENSE方法[7]，一种基于神经结构搜索[8]来构建准确的仿真器，而仅依赖有限数量的训练数据。他们通过对包括天体物理学，气候科学和聚变能等在内的案例进行仿真来对其进行测试。</p>

<p><br />
<strong>2.2 改善image-to-illustration</strong></p>

<p><br />
GANILLA[9]是一种使用GAN来改进未配对的图像到图像翻译任务[10]中样式和内容传递的方法。提出了一种具有改进的生成器网络用于图像到插图的模型，并基于新的定量评估框架对模型进行了评估，该框架同时考虑了内容和样式。这项工作的新颖性在于拟议的生成器网络，该生成器网络考虑了先前模型无法实现的样式和内容之间的平衡。可以在此处阅读原文：GANILLA: Generative Adversarial Networks for Image to Illustration Translation[11]</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*l_B4vfaHVkXDwzM7SldiqQ.png" alt="" /></p>

<p><br />
<strong>2.3 Andrew Ng谈自监督学习</strong></p>

<p><br />
deeplearning.ai的创始人Andrew Ng加入人工智能播客[12]，讨论的主题包括他早期从事ML的工作，AI的未来和AI教育，正确使用ML的建议，他的个人目标以及在2020年代应该关注ML技术。</p>

<p><br />
Andrew解释了为什么他对自监督的表示学习感到非常兴奋。自监督式学习涉及一个学习问题，该问题旨在从数据本身获得监督，以利用大量未标记数据，这比纯净标记数据更常见。这些表示很重要，可用于处理下游任务，类似于BERT等语言模型中使用的任务。</p>

<p><br />
使用自监督学习来学习广义的视觉表示也引起了很大关注，这使模型在资源匮乏的环境中更加准确。例如，最近一种名为SimCLR[13]的方法（由Geoffrey Hinton领导）提出了一种视觉表示的对比自监督学习框架，以改善在不同环境下的图像分类结果，例如转移学习和半监督学习。</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*8zLzHFCyM3goc9y7KApHfg.png" alt="" /></p>

<h2 id="3tools-and-datasets-️"><strong>3、Tools and Datasets ⚙️</strong></h2>

<p><strong>3.1 JAX libraries</strong></p>

<p><br />
JAX[14]是一个新库，结合了NumPy和自动微分功能，可以进行高性能ML研究。为了简化使用JAX构建神经网络的管道，DeepMind发布了Haiku[15]和RLax[16]。使用熟悉的面向对象编程模型，RLax简化了强化学习代理的实现，而Haiku简化了神经网络的构建。</p>

<p><br />
<strong>3.2 处理维基百科数据的工具</strong></p>

<p><br />
Sparkwiki[17] 是处理Wikipedia数据的工具。此版本是有趣的行为分析研究的众多努力的一部分，例如，捕获跨不同语言版本的Wikipedia的趋势和语言偏见[18]。作者发现，独立于语言，维基百科用户的浏览行为表明，他们倾向于在电影，音乐和体育等类别上拥有共同的兴趣，但是随着当地事件和文化特殊性的出现，差异变得更加明显。</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*K7N9KbQlbuqowUeePjLtdw.jpeg" alt="" /></p>

<p><br />
<strong>3.3 Rust Tokenizers, DistilBERT base cased, Model cards</strong></p>

<p><br />
Hugging Face发行的新版Transformers[19]包括其快速分词器库的集成，该库旨在加速BERT，RoBERTa，GPT2等模型以及其他社区构建的模型。</p>

<p><br />
<strong>3.4 夸夸语料</strong></p>

<p><br />
夸夸语料[20]，来自豆瓣互相表扬组数据。</p>

<h2 id="4ethics-in-ai-"><strong>4、Ethics in AI 🚨</strong></h2>

<p><strong>4.1 NLP和ML模型的道德考量</strong></p>

<p><br />
在NLP Highlights的新内容中[21]，Emily Bender和主持人讨论了在学术界和实际使用情况下开发NLP模型和技术时的一些道德考量。讨论中的一些主题包括设计NLP任务，数据收集方法以及最终发布结果时的道德考虑。</p>

<p><br />
除了上述所有考虑因素之外，AI社区中经常讨论的一个问题过于关注优化指标，这与AI旨在实现的目标背道而驰。Rachel Thomas和David Uminsky[22]讨论了通过对不同用例进行透彻分析而可能出错的地方。他们还提出了一个缓解该问题的简单框架，其中涉及多个指标的使用和组合，然后是那些直接受到该技术影响的人的参与。</p>

<h2 id="5articles-and-blog-posts-️"><strong>5、Articles and Blog posts ✍️</strong></h2>

<p><strong>5.1 The Annotated GPT-2</strong></p>

<p><br />
Aman Arora最近发表了一篇特别的博客文章，标题为“ The Annotated GPT-2[23]”，解释了基于Transformer的模型GPT-2的内部工作原理。他的方法受到The Annotated Transformer[24]的启发，采用了注释方法，通过代码和易于理解的解释来解释模型的重要部分。Aman付出了巨大的努力，使用PyTorch和Hugging Face的Transformers库重新实现OpenAI的GPT-2。这是出色的工作！</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*oRFMJTEojyQ-uocVES5GYA.png" alt="" /></p>

<p><br />
<strong>5.2 Beyond BERT?</strong></p>

<p><br />
Sergi Castella[25]对BERT以外的内容感兴趣。主要主题包括改善指标，Hugging Face的Transformers库如何支持研究，查看有趣的数据集，解压缩模型等。</p>

<p><br />
<strong>5.3 矩阵压缩算子</strong></p>

<p><br />
TensorFlow博客发布了一篇博客文章，Matrix Compression Operator[26]，解释了在深度神经网络模型中压缩矩阵背后的技术和重要性。矩阵压缩可以帮助构建更有效的微型模型，这些模型可以集成到较小的设备中，例如电话和家庭助理。通过低秩逼近和量化等方法专注于模型的压缩，这意味着我们无需牺牲模型的质量。</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*fpAdJvBIf4SKxF3gTIpe_g.png" alt="" /></p>

<h2 id="6education-"><strong>6、Education 🎓</strong></h2>

<p><strong>6.1 NLP基础</strong></p>

<p><br />
NLP基础[27]从基础开始讲授NLP概念，同时分享最佳实践，重要参考，应避免的常见错误以及NLP的未来。包含一个Colab笔记本[28]，该项目将在此github[29]维护。</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/1*mS5NcoJ_c8hYTjiJsuu_8g.gif" alt="" /></p>

<p><br />
<strong>6.2 数学基础课</strong></p>

<p><br />
Machine Learning Tokyo 将在3月8日主持一个远程在线讨论，其中回顾他们最近的在线学习课程中[30]涉及的章节。该小组以前研究过Marc Peter Deisenroth，Ado Faisal和Cheng Soon Ong所著的《机器学习数学》[31]一书章节。</p>

<p><br />
<strong>6.3 书籍推荐</strong></p>

<p><br />
在上一部分中，我们讨论了矩阵压缩对于构建微型ML模型的重要性。如果你有兴趣了解有关如何为嵌入式系统构建更小的深度神经网络的更多信息，请查看Pete Warden和Daniel Situnayake撰写的这本名为TinyML[32]的出色著作。</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/0*omOa3aw2bfMzX2Qm.jpg" alt="" /></p>

<p><br />
另一本值得关注的有趣书籍是即将出版的题为“Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD[33]”，作者为Jeremy Howard和Sylvain Gugger。该书旨在提供必要的数学基础，以建立和训练模型来处理计算机视觉和NLP领域中的任务。</p>

<p><br />
<img src="https://cdn-images-1.medium.com/max/800/0*i2OmtWOncatOYsZv.jpg" alt="" /></p>

<p><br />
<strong>6.4 计算机科学自学指南</strong></p>

<p><br />
使用建议的教科书或视频讲座系列，以大致顺序学习以下所有九个主题，但理想情况下两者都学习。针对每个主题进行100-200小时的学习，然后在整个职业生涯中重温最爱favorites。另外在reddit上也有类似的讨论[34]。</p>

<h2 id="7noteworthy-mentions-️"><strong>7、Noteworthy Mentions ⭐️</strong></h2>

<p>Torchmeta[35]是一个是由Tristan Deleu创作的可以轻松使用相关的数据加载器进行元学习研究的库。</p>

<p><br />
Manuel Tonneau撰写了一篇文章，仔细研究了语言建模中涉及的一些机制[36]，包括贪婪和波束搜索以及原子核采样等主题。</p>

<p><br />
MIT发布了名为“Introduction to Deep Learning[37]”的课程的完整提纲和课程表，其中包括已授课的视频， 他们的目标是每周发布视频讲座和幻灯片。</p>

<p><br />
了解如何使用基于Transformer的方法在不到300行代码中训练用于命名实体识别（NER）的模型[38]。您可以在此处找到随附的Google Colab[39]。</p>

<hr />
<p><br />
<strong>本文参考资料</strong></p>

<p><br />
[1] <strong>self-distillation:</strong> https://arxiv.org/pdf/1503.02531.pdf</p>

<p><br />
[2] <strong>Self-Distillation Amplifies Regularization in Hilbert Space:</strong> http://xxx.itp.ac.cn/abs/2002.05715</p>

<p><br />
[3] <strong>The 2010s: Our Decade of Deep Learning / Outlook on the 2020s:</strong> http://people.idsia.ch/~juergen/2010s-our-decade-of-deep-learning.html</p>

<p><br />
[4] <strong>Deep Learning for Symbolic Mathematics:</strong> https://arxiv.org/abs/1912.01412</p>

<p><br />
[5] <strong>《CodeBERT: A Pre-Trained Model for Programming and Natural Languages》:</strong> https://arxiv.org/abs/2002.08155</p>

<p><br />
[6] <strong>如何使用人工智能（AI）来生成仿真器:</strong> https://www.sciencemag.org/news/2020/02/models-galaxies-atoms-simple-ai-shortcuts-speed-simulations-billions-times</p>

<p><br />
[7] <strong>论文提出了DENSE方法:</strong> https://arxiv.org/abs/2001.08055</p>

<p><br />
[8] <strong>神经结构搜索:</strong> https://en.wikipedia.org/wiki/Neural_architecture_search</p>

<p><br />
[9] <strong>GANILLA:</strong> https://github.com/giddyyupp/ganilla</p>

<p><br />
[10] <strong>图像到图像翻译任务:</strong> https://paperswithcode.com/task/image-to-image-translation</p>

<p><br />
[11] <strong>GANILLA: Generative Adversarial Networks for Image to Illustration Translation:</strong> https://arxiv.org/abs/2002.05638</p>

<p><br />
[12] <strong>人工智能播客:</strong> https://www.youtube.com/watch?v=0jspaMLxBig</p>

<p><br />
[13] <strong>SimCLR:</strong> https://arxiv.org/abs/2002.05709</p>

<p><br />
[14] <strong>JAX:</strong> https://github.com/google/jax</p>

<p><br />
[15] <strong>Haiku:</strong> https://github.com/deepmind/dm-haiku</p>

<p><br />
[16] <strong>RLax:</strong> https://github.com/deepmind/rlax</p>

<p><br />
[17] <strong>Sparkwiki:</strong> https://github.com/epfl-lts2/sparkwiki</p>

<p><br />
[18] <strong>捕获跨不同语言版本的Wikipedia的趋势和语言偏见:</strong> https://arxiv.org/abs/2002.06885</p>

<p><br />
[19] <strong>新版Transformers:</strong> https://github.com/huggingface/transformers/releases/tag/v2.5.0</p>

<p><br />
[20] <strong>夸夸语料:</strong> https://github.com/xiaopangxia/kuakua_corpus</p>

<p><br />
[21] <strong>NLP Highlights的新内容中:</strong> https://soundcloud.com/nlp-highlights/106-ethical-considerations-in-nlp-research-emily-bender</p>

<p><br />
[22] <strong>Rachel Thomas和David Uminsky:</strong> https://arxiv.org/abs/2002.08512</p>

<p><br />
[23] <strong>The Annotated GPT-2:</strong> https://amaarora.github.io/2020/02/18/annotatedGPT2.html</p>

<p><br />
[24] <strong>The Annotated Transformer:</strong> https://nlp.seas.harvard.edu/2018/04/03/attention.html</p>

<p><br />
[25] <strong>Sergi Castella:</strong> https://towardsdatascience.com/beyond-bert-6f51a8bc5ce1</p>

<p><br />
[26] <strong>Matrix Compression Operator:</strong> https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html?linkId=82298016</p>

<p><br />
[27] <strong>NLP基础:</strong> https://medium.com/dair-ai/fundamentals-of-nlp-chapter-1-tokenization-lemmatization-stemming-and-sentence-segmentation-b362c5d07684</p>

<p><br />
[28] <strong>Colab笔记本:</strong> https://colab.research.google.com/drive/18ZnEnXKLQkkJoBXMZR2rspkWSm9EiDuZ</p>

<p><br />
[29] <strong>此github:</strong> https://github.com/dair-ai/nlp_fundamentals</p>

<p><br />
[30] <strong>在线学习课程中:</strong> https://www.meetup.com/Machine-Learning-Tokyo/events/268817313/</p>

<p><br />
[31] <strong>《机器学习数学》:</strong> https://mml-book.github.io/</p>

<p><br />
[32] <strong>TinyML:</strong> https://tinymlbook.com/?linkId=82595412</p>

<p><br />
[33] <strong>Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD:</strong> https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527</p>

<p><br />
[34] <strong>reddit上也有类似的讨论:</strong> https://www.reddit.com/r/learnprogramming/comments/87j7fw/teach_yourself_computer_science_a_diy_curriculum/</p>

<p><br />
[35] <strong>Torchmeta:</strong> https://arxiv.org/abs/1909.06576</p>

<p><br />
[36] <strong>语言建模中涉及的一些机制:</strong> https://creatext.ai/blog-posts/machine-text-writing-gpt2-beam-search?utm_medium=newsletter</p>

<p><br />
[37] <strong>Introduction to Deep Learning:</strong> http://introtodeeplearning.com/</p>

<p><br />
[38] <strong>训练用于命名实体识别（NER）的模型:</strong> https://github.com/huggingface/transformers/blob/master/examples/ner/run_pl_ner.py</p>

<p><br />
[39] <strong>随附的Google Colab:</strong> https://colab.research.google.com/drive/184LPlygvdGGR64hgQl3ztqzZJu8MmITn</p>

      <hr />
      <footer role="contentinfo">
        <div class="social-share">
  <!-- Go to www.addthis.com/dashboard to customize your tools --> 
  <div class="addthis_inline_share_toolbox"></div>
  <!--
  <h4>Share on</h4>
  <ul>
    <li>
      <a href="https://twitter.com/intent/tweet?text=https://dair.ai/NLP%E7%AE%80%E6%8A%A5-Issue-5-The_Annotated_GPT-2-CodeBERT-JAX-GA/" class="twitter" title="Share on Twitter"><i class="fa fa-twitter"></i><span> Twitter</span></a>
    </li>
    <li>
      <a href="https://www.facebook.com/sharer/sharer.php?u=https://dair.ai/NLP%E7%AE%80%E6%8A%A5-Issue-5-The_Annotated_GPT-2-CodeBERT-JAX-GA/" class="facebook" title="Share on Facebook"><i class="fa fa-facebook"></i><span> Facebook</span></a>
    </li>
    <li>
      <a href="https://plus.google.com/share?url=https://dair.ai/NLP%E7%AE%80%E6%8A%A5-Issue-5-The_Annotated_GPT-2-CodeBERT-JAX-GA/" class="google-plus" title="Share on Google Plus"><i class="fa fa-google-plus"></i><span> Google+</span></a>
    </li>
  </ul>-->
</div><!-- /.social-share -->
        <p class="byline"><strong>NLP简报（Issue#5）：The Annotated GPT-2、CodeBERT、JAX、GANILLA等</strong> was published on <time datetime="2020-02-29T00:00:00-06:00">February 29, 2020</time>.</p>
        
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dair-ai'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<!--
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
-->

      </footer>
    </div><!-- /.article-wrap -->
  
    <section id="disqus_thread"></section><!-- /#disqus_thread -->
  
  </article>
</div><!-- /#main -->

<div class="footer-wrap">
  
  <div class="related-articles">
  <h4>You might also enjoy <small class="pull-right">(<a href="https://dair.ai/posts/">View all posts</a>)</small></h4>
    <ul>
    
      <li><a href="https://dair.ai/NLP_Newsletter_NLP_7-ZH-.md/" title="NLP 简报（Issue#7）: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,…">NLP 简报（Issue#7）: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,…</a></li>
    
      <li><a href="https://dair.ai/NLP_Newsletter_NLP_7/" title="NLP Newsletter: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,…">NLP Newsletter: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,…</a></li>
    
      <li><a href="https://dair.ai/NLP_Newsletter_-7_-FR/" title="NLP Newsletter [FR] #7: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,…">NLP Newsletter [FR] #7: NLP Paper Summaries, Learning to Simulate, Transformers Notebooks, Med7, Measuring Compositional Generalization, Neural Tangents,…</a></li>
    
    </ul>
    <hr />
  </div><!-- /.related-articles -->
  
  <footer>
    

<span>&copy; 2020 dair.ai. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="http://mademistakes.com/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a> theme.</span>

  </footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="https://dair.ai/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="https://dair.ai/assets/js/scripts.min.js"></script>

<!-- Asynchronous Google Analytics snippet -->
<script>
  var _gaq = _gaq || [];
  var pluginUrl =
 '//www.google-analytics.com/plugins/ga/inpage_linkid.js';
  _gaq.push(['_require', 'inpage_linkid', pluginUrl]);
  _gaq.push(['_setAccount', 'UA-158959084-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
<script async defer src="https://buttons.github.io/buttons.js"></script>


  
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'dair-ai'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<!--
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
-->




</body>
</html>
